[
  {
    "objectID": "en_abraham_wald",
    "title": "The Architecture of Silence: Abraham Wald and the Epistemology of Missing Data",
    "description": "In 1943, the US military wanted to armor the bullet holes on their B-17s. A mathematician told them to armor the empty spaces. This is the story of how Survivorship Bias destroys modern AI models and business strategies.",
    "content": "In the high-stakes environment of data engineering and executive decision-making, we are often seduced by the dashboard. We trust the rows in our SQL databases and the logs in our SIEMs because they are tangible. They are what we see. But in my experience architecting systems‚Äîfrom defense industry logistics to modern cloud infrastructures‚Äîthe most dangerous data is not the outlier; it is the data that never made it into the database. Today, we step back from the code to examine a fundamental \"First Principle\" of data analysis: Survivorship Bias . To do so, we must revisit 1943 and the mind of a man who saw the invisible. The War of Numbers World War II was the first conflict where victory depended largely on information processing and the application of mathematical rigor to the uncertainty of the battlefield. It wasn't just fought on the beaches; it was fought in offices where equations were the ammunition and the enemy was cognitive error. The Allied Air Forces faced a crisis. Their strategic bombers, the B-17 Flying Fortresses, were suffering catastrophic attrition rates over Europe. The intuitive solution for the military command was simple: armor the planes. But armor is a zero-sum game; every kilogram of steel reduces bomb load and maneuverability, paradoxically making the plane easier to shoot down. The military turned to data. They analyzed the bombers returning from missions and mapped every bullet hole. The data spoke with seductive clarity: the wings, the central fuselage, and the tail were riddled with damage. The military logic, guided by visible empirical evidence, dictated reinforcing these \"wounded\" areas. It makes sense, right? You reinforce where you get hit. This is where Abraham Wald , a Jewish-Hungarian mathematician from the Statistical Research Group (SRG) at Columbia University, intervened. Abraham Wald The Inversion of Logic Wald was an outsider. Excluded from formal education in his youth due to his religion, he developed a radical intellectual independence. He looked at the same diagrams as the generals but reached the diametrically opposite conclusion. His argument was elegant and counterintuitive: \"The armor doesn't go where the bullet holes are. It goes where the holes aren't: the engines and the cockpit\" . How did he arrive at this conclusion? By asking the one question nobody else asked: Where are the missing planes? Wald assumed that German flak was random. It didn't have a guidance system seeking out wings. Therefore, hits should be uniformly distributed. If the planes returning to base had holes in the fuselage but pristine engines, it wasn't because the engines weren't getting hit. It was because the planes hit in the engines never came back . The \"red dots\" on the diagrams didn't indicate critical damage; they indicated survivable damage. The empty spaces were the kill zones. Wald taught us that truth often resides not in the data we have, but in the silence of the data we lack. The Mathematics of Survival While the story is often told as a \"Eureka!\" moment, Wald's contribution was a rigorous statistical model involving complex conditional probabilities. Let's look at this through a simplified engineering lens. Wald essentially established an inverse relationship between observed damage density and vulnerability. If we define vulnerability as the probability of a plane being downed given a hit in a specific zone, and we observe the density of hits in survivors, the logic flows as follows: If vulnerability(engine) ‚âà 1 (Lethal) ‚Üí Engine Hits in Survivors ‚âà 0 In other words: if getting hit in the engine almost always means death, then surviving planes will almost never have engine damage‚Äînot because engines weren't hit, but because those planes crashed. Wald demonstrated that a returning B-17 with 100 holes in its wings provides statistical proof of the wing's robustness. Conversely, the absence of data on fuel pump damage indicates a critically low failure threshold. The Ghost in the Machine: Modern Implications Why does this matter to a modern CTO or Data Engineer? Because Survivorship Bias is an epidemic in the digital economy. We are building algorithms and strategies based on filtered datasets, often with disastrous results. 1. The Startup Graveyard In the startup ecosystem, we obsess over the \"Garage Myth.\" We see Bill Gates or Mark Zuckerberg drop out of college and succeed, so we infer that formal education is a hurdle. This is pure Waldian error. We are analyzing the \"returning bombers.\" For every Zuckerberg, there are thousands of dropouts who failed and are invisible to Forbes. By studying only the survivors (\"unicorns\"), we isolate characteristics that might be irrelevant or even harmful. 2. AI and Algorithmic Bias This is where the legacy of Wald becomes critical for ethical engineering. Machine Learning models are inference engines. If fed survivor-biased data, they automate discrimination. Amazon, for example, had to scrap a recruiting AI because it penalized",
    "url": "https://datalaria.com/en/posts/abraham_wald/",
    "slug": "abraham_wald",
    "lang": "en",
    "categories": [
      "case-studies"
    ],
    "tags": [
      "survivorship bias",
      "abraham wald",
      "decision theory",
      "machine learning",
      "cybersecurity"
    ],
    "date": "2026-02-21",
    "timestamp": 0,
    "domain": "General",
    "image": "wald-cover.png"
  },
  {
    "objectID": "en_ai-education-deep_research",
    "title": "Researching with AI: How I Created a Detailed Report on the Global Impact of AI in Education in Minutes with Gemini's Deep Research",
    "description": "A case study on how I used Gemini's Deep Research feature to generate an exhaustive report on the impact of AI in education and a revealing summary of it.",
    "content": "Deep research is the foundation of knowledge, but let's be honest: it's an arduous process that requires a lot of dedication and work. Gathering dozens of sources, synthesizing contradictory data, and structuring a coherent narrative can take weeks, if not months. What if we could accelerate that process from weeks to minutes? That's the promise of Gemini's Deep Research feature, a tool I've put to the test to create a comprehensive report on one of the hottest topics of our time: the impact of Artificial Intelligence in Education . This post is not just about the fascinating findings of that research, but also about the process itself. It's the story of how a simple question to an AI can generate a scholarly-quality analysis, and how we can transform that dense information into a story accessible to everyone. The Starting Point: A Question to Gemini It all started with a conversation among friends about how we imagined education for this generation and the next, considering the transformative impact of Artificial Intelligence. To add more depth and detail to our reflections, we posed a simple request to Gemini using its Deep Research option to see what would happen. We simply asked, \"Research the global impact of AI in education.\" The tool got to work, analyzing and synthesizing information from a multitude of sources to generate a complete and structured report in about 5-10 minutes... and this was the result of its work: Research on the impact of AI in education by Deep Research What follows are the most revealing findings from that analysis, a body of knowledge we can now explore in a fraction of the time it would have traditionally taken, while maintaining an exceptionally high level of content quality. The Global Landscape: An Explosive Market Clashing with Ethics To begin, let's talk about money. The AI in education market is booming, with projections exceeding $32 billion by 2030 , driven by an insatiable demand for personalized learning. However, this dizzying expansion clashes with the vision of organizations like UNESCO , which advocate for a human-centered and ethical approach. The problem is that technology moves much faster than policy and regulation. A 2023 survey revealed an alarming fact: only 10% of schools and universities worldwide have an official framework for the use of AI . This creates a \"gulf between policy and practice,\" where risks are not being addressed in its day-to-day use. If you want a quick overview of this complex landscape, I've used NotebookLM (recalling what I learned in the NotebookLM post on Datalaria ) to generate a video summary of the key points from this report, which you can watch here: Video Summary of the AI in Education Report . The Global Race for AI in Education: 4 Countries, 4 Strategies The way each country adopts AI in its classrooms is a reflection of its values and geopolitical ambitions. The classroom has become a new stage for global competition. China: Its strategy is centralized and mandatory. Starting in 2025, AI education will be compulsory for all primary and secondary school students . The curriculum is systematic, differentiated by age, and establishes a minimum of eight hours of AI instruction per year. Primary school students focus on experiential learning with robotics, while secondary students tackle advanced projects and algorithms. Singapore: It integrates AI as part of its \"EdTech Masterplan 2030,\" with a systemic approach and strong investment in R&D. The strategy centers on the national Student Learning Space (SLS) platform, which is being enhanced with AI tools accessible to all students. Pilot programs are already underway, such as an Adaptive Learning System for mathematics that has been offering personalized recommendations in 33 schools since 2023. Finland: True to its Nordic values, it bases its strategy on ethics and data privacy . Its cornerstone is teacher training and AI literacy for all citizens. One of its most recognized initiatives is the free and world-renowned \"Elements of AI\" course from the University of Helsinki. Furthermore, its focus on ethics is so profound that they even require ethical reviews for the use of tools like ChatGPT in the classroom. United States: With a federal structure, it lacks a single mandate. The approach relies on incentives and public-private partnerships, creating a patchwork of diverse policies. This is reflected in initiatives like the \"Presidential AI Challenge,\" which encourages students to use AI to solve community problems. This decentralization also leads to disparate reactions, such as when large school districts (New York, Los Angeles) initially banned ChatGPT only to later reverse their decision. But Does It Really Work? The Scientific Evidence Beyond the money and policies, does AI improve learning? Science says yes, and emphatically so. A 2025 meta-analysis found a very large positive impact of AI on educational outcomes (with a Hedges' g effect size of 0.86). But what does tha",
    "url": "https://datalaria.com/en/posts/ai-education-deep_research/",
    "slug": "ai-education-deep_research",
    "lang": "en",
    "categories": [
      "Tools",
      "AI"
    ],
    "tags": [
      "ai",
      "gemini",
      "deep research",
      "education",
      "research",
      "edtech",
      "unesco"
    ],
    "date": "2025-09-27",
    "timestamp": 0,
    "domain": "General",
    "image": "cover.png"
  },
  {
    "objectID": "en_ai_agents_part1",
    "title": "Project Autopilot: Why I Fired Myself as Community Manager to Build an AI agents Army",
    "description": "Experimenting with AI and new tech is my passion; distributing that content... not so much. In this series, I document how I'm building an automated team of AI Agents to handle my posts and manage their distribution across social media.",
    "content": "In the world of engineering and technical blogging, we often face the \"Builder's Paradox.\" We can spend 40 hours perfecting a specific topic, defining an architecture, or debugging tiny technical details. Yet, we can't seem to find 15 minutes to effectively promote that work on social media. I have reached that point with Datalaria . The content is there, the architecture is optimized, but the distribution suffers due to the main bottleneck... well, me. Today, I am making a strategic decision. I am \"firing\" myself from the role of Community Manager‚Äîa role I never truly exercised anyway. In my place, I am not hiring an agency; I am building one, experimenting with the trendy concept of \"AI Agents.\" Welcome to Project Autopilot : a 5-part series where we will build, live and in public, an autonomous system of AI Agents that reads this blog, analyzes it in detail, and autonomously prepares content for promotion and distributes it across social media while I am off doing other things. The Strategy: Extreme Dogfooding The concept is simple but technically ambitious. We are going to execute a strategy of \"dogfooding\" (eating our own food). Instead of using third-party tools like Buffer or Hootsuite , we will build a custom distribution engine using the very technologies we write about: Generative AI and CI/CD Pipelines . The \"Big Goal\" is to transform the blogging process. Currently, \"publishing\" means pushing a Markdown file to GitHub. In the future, that git push will trigger a chain reaction where intelligent agents analyze, create, and distribute content. The Architecture: Meet the Team To solve this, a simple Python script isn't enough. We need reasoning capabilities. We need a system that understands context, tone, and audience. We are designing an event-driven architecture hosted entirely within GitHub Actions , using Google Gemini as the brain and CrewAI as the orchestrator. Here is the conceptual flow of the system we are going to build: graph TD %% Styles classDef human fill:#ff9f43,stroke:#333,stroke-width:2px,color:white; classDef code fill:#5f27cd,stroke:#333,stroke-width:2px,color:white; classDef ai fill:#0abde3,stroke:#333,stroke-width:2px,color:white; classDef social fill:#ee5253,stroke:#333,stroke-width:2px,color:white; %% Nodes User(\"üë±‚Äç‚ôÇÔ∏è Me / Author\"):::human Git[\"üìÇ GitHub Repository <br/> Push new .md file\"]:::code Action[\"‚öôÔ∏è GitHub Actions <br/> CI/CD Runner\"]:::code %% FIX: Subgraph with padding hack for title subgraph TeamAI [\"ü§ñ The Team (CrewAI + Gemini)<br/><br/>\"] direction TB Orchestrator{\"üß† Orchestrator\"}:::ai Analyst[\"üïµÔ∏è Agent 1: The Analyst <br/> (Extracts Metadata & Hooks)\"]:::ai WriterX[\"üê¶ Agent 2: Twitter Writer <br/> (Viral/Short Content)\"]:::ai WriterLI[\"üíº Agent 3: LinkedIn Writer <br/> (Professional Tone)\"]:::ai end Review(\"üëÄ Human Review <br/> Pull Request / Draft\"):::human X[\"Twitter / X API\"]:::social LI[\"LinkedIn API\"]:::social %% Connections User -->|git push| Git Git -->|Trigger| Action Action -->|Start Process| Orchestrator Orchestrator -->|Raw Text| Analyst Analyst -->|JSON| Orchestrator Orchestrator -->|Context + Hooks| WriterX Orchestrator -->|Context + Key Points| WriterLI WriterX -->|Draft| Review WriterLI -->|Draft| Review Review -->|Approve| X Review -->|Approve| LI Architectural Decisions The Trigger (GitHub Actions): Why pay for a server? The blog is static (Hugo), so the automation should be ephemeral. It runs only when I publish. The Brain (Gemini Pro): We chose this model for its large context window. It needs to read entire technical tutorials without \"forgetting\" the beginning. The Orchestrator (CrewAI): This allows us to assign specific personas or roles. We don't want generic AI; we want a \"Cynical Twitter Expert\" and a \"Corporate Strategist\" working in parallel. Proof of Concept: Can AI Understand My Code? Before writing a single line of the final pipeline, I needed to validate the core hypothesis: Can Gemini actually understand the structure of my Hugo posts? I ran a test using a system prompt designed to act as a \"Senior Tech Editor.\" The goal was not to write text, but to extract structured data (JSON) from my raw Markdown files. The result was promising: The model correctly identified the Tech Stack , generated a summary, and most importantly, extracted \"Provocative Angles\" for marketing. This structured JSON is what will feed our writer agents in the next phase. The Roadmap This series is the core of Datalaria's content strategy for the coming months. We will document the pain, the bugs, and the victories in real-time. Post 1: The Strategy (You are here). The Master Plan and Architecture. Post 2: The Brain. Configuring Gemini Pro and LangChain/CrewAI to read and \"understand\" Markdown. Post 3: The Creatives. Prompt Engineering to create distinct personalities for LinkedIn vs. Twitter. Post 4: The API Nightmare. An honest look at the challenges of connecting to Social Media APIs. Post 5: The Final Orchestrator. CI/CD integration with GitHub Actions fo",
    "url": "https://datalaria.com/en/posts/ai_agents_part1/",
    "slug": "ai_agents_part1",
    "lang": "en",
    "categories": [
      "Automation",
      "Artificial Intelligence",
      "DevOps"
    ],
    "tags": [
      "Agents",
      "Gemini",
      "CrewAI",
      "GitHub Actions",
      "Python",
      "Dogfooding"
    ],
    "date": "2025-12-27",
    "timestamp": 0,
    "domain": "Projects",
    "image": "cover.png"
  },
  {
    "objectID": "en_ai_agents_part2",
    "title": "Autopilot - The Brain: Configuring Gemini and CrewAI to Read My Blog",
    "description": "Chapter two of Project Autopilot. We open the IDE to connect Python with Gemini Flash and create our first Analyst Agent capable of understanding Markdown code.",
    "content": "In Post 1: The Strategy , I promised I wouldn't use third-party tools to manage my social media. I promised to build an \"army of agents.\" Today, we leave PowerPoint behind and open the IDE. We are going to build the system's Brain . Today's goal is technical and concrete: create a Python script capable of reading a .md file from my local repository, \"reading\" it as a senior engineer would, and returning a structured analysis in JSON. The Tech Stack: Speed Over Brute Force For this task, I made two architectural decisions: The Orchestrator: CrewAI. I need something that handles \"Agents\" and \"Tasks,\" not just text strings. CrewAI allows me to define roles (who you are) and goals (what you want), which is vital for the next steps. The Model: Google Gemini Flash. At first, I tried using the most powerful model (Pro), but I realized a beginner's mistake: for massive reading and data extraction tasks, you don't need the philosopher, you need the fast librarian. Flash is much faster, cheaper (free in the current tier), and has a gigantic context window. Step 1: Repository Hygiene (Clean Monorepo) Before writing code, we must organize the house. My blog is built on Hugo, and I don't want to clutter the website folder with Python scripts. I opted for a \"Clean Monorepo\" structure. I created an autopilot folder at the root of the project that acts as an independent module. [code block] Lesson learned: Configure your .gitignore before making the first commit. If you upload your API Key to GitHub by mistake, bots will take seconds to find it, or worse, you might encounter unwanted costs from other users finding your API Key. Step 2: The Code (Hands-on) The heart of this system isn't main.py , but how we define the agent. Using the crewai and langchain_google_genai libraries, I defined my first digital employee: \"The Analyst.\" The \"Obsession with OpenAI\" Problem Here I hit the first wall. CrewAI is designed by default to look for an OpenAI API Key (GPT-4). Although I configured Gemini, the script failed with the error: ValueError: OPENAI_API_KEY is required The solution was to explicitly force the LLM (Large Language Model) inside the agent definition. This is what the code looks like in src/agents.py : [code block] Step 3: Prompt Engineering (JSON Mode) For this to be useful, the agent can't simply \"chat\" about the article. I need data that I can process computationally later. In src/tasks.py , I defined the task with strict output instructions. I didn't use the API's native \"JSON Mode\" (which is sometimes complex to configure), but instead relied on the model's instruction-following capability: \"OUTPUT FORMAT: Return ONLY a valid JSON object with keys: summary, target_audience, tech_stack, key_takeaways, marketing_hooks.\" The Result: It Works! I ran the script against one of my densest technical articles (about S&OP processes). I was afraid the model might hallucinate or get lost in the text. The result in the terminal was this clean JSON: [code block] It's impressive. The model not only summarized the text but understood the deep context : it identified that the article talked about \"Mermaid.js\" and \"BPMN\" and generated marketing hooks (\"marketing_hooks\") that actually sound like something I would write on Twitter. What's Next? We already have the Brain ( autopilot ) capable of reading and understanding what I write in content . The data is structured and ready. But a JSON doesn't get likes. In the next post, we are going to build The Creatives . We will use this data to feed two new agents with opposing personalities: a viral expert for Twitter and a corporate strategist for LinkedIn. And we will see how Prompt Engineering can drastically change the tone of an AI. üëâ Source Code: You can see the code for this module in the /autopilot folder of the Datalaria GitHub repository .",
    "url": "https://datalaria.com/en/posts/ai_agents_part2/",
    "slug": "ai_agents_part2",
    "lang": "en",
    "categories": [
      "Software Engineering",
      "Generative AI",
      "Python"
    ],
    "tags": [
      "CrewAI",
      "Gemini API",
      "Backend",
      "Clean Code",
      "Automation"
    ],
    "date": "2025-12-31",
    "timestamp": 0,
    "domain": "Projects",
    "image": "cover.png"
  },
  {
    "objectID": "en_ai_agents_part3",
    "title": "Autopilot - The Creatives: How I Programmed an AI to be Cynical on Twitter and Corporate on LinkedIn",
    "description": "Chapter three of Project Autopilot. Transforming cold data into viral stories by creating two agents with opposing personalities: a cynical influencer and a corporate leader.",
    "content": "In Post 2: The Brain , we achieved something technically important: a Python script capable of reading my technical articles and extracting their essence into a structured JSON. But I have a problem. If I publish a JSON on Twitter, no one is going to read it. Data is cold. Social media is emotional. For this \"Autopilot\" system to work, I don't need more analysts; I need creatives . I need writers who understand the psychology of each platform. Today, we are going to give the machine a soul . We are going to create \"The Creatives.\" The Role Theory ( The Stanislavski Method for AI) Large Language Models (LLMs) like Gemini are, in essence, method actors. If you ask them to \"write a tweet,\" they will give you a generic, boring tweet full of hashtags like #Technology #Innovation. But if you give them a role , a backstory , and a motivation, their behavior changes radically. In prompt engineering, this is the difference between a chatbot and an agent. For Datalaria, I don't want a generic voice. I want to cover two ends of the spectrum: 1. The Chaos (Twitter/X): Brief, direct, slightly cynical, and allergic to corporate speak. 2. The Order (LinkedIn): Professional, inspiring, focused on business value. Designing the Personalities (The Code) Using CrewAI , defining these personalities is as simple (and complex) as writing a biography. Here is the actual code from src/agents.py that defines my two new digital employees. 1. The Tech Influencer (Twitter) I explicitly asked it to hate corporate jargon and use lowercase for aesthetics. [code block] 2. The Thought Leader (LinkedIn) Here we look for the \"Broetry\" style (short sentences with lots of whitespace) that works on LinkedIn. [code block] Refactoring: The Multilingual Challenge Datalaria is a global blog, so I faced a challenge: Do I need to create 4 different agents to write in Spanish and English? The engineering answer is NO . An agent is an entity with a personality; language is just a tool. Instead of duplicating agents, I duplicated the Tasks ( Tasks ) . In src/tasks.py , I now explicitly define the output language: [code block] This makes my pipeline scalable. If tomorrow I want to publish in French, I just add a task, I don't hire a new agent. The Battle of the Agents: Real Results To test this, I used my article on \"S&OP Processes with AI and BPMN.\" It's a dense and boring topic if not sold well. Let's see what the agents did with the same input. The Result on Twitter (The Cynic) Author's note: This result hurt a little; it's more direct than I am. stop drowning in walls of text. manually turning industrial narratives into diagrams is a grind. it‚Äôs a waste of brainpower. we‚Äôre using genai to turn messy s&op docs into precise bpmn diagrams in seconds. here is how you stop being a human translator. üßµ most \"business analysis\" is just expensive friction. genai identifies hidden dependencies and feedback loops that human bas miss. it‚Äôs not just drawing; it‚Äôs uncovering the logic buried under corporate fluff. ProcessEngineering #Industry40 The Result on LinkedIn (The Corporate) Stop drowning in 'walls of text' and PDF graveyards. üß± Industrial processes are the heartbeat of your company. But they are often buried in dense S&OP narratives that no one reads. This creates a massive gap between what the business needs and what engineering builds. I‚Äôve spent years seeing Technical Leads and Ops Managers struggle with this \"translation layer.\" The good news? Generative AI is changing the game. By using AI as a virtual Business Analyst, you can transform complex industrial narratives into precise BPMN diagrams in seconds. It‚Äôs not just about speed. It‚Äôs about clarity. üëá How is your team currently bridging the gap between business narratives and technical execution? Let‚Äôs discuss in the comments. Conclusion The difference is abysmal. The same model (Gemini 1.5 Flash), reading the same article, has generated two pieces of content completely distinct, adapted to the channel and the language. I already have: 1. The Brain that understands the code. 2. The Creatives that write the copy. 3. The files generated on my hard drive. But there is still a \"human\" in the loop. I still have to run python main.py manually and copy-paste these texts onto social media. In the next post, we enter hostile territory. We are going to try to connect these agents with the outside world. Coming soon Post 4: The API Nightmare. I will try to connect my agents to Twitter and LinkedIn and (probably) almost lose my mind in the process. üëâ Source Code: The updated code with the new agents and multilingual support is available in the /autopilot folder of the GitHub repo .",
    "url": "https://datalaria.com/en/posts/ai_agents_part3/",
    "slug": "ai_agents_part3",
    "lang": "en",
    "categories": [
      "Generative AI",
      "Prompt Engineering",
      "Python"
    ],
    "tags": [
      "CrewAI",
      "AI Personality",
      "Automation",
      "Marketing",
      "Storytelling"
    ],
    "date": "2026-01-03",
    "timestamp": 0,
    "domain": "Projects",
    "image": "cover.png"
  },
  {
    "objectID": "en_ai_agents_part4",
    "title": "Autopilot - the API Nightmare: How I Defeated LinkedIn Bureaucracy to Automate My Company",
    "description": "Chapter 4 of Project Autopilot. What was supposed to be a 10-minute script turned into a war of forms. Here is how we unlocked the 'w_organization_social' permission to post as a Company.",
    "content": "Until now, everything was fun. We had AI agents with cynical personalities ( Post 3 ) and a brain capable of analyzing text ( Post 2 ). But everything lived in the safety of my terminal, on localhost . For Datalaria Autopilot to become real, it had to get out into the world. Here is where the project stopped being an engineering problem and became a battle against API Bureaucracy . The Goal: Publishing as a Brand, Not a Person My requirement was clear: I don't want the bot posting on my personal LinkedIn profile. I want it to post on the Datalaria Company Page , with the official logo and a corporate tone. Technically, this requires a change in the API endpoint: * Personal Profile: urn:li:person:12345 * Company Page: urn:li:organization:110125695 It looks like a one-line code change. It ended up being a couple of days of waiting and red tape. Battle 1: Twitter (X) and the Anti-Bot Wall First up, Twitter. Getting API access these days requires passing an audition. I had to apply for the Free Tier and write a \"motivational letter\" explaining that I am not a Russian spam bot, but an AI technical enthusiast. After getting past the 403 Forbidden error (I forgot to enable \"Read & Write\" permissions) and the Duplicate Content error (I tried sending the same \"Hello World\" twice), I achieved connection. Twitter/X was ready, and in principle, everything worked simply. Battle 2: The Final Boss (LinkedIn Company Pages) The biggest problem occurred with LinkedIn. I designed my social_manager.py script to use a company ID if it existed in the environment variables: [code block] When I ran it, the console spat out a blood-red error: ‚ùå Error posting to LinkedIn: Status 403: ACCESS_DENIED The Ghost Permission: w_organization_social I discovered that the standard LinkedIn token only gives you the w_member_social permission (posting as a person). The permission for companies ( w_organization_social ) did not exist in my developer dashboard. To unlock it, I had to complete an administrative scavenger hunt: Page Verification: I had to generate a special URL in the Developer Portal and approve it with my admin account. Result: Company Verified . ‚úÖ Even so, it didn't work: The permission still didn't appear. The Hidden Request: I had to request access to the \"Marketing Developer Platform\" product. The Form: LinkedIn made me fill out a questionnaire detailing that I am a \"Direct Customer,\" that I am not an advertising agency, and that my usage is strictly internal for organic automation. The Victory After a few hours of waiting, the approval email arrived. I re-generated the token and... there it was! With the new \"Super Token\" loaded into my .env , I ran the script one last time. [code block] And the definitive proof on the social network: Conclusion and Next Steps I have achieved what seemed impossible: a Python script that has legal authorization to act as my company. But there is one final problem: This token expires in 60 days. If I do nothing, in two months this whole system will break. Furthermore, I am still running the script manually from my laptop. In the final post of this series, we are going to automate it all. We will use GitHub Actions so the system runs itself every time I upload an article, and (if the API lets us) we will implement automatic token renewal. Coming Up - Post 5: Total Automation (CI/CD). üëâ Source Code: The final social_manager.py module is available in the GitHub repo .",
    "url": "https://datalaria.com/en/posts/ai_agents_part4/",
    "slug": "ai_agents_part4",
    "lang": "en",
    "categories": [
      "Backend",
      "Python",
      "APIs"
    ],
    "tags": [
      "LinkedIn API",
      "Twitter API",
      "OAuth",
      "Automation",
      "DevOps"
    ],
    "date": "2026-01-07",
    "timestamp": 0,
    "domain": "Projects",
    "image": "cover.png"
  },
  {
    "objectID": "en_ai_agents_part5",
    "title": "Autopilot - the Final: From Localhost to the Cloud with GitHub Actions and CI/CD",
    "description": "Final chapter of Project Autopilot. I no longer run scripts on my computer. Now, a simple 'git push' wakes up my AI agents, generates content, and publishes it to social media upon my approval.",
    "content": "We have come a long way. We started by designing a Brain capable of reading ( Post 2 ), we gave it personality with Creative Agents ( Post 3 ), and we fought against bureaucracy to get some Hands (APIs) that could publish legally ( Post 4 ). But there was one last big step left so as not to be a slave to my terminal . Right now, to publish, I had to be at my computer, open the console, and run python main.py . That's not \"Autopilot.\" That's \"Assisted Driving.\" Today, in the final chapter, we cut the cords. We move to the cloud and automate the entire process with my AI agents. The Pipeline Architecture The goal is GitOps : that my only interaction with the system is pushing changes to Git. Everything else must happen by magic (or rather, by GitHub Actions ). I have designed a two-phase workflow: Detection and Preview Phase (Automatic): GitHub detects a new .md file (or changes to an existing one). The Orchestrator activates. The system detects the post language (Spanish/English) and calculates the correct URL. The AI (or template system) proposes a tweet and a LinkedIn post. The system shows me a \"Preview\" in the execution logs, but does not publish anything . Publishing Phase (Manual): The process pauses automatically thanks to GitHub Environments . I receive an alert to review the deployment. If I click the green button ( Approve ), the system executes the real call to the APIs. %%{init: {'theme': 'base', 'themeVariables': { 'primaryColor': '#f0f4f8', 'edgeLabelBackground':'#ffffff', 'tertiaryColor': '#e6e6e6'}}}%% graph TD %% Initial Node START([GitHub detects change in .md]) --> ORC %% --- PHASE 1: AUTOMATIC --- subgraph Phase1 [\"üîπ Phase 1: Detection and Preview (Automatic)\"] direction TB ORC[Orchestrator Activates] %% Parallel Tasks ORC --> TASK1[Detect Language and Calculate URL] ORC --> TASK2[AI proposes Tweet and LinkedIn] %% Convergence TASK1 --> LOGS TASK2 --> LOGS LOGS[Show 'Preview' in Execution Logs] LOGS --> NOPUB[üö´ NOTHING PUBLISHED YET] end NOPUB --> PAUSE %% --- PHASE 2: MANUAL --- subgraph Phase2 [\"üî∏ Phase 2: Publishing (Manual)\"] direction TB PAUSE((‚è∏Ô∏è AUTOMATIC PAUSE<br/>GitHub Environments)) PAUSE --> ALERT[üîî Alert arrives to review deployment] ALERT --> DECISION{Approve Deployment?} %% Approval Path DECISION -- \"Green Button (Approve) ‚úÖ\" --> EXEC[üöÄ Execute real API call] %% Rejection Path (Implicit) DECISION -- \"Reject / Cancel ‚ùå\" --> STOP([End of flow without publishing]) end %% Styles to highlight final steps style EXEC fill:#d4edda,stroke:#28a745,stroke-width:2px,color:#155724 style STOP fill:#f8d7da,stroke:#dc3545,stroke-width:2px,color:#721c24 style PAUSE fill:#fff3cd,stroke:#ffc107,stroke-width:3px The Orchestrator ( orchestrator.py ) I needed a script to join all the pieces together. To do this, I started developing a Python orchestrator that acts as a bridge between the Markdown file and my social media modules. This script is in charge of the \"fine\" logic that we sometimes forget: * Is it a post in English ( /en/ ) or Spanish ( /es/ )? * Does it have a featured image to generate the Twitter/X or LinkedIn card? * Do I want the AI to write it, or do I want to write it myself? The Star Feature: \"Director's Cut\" Sometimes, the AI doesn't get the tone exactly right, or I simply want to write the copy myself for a special announcement. To not lose automation but maintain control, I implemented a \"Manual Overwrite\" logic using Hugo's Frontmatter . If my script detects this in the article header: [code block] The system ignores the automatic generation and uses my exact words. It is the perfect balance: automation by default, manual control when necessary. Security and CI/CD: Sleeping Soundly The .github/workflows/autopilot.yml file is where the magic happens. Here we define the \"Secrets\" (my Twitter and LinkedIn API keys) and the rules of the game. The most interesting part is the environment protection: [code block] By defining the environment as production , GitHub forces me to review and approve the deployment. This prevents a code error or an AI \"hallucination\" from publishing unwanted content. Additionally, we have configured the system so that Twitter/X generates the Cards with images automatically and LinkedIn treats the content as an \"Article,\" ensuring that on both networks the blog's featured image looks large and attractive. The Final Result Now, my publishing process is this: I write my article in Markdown peacefully. I run git push . I have a coffee. ‚òï I check GitHub from my phone, see the \"Preview\" of the generated tweet (in the correct language). I smile and hit Approve . In seconds, the content appears on Twitter and LinkedIn. Without opening the terminal. Without touching Python. From anywhere. Twitter/X Publication LinkedIn Publication Autopilot Project Conclusion What started as an experiment to test how AI agents work and how they can help in my day-to-day life has turned into a professional publishing system. We have touched upon: * Prompt Engineering to define p",
    "url": "https://datalaria.com/en/posts/ai_agents_part5/",
    "slug": "ai_agents_part5",
    "lang": "en",
    "categories": [
      "DevOps",
      "GitHub Actions",
      "Python"
    ],
    "tags": [
      "CI/CD",
      "Automation",
      "Pipeline",
      "GitOps",
      "Workflow"
    ],
    "date": "2026-01-10",
    "timestamp": 0,
    "domain": "Projects",
    "image": "cover.png"
  },
  {
    "objectID": "en_ai_agents_part6",
    "title": "Autopilot - Extra: Coding a Zero-Maintenance, AI-Native Newsletter System",
    "description": "Extra chapter of the Autopilot Project. We implement an automated newsletter system with Brevo that sends personalized emails to subscribers in their preferred language every time I publish a new article.",
    "content": "I thought the Autopilot was complete. Twitter, LinkedIn, Dev.to... everything automated with a simple git push . But I was missing something important: a direct connection with my readers . Social media is great for reach, but algorithms decide who sees my content. With a newsletter , I have control. The email arrives directly in the inbox of those who really want to read me. So I got to work adding this new piece to the puzzle. The Goal I wanted a system that met these requirements: Elegant form integrated into the footer of all pages Bilingual : works perfectly in Spanish and English Language segmentation : each subscriber chooses their preferred language Total automation : when I publish a post, the newsletter is sent automatically Personalized content : emails with the subscriber's name and AI-generated text Choosing the Platform: Brevo After evaluating options like Mailchimp, ConvertKit, and Sendinblue (now Brevo), I chose Brevo for several reasons: Free API for small volumes (300 emails/day) Advanced personalization with content variables Lists and segmentation to manage languages Simple integration with Netlify Functions GDPR compliance Headquarters in France with native GDPR compliance. Solution Architecture The system has three main components: %%{init: {'theme': 'base', 'themeVariables': { 'primaryColor': '#f0f4f8', 'edgeLabelBackground':'#ffffff', 'tertiaryColor': '#e6e6e6'}}}%% flowchart LR subgraph Frontend[\"üåê Frontend (Hugo + Netlify)\"] FORM[Footer Form] FUNC[Netlify Function] end subgraph Backend[\"ü§ñ Backend (Autopilot)\"] ORC[Orchestrator] BRAIN[AI Newsletter Agent] MGR[Newsletter Manager] end subgraph Brevo[\"üìß Brevo\"] LIST_ES[ES List #3] LIST_EN[EN List #4] CAMP[Campaigns] end FORM -->|\"POST + language\"| FUNC FUNC -->|\"API Contacts\"| LIST_ES FUNC -->|\"API Contacts\"| LIST_EN ORC -->|\"New post\"| BRAIN BRAIN -->|\"AI Content\"| MGR MGR -->|\"API Campaigns\"| CAMP CAMP -->|\"Email ES\"| LIST_ES CAMP -->|\"Email EN\"| LIST_EN Part 1: The Subscription Form Footer Design The form lives in layouts/partials/extend_footer.html , a Hugo partial that is automatically injected across the entire website. I designed it to adapt to the page language: [code block] The language selector is key: it defaults to the page language , but users can change it if they prefer to receive emails in another language. Netlify Function: The Bridge to Brevo The form sends data to a Netlify Function that handles communication with the Brevo API: [code block] The most important thing here is language segmentation : - Subscribers who choose ES go to List #3 - Those who choose EN go to List #4 Part 2: The Newsletter Agent For the email content, I created a new \"Agent\" in the Autopilot's AI system. This agent has a different personality from the Twitter or LinkedIn ones: [code block] The key difference from social media agents is the personal tone . An email is a one-on-one conversation, not a post for the masses. The agent doesn't just translate. If the post is in Spanish, the 'brain' switches context to generate culturally relevant copy for the Hispanic audience, not just a translation Part 3: The Newsletter Manager This is the piece that orchestrates all the sending. It handles: Generating the HTML email with professional design Personalizing content with the subscriber's name Creating and sending the campaign to the correct list [code block] The HTML Template The email has a clean design with dynamic personalization: [code block] The {{ contact.FIRSTNAME }} variable is Brevo magic: it automatically replaces with each subscriber's name. Part 4: Integration with the Orchestrator The final step was connecting all this with the existing Autopilot flow: [code block] The logic is simple but powerful: - If I publish a post in /es/posts/ ‚Üí it's sent to the ES List - If I publish in /en/posts/ ‚Üí it's sent to the EN List No mixing or automatic translations. Each audience receives content in their language, about posts written for them. The Result After all this implementation, my workflow became: Write an article in /content/es/posts/new-article/ Do git push GitHub Actions detects the change The Autopilot: Generates tweets and LinkedIn posts (as before) NEW : Generates personalized newsletter content Creates a campaign in Brevo Sends the email to all subscribers in the ES List Email received by subscribers: The email includes: - ‚úÖ Personalized greeting with name - ‚úÖ AI-generated content with subtle emojis - ‚úÖ Gradient call-to-action button - ‚úÖ Datalaria logo - ‚úÖ Unsubscribe link GitHub Actions Configuration For everything to work in production, I added the following variables to the workflow: [code block] Lessons Learned 1. Brevo doesn't support inline filters Initially I tried using segmentConditions to filter by LANGUAGE attribute directly in the API call. It doesn't work. Brevo requires predefined lists or saved segments . The solution was to create two separate lists. 2. Line breaks matter The AI-generated content came as plain text. I had",
    "url": "https://datalaria.com/en/posts/ai_agents_part6/",
    "slug": "ai_agents_part6",
    "lang": "en",
    "categories": [
      "DevOps",
      "Python",
      "Email Marketing"
    ],
    "tags": [
      "Newsletter",
      "Brevo",
      "API",
      "Automation",
      "Hugo",
      "Netlify Functions"
    ],
    "date": "2026-01-24",
    "timestamp": 0,
    "domain": "Projects",
    "image": "cover.png"
  },
  {
    "objectID": "en_ai_agents_part7",
    "title": "Autopilot - Ctrl: AI Content Auditing with GitHub Copilot CLI",
    "description": "I built autopilot-ctrl, a CLI that uses GitHub Copilot CLI to automatically audit and improve AI-generated content before publishing to social media.",
    "content": "This is a submission for the GitHub Copilot CLI Challenge What I Built autopilot-ctrl is a command-line tool that audits AI-generated social media content before publishing. Think of it as a \"quality gate\" for your content pipeline. The Problem My blog has an autopilot system that automatically generates posts for Twitter, LinkedIn, and Newsletter every time I publish an article. It works great... most of the time. But sometimes the AI produces: üê¶ Generic tweets without hooks üíº LinkedIn posts without proper structure üìß Newsletter intros that reveal too much (or too little) I needed a way to evaluate quality BEFORE publishing and, if something doesn't pass, improve it automatically. The Solution autopilot-ctrl uses GitHub Copilot CLI to: Audit content against platform-specific criteria Assign a quality score (0-10) Identify specific issues Generate improved versions of failing content [code block] Demo Available commands: [code block] Screenshots of the flow: Source code: github.com/Dalaez/datalaria/autopilot/ctrl My Experience with GitHub Copilot CLI üöÄ How I Used Copilot CLI The magic of autopilot-ctrl lies in how it integrates Copilot CLI in non-interactive mode: [code block] Each audit sends a structured prompt to Copilot CLI and parses the natural language response to extract: - Numeric score (e.g., \"Rating: 7/10\") - List of issues (e.g., \"No engagement\", \"Generic hook\") - Improvement suggestions üí° What I Learned Flag order matters : -p MUST be the last argument Simple prompts work better : Long, structured prompts in non-interactive mode return empty responses Copilot responds in natural language : I had to create flexible parsers to extract data from responses like \" Rating: 7/10 \" ‚ö° The Impact on My Workflow Before autopilot-ctrl, I manually reviewed every generated post. Now: git push ‚Üí Autopilot generates content python -m ctrl audit generated_content.json ‚Üí Copilot evaluates If something fails ‚Üí python -m ctrl fix generates improvements Approved content ‚Üí Gets published automatically Time saved : ~15 minutes per publication. üõ†Ô∏è Tech Stack Python + Click : CLI framework Rich : Terminal UI with tables and colors GitHub Copilot CLI : AI evaluation engine YAML configs : Customizable prompts per platform Conclusion autopilot-ctrl demonstrates that GitHub Copilot CLI isn't just for generating code. It's a powerful tool for integrating AI into any pipeline - in this case, content quality evaluation. If you have a system that generates content automatically, consider adding a \"quality gate\" with Copilot CLI. Your audience (and your engagement metrics) will thank you. Questions? Drop them in the comments üëá This post is part of the Autopilot Project series, where I document how I automate content creation and publishing using AI.",
    "url": "https://datalaria.com/en/posts/ai_agents_part7/",
    "slug": "ai_agents_part7",
    "lang": "en",
    "categories": [
      "DevOps",
      "Python",
      "AI"
    ],
    "tags": [
      "devchallenge",
      "githubchallenge",
      "cli",
      "githubcopilot",
      "Copilot CLI",
      "Content Audit",
      "Automation"
    ],
    "date": "2026-02-01",
    "timestamp": 0,
    "domain": "Projects",
    "image": "cover.png"
  },
  {
    "objectID": "en_app_flashcards",
    "title": "AI-Powered Programming: Creating My Own Magical Flashcards Study App",
    "description": "A practical case study of how a family need led me to create a flashcards study application from scratch, relying on AI to make learning more enjoyable and effective.",
    "content": "Technology truly shines when it solves a real problem, no matter how small. A few days ago, I found myself in a situation many parents will recognize: helping my oldest son review a science lesson. The added difficulty was that the subject is bilingual, so he not only had to memorize terms like \"joints\" or \"spinal column\" but also their English translation and phonetics. Our initial method was the classic one: the ancient art of \"covering the answer with your hand\" to guess. It was functional, but monotonous, unmotivating, and not very effective. As we struggled to stay focused, an idea struck me: what if instead of fighting distraction, we combat it with a better tool? Could I, with the help of AI, create a small, custom study application in a matter of minutes? This post is the result of that experiment‚Äîthe creation of a custom study app with flashcards . Let's look at the initial specifications, the creation process, and, most importantly, the final result ready for use. Beyond Anki and Quizlet: The Search for Custom Simplicity Incredibly powerful study tools like Quizlet, AnkiApp, or ProProfs already exist. They are fantastic and offer a multitude of possibilities. However, they often come with a learning curve or a number of options that can be overwhelming for an immediate and specific need. Primarily, I didn't need a social ecosystem for the required concepts, nor multiple spaced repetition methods. The priority was to have a quick solution with very specific requirements: Flexible data entry : The ability to create lists of terms manually, but also the option to generate translations or definitions automatically. Simple gamification : Adding a game-like element with points and images to keep a child engaged. Bilingual focus : Making it easy to review terms in multiple languages. No distractions : A clean, straightforward interface. With these goals in mind, and relying on the same \"AI copilot\" techniques I've explored in other posts, the \"Flashcards de Estudio\" app was born. The Solution: Introducing the \"Flashcards de Estudio\" App The application is designed to be minimalist yet powerful, offering three ways to start studying in seconds: 1. Manual Mode: Total Control This is the most direct method. It allows you to add \"Term\" and \"Definition\" rows one by one. It's perfect for short review lists or when you already have the material prepared and just want to quickly digitize it to start studying. 2. Automatic Mode (AI): The Magic Touch This is where the magic of AI comes into play. In this mode, you just need to type a term, and the AI automatically generates the definition or translation in the language you choose. For my son's science lesson, I simply entered the list of words in Spanish, and the AI instantly completed their English translation and an approximation of their phonetics. It's a spectacular time-saver. To use it, you need to generate a free Key at Google AI Studio and enter it in the \"Your Gemini API Key\" field to use the Gemini AI engine for content generation. The application supports translations and definition generation in several languages: 3. Import from File: For Power Users For longer lists (vocabulary for an entire topic, lists of capitals, etc.), the application allows you to upload a text ( .txt ) or CSV ( .csv ) file with the terms and definitions. You simply prepare your list in a file, upload it, and the application generates the cards instantly. This feature is ideal for those who want to prepare material for their children or for students who need to digitize entire subjects. Flashcards Template How It Works: Easy and Fun Once the terms and the mode of operation have been selected, it's simply a matter of starting to study. During the session, cards will be displayed for the user to practice the translations or terms before flipping them and indicating whether they got it right or wrong. If correct, the application will add 2 points and a congratulatory emoji and message will appear; if wrong, it will not add points and an encouraging message will appear. In these cases, the mistakes are recorded so that at the end of the session, they can be reviewed again until we are sure we have mastered them. If all answers are successfully correct, an animated celebration gif will be displayed at the end, and from this screen, we can either study all the terms again or return to the home screen to generate new content. A Living and Community-Open Project What started as a solution for an afternoon of studying has become a personal project that I plan to continue improving. \"Flashcards de Estudio\" is a living project . My intention is to gradually add new features, such as different game modes, progress tracking over time, or the ability to share card decks. I will chronicle these improvements in future blog posts. Also, once the comment feature is active on Datalaria, I will be happy to gather your ideas and suggestions to make this an even better learning tool for everyone. Con",
    "url": "https://datalaria.com/en/posts/app_flashcards/",
    "slug": "app_flashcards",
    "lang": "en",
    "categories": [
      "Projects",
      "Tools"
    ],
    "tags": [
      "ai",
      "flashcards",
      "learning",
      "study",
      "web development",
      "no-code",
      "gamification"
    ],
    "date": "2025-10-17",
    "timestamp": 0,
    "domain": "Projects",
    "image": "Cover.PNG"
  },
  {
    "objectID": "en_app_openweather_part1_backend",
    "title": "Weather Service Project (Part 1): Building the Data Collector with Python and GitHub Actions or Netlify",
    "description": "First installment in the series on building a weather service. We focus on the backend: connecting to the OpenWeatherMap API, storing data in CSV, and automating everything 24/7 for free with GitHub Actions or Netlify.",
    "content": "As I mentioned in a previous post, one of my goals with Datalaria is to get my hands dirty with projects that allow me to learn and connect different technologies in the data world. Today, we begin a series dedicated to one of those projects: the creation of a complete global weather service , from data collection to visualization and prediction, all serverless and using free tools. In this first installment, we will focus on the heart of the system: the backend data collector . We'll see how to build a \"robot\" that works for us 24/7, connecting to an external API, saving structured information, and doing all this automatically and for free. Let's dive in! The First Step: Talking to the OpenWeatherMap API Every weather service needs a data source. I chose OpenWeatherMap for its popularity and generous free plan. The initial process is straightforward: Register : Create an account on their website. Get the API Key : Generate a unique key that will identify us in each call. It's like our \"key\" to access their data. Store the Key : Never directly in the code! We'll discuss this further below. With the key in hand (or almost!), I wrote a first test_clima.py script to test the connection using Python's fantastic requests library: [code block] First Obstacle Overcome (with Patience): When I first ran it, I got a 401 Unauthorized error! üò± It turns out that OpenWeatherMap API Keys can take a few hours to activate after being generated. The lesson: sometimes, the solution is simply to wait. ‚è≥ The \"Database\": Why CSV and Not SQL? With data flowing, I needed to store it. I could have set up an SQL database (PostgreSQL, MySQL...), but that would involve complexity, a server (cost), and for this project, it was overkill. I opted for radical simplicity: CSV (Comma Separated Values) files . Advantages : Easy to read and write with Python, perfectly versionable with Git (we can track changes), and sufficient for the initial data volume we'd be handling. Key Logic : I needed to append a new row to each city's file daily, but only write the header ( date_time , city , temperature_c , etc.) the first time. Python's native csv library and os.path.exists make this trivial: [code block] The Automation Robot: GitHub Actions to the Rescue ü§ñ Here comes the magic: how to make this script run daily without having a server constantly on? The answer is GitHub Actions , the automation engine integrated into GitHub. It's like having a small robot working for us for free. Security First: Never Upload Your API Key! The biggest mistake would be to upload registrar_clima.py with the API_KEY written directly in the code. Anyone could see it on GitHub. Solution : Use GitHub's Repository Secrets . Go to Settings > Secrets and variables > Actions in your GitHub repository. Create a new secret named OPENWEATHER_API_KEY and paste your key there. In your Python script, read the key securely using os.environ.get(\"OPENWEATHER_API_KEY\") . The Robot's Brain: The .github/workflows/update-weather.yml File This YAML file tells GitHub Actions what to do and when: [code block] This last step is crucial! The Action itself acts as a user, performing git add , git commit , and git push of the CSV files that the Python script has just modified. This way, the updated data is saved in our repository every day. The Serverless Alternative: Deployment and Automation with Netlify üöÄ While GitHub Actions is a fantastic automation tool, for this project I decided to explore an alternative even more integrated with the \"serverless\" concept: Netlify . Netlify not only allows us to deploy our static frontend (like GitHub Pages) but also offers serverless functions and, crucially for our backend, scheduled functions (or Cron Jobs) . Deploying the Static Frontend with Netlify Connect Your Repository : The process is incredibly simple. Log in to Netlify, click \"Add new site,\" and select \"Import an existing project.\" Connect with your GitHub account and choose your Weather Service project repository. Basic Configuration : Netlify will automatically detect your project. Ensure that the \"Build command\" is empty (as it's a static site with no build process) and that the \"Publish directory\" is the root of your repository ( ./ ). Continuous Deployment : Netlify will automatically configure continuous deployment. Every time you git push to your main branch (or whichever branch you've configured), Netlify will rebuild and deploy your site. Automating the Backend with Netlify Functions (and Cron Jobs) This is where Netlify Serverless Functions shine for our data collector. Instead of a GitHub Actions workflow, we can use a Netlify function to run our Python script on a schedule: Project Structure : Create a netlify/functions/ folder at the root of your project. Inside, you can have a Python file like collect_weather.py . Dependency Management : You'll need a requirements.txt file at the root of your project for Netlify to install Python dependencies ( requests , pandas , scikit-learn ",
    "url": "https://datalaria.com/en/posts/app_openweather_part1_backend/",
    "slug": "app_openweather_part1_backend",
    "lang": "en",
    "categories": [
      "Projects",
      "Tools"
    ],
    "tags": [
      "python",
      "api",
      "github actions",
      "automation",
      "serverless",
      "data",
      "backend",
      "netlify"
    ],
    "date": "2025-10-31",
    "timestamp": 0,
    "domain": "Projects",
    "image": "cover.png"
  },
  {
    "objectID": "en_app_openweather_part2_frontend",
    "title": "Weather Service Project (Part 2): Building the Interactive Frontend with GitHub Pages or Netlify and JavaScript",
    "description": "Second installment of the Weather Service project. We dive into the frontend: serving a dynamic dashboard with GitHub Pages or Netlify, reading CSV data with PapaParse.js, and creating interactive charts with Chart.js.",
    "content": "In the first part of this series , we laid the groundwork for our global weather service. We built a Python script to fetch weather data from OpenWeatherMap, efficiently stored it in city-specific CSV files, and automated the entire collection process using GitHub Actions. Our \"robot\" is diligently gathering data 24/7. But what good is data if you can't see it? Today, we shift our focus to the frontend : building an interactive, user-friendly dashboard that allows anyone to explore the weather data we've collected. We'll leverage the power of static site hosting with GitHub Pages or Netlify , use \"vanilla\" JavaScript to bring it to life, and rely on some excellent libraries for data handling and visualization. Let's make our data shine! Free Web Hosting: GitHub Pages vs. Netlify The first hurdle for any web project is hosting. Traditional servers can be costly and complex to manage. Following our \"serverless and free\" philosophy, both GitHub Pages and Netlify are perfect solutions for hosting static websites directly from your GitHub repository. Option 1: GitHub Pages GitHub Pages allows you to host static websites directly from your GitHub repository. Activation is trivial: 1. Go to Settings > Pages in your repository. 2. Select your main branch (or the branch containing your web content) as the source. 3. Choose the /root folder (or a /docs folder if you prefer) as the location of your web files. 4. Click Save . And just like that, your index.html file (and any linked assets) becomes publicly accessible at a URL like https://your-username.github.io/your-repository-name/ . Simple, effective, and free! üöÄ Option 2: Netlify (the final choice for this project!) For this project, I ultimately opted for Netlify due to its flexibility, ease of managing custom domains, and integrated continuous deployment. It also allows me to host the project directly under my Datalaria domain ( https://datalaria.com/apps/weather/ ). Steps to deploy on Netlify: Connect Your Repository : Log in to Netlify. Click \"Add new site\" then \"Import an existing project\". Connect your GitHub account and select your Weather Service project repository. Deployment Configuration : Owner : Your GitHub account. Branch to deploy : main (or the branch where your frontend code resides). Base directory : Leave this empty if your index.html and assets are in the root of the repository, or specify a subfolder if applicable (e.g., /frontend ). Build command : Leave it empty, as our frontend is purely static with no build step required (no frameworks like React/Vue). Publish directory : . (or the subfolder containing your static files, e.g., /frontend ). Deploy Site : Click \"Deploy site\". Netlify will fetch your repository, deploy it, and provide you with a random URL. Custom Domain (Optional but recommended) : To use a domain like datalaria.com/apps/weather/ : Go to Site settings > Domain management > Domains > Add a custom domain . Follow the steps to add your domain and configure it with your provider's DNS (by adding CNAME or A records). For the specific path ( /apps/weather/ ), you would typically configure a \"subfolder\" or \"base URL\" within your application if it's not directly at the root of the domain. In this case, our index.html is designed to be served from a subpath. Netlify handles this transparently once the site is deployed and your domain is configured. It's that simple! Each git push to your configured branch will trigger a new deployment on Netlify, keeping your dashboard always up-to-date. The Frontend Tech Stack: HTML, CSS, and JavaScript (with a little help) For this dashboard, I opted for a lightweight approach: plain HTML for structure, a bit of CSS for styling, and \"vanilla\" JavaScript (without complex frameworks) for interactivity. To handle specific tasks, I incorporated two fantastic libraries: PapaParse.js : The fastest in-browser CSV parser for JavaScript. It's the bridge between our raw CSV files and the JavaScript data structures we need for visualization. Chart.js : A powerful and flexible JavaScript charting library that makes creating beautiful, responsive, and interactive charts incredibly easy. The Dashboard Logic: Bringing Data to Life in index.html Our index.html acts as the main canvas, orchestrating the fetching, parsing, and rendering of weather data. 1. Dynamic City Loading In stead of hardcoding a list of cities, we want our dashboard to automatically update if we add new cities in the backend. We achieve this by fetching a simple ciudades.txt file (containing one city name per line) and dynamically populating a <select> dropdown element using JavaScript's fetch API. [code block] 2. Reacting to User Selection When a user selects a city from the dropdown, we need to respond immediately. An addEventListener on the <select> element detects the change event and calls our main function to fetch and draw the data for the newly selected city. [code block] 3. Fetching, Parsing, and Drawing Data This is the central functi",
    "url": "https://datalaria.com/en/posts/app_openweather_part2_frontend/",
    "slug": "app_openweather_part2_frontend",
    "lang": "en",
    "categories": [
      "Projects",
      "Tools"
    ],
    "tags": [
      "javascript",
      "frontend",
      "github pages",
      "html",
      "css",
      "papaparse",
      "chartjs",
      "serverless",
      "data-visualization",
      "netlify"
    ],
    "date": "2025-11-08",
    "timestamp": 0,
    "domain": "Projects",
    "image": "cover.png"
  },
  {
    "objectID": "en_app_openweather_part3_ai_prediction",
    "title": "Weather Service Project (Part 3): Predicting the Future with AI and OpenWeatherMap",
    "description": "The final installment of our Weather Service project. We dive into adding predictive capabilities, combining official OpenWeatherMap forecasts with our custom-built AI (Linear Regression) model to predict tomorrow's weather and visualize its accuracy.",
    "content": "In the first part of this series , we set up the backbone of our global weather service, collecting raw data using Python and GitHub Actions. Then, in Part 2 , we transformed that raw data into a beautiful, interactive dashboard, leveraging GitHub Pages/Netlify, JavaScript, PapaParse.js, and Chart.js. Now, it's time for the grand finale: adding predictive power to our Weather Service. We'll explore how to augment our historical data visualization with actual forecasts. This installment focuses on a dual approach: integrating an official, reliable forecast from a professional service (OpenWeatherMap) and, more excitingly, building and training our very own simple AI model (Linear Regression) to predict tomorrow's weather based on the historical data we've meticulously collected. Finally, we'll visualize both forecasts on our dashboard, allowing for a direct comparison and a real-world test of our AI's accuracy. Let's turn our data into a crystal ball! üîÆ The Predictive Core: OpenWeatherMap and Our Custom AI The goal for this predictive functionality was twofold: Official Forecast : Obtain a reliable, multi-day forecast from a professional weather service (OpenWeatherMap - OWM). Custom AI Prediction : Create our own simple AI model (Linear Regression) trained on the historical data we've collected, to predict the next day's weather. Visualization & Comparison : Display and compare both forecasts to gauge the accuracy and performance of our custom AI model. 1. ‚öôÔ∏è Backend Logic: read_weather.py Gets Smarter Our read_weather.py script, previously responsible for data collection, now expands its role to gather data from both OWM and our historical archives, consolidating everything into a single predicciones.json file. Step 1: Fetching OpenWeatherMap's 5-Day Forecast We decided that in addition to our 1-day AI prediction, a 5-day forecast from OWM would provide valuable context. API Endpoint : We opted for the free data/2.5/forecast API (since OneCall 3.0 required a payment method). Data Processing : This API returns data in 3-hour blocks. We had to add Python logic to: Iterate over the list of ~40 forecasts. Group them by day (ignoring the current day). For each of the next 5 days, calculate the maximum, minimum, and average temperature from all 3-hour blocks within that day. Result : A list of 5 objects (one per day) containing OWM's max, min, and average temperature predictions. Step 2: Implementing Our AI Model (1-Day Prediction) This is the core of our \"homemade AI.\" For each city: Data Loading : We used pandas to read the city's historical CSV file (e.g., datos/Madrid.csv ). Feature Engineering : Since we had multiple readings per day, the most crucial step was transforming this data: Resampling : We used df.resample('D') from pandas to group data by day, calculating the actual daily aggregates (e.g., temp_max , temp_min , avg_temp , avg_humidity ). Feature Creation (X) : We created new \"shifted\" columns ( .shift(1) ) so that each row (representing a day) contained the previous day's data (e.g., temp_max_lag1 , avg_humidity_lag1 ). We also added day_of_year to capture seasonality. Target Creation (y) : We defined what we wanted to predict (e.g., the actual temp_max of the current day). Training 3 Models : Instead of one, we trained three independent Linear Regression models ( scikit-learn ): model_max : Trained with y = df_clean['temp_max'] . model_min : Trained with y = df_clean['temp_min'] . model_avg : Trained with y = df_clean['avg_temp'] . Prediction : We took the last row of aggregated data (representing \"today's\" data). Fed this data to the 3 models to predict \"tomorrow's\" values. We included a safeguard ( MIN_RECORDS_FOR_IA = 10 ) so the model only attempts to predict if it has sufficient historical data (e.g., 10 clean days). Step 3: Consolidate and Save The script combines the results from Steps 1 and 2 into a JSON structure and saves it to predicciones.json : [code block] 2. üé® Frontend Logic: index.html Visualizes the Future The frontend is responsible for loading this predicciones.json file and presenting it in a visually appealing and informative way. Step 1: Data Loading loadPredictions() : We created a new async function that runs once during initialization (before updateDashboard ). allPredictionsCache : This function loads predicciones.json and saves it into this new global variable so that all visualization functions have access to it. Step 2: Visualization in the \"Super-Cards\" (KPIs) We wanted a direct and clear comparison. OWM 5-Day Forecast : We created a helper function buildForecastHTML() . This function takes the pred_owm_5day list and generates an HTML block with a list of the 5 days and their max/min temperatures (e.g., \"Sat, Nov 9: 15.1¬∞C / 10.0¬∞C\"). AI 1-Day Forecast (Comparison) : We created a second helper function buildIAForecastHTML() . This function takes the pred_ia object and the first day of the OWM forecast ( pred_owm_5day[0] ). Comparison Logic : For max, min, and averag",
    "url": "https://datalaria.com/en/posts/app_openweather_part3_ai_prediction/",
    "slug": "app_openweather_part3_ai_prediction",
    "lang": "en",
    "categories": [
      "Projects",
      "AI",
      "Tools"
    ],
    "tags": [
      "machine-learning",
      "regression",
      "openweathermpa",
      "python",
      "pandas",
      "scikit-learn",
      "javascript",
      "frontend",
      "data-prediction",
      "weather-forecast",
      "serverless"
    ],
    "date": "2025-11-15",
    "timestamp": 0,
    "domain": "S&OP",
    "image": "cover.png"
  },
  {
    "objectID": "en_app_openweather_part4_extras_ux",
    "title": "Project Weather (Extras): Beyond AI - Building a Robust, User-Centric Dashboard",
    "description": "An in-depth analysis of the 'extra' features and design decisions behind Project Weather, showcasing how advanced data collection, internationalization, custom filtering, and UI/UX enhancements transformed a basic dashboard into a comprehensive, robust, and user-friendly weather application, complementing its core AI prediction capabilities.",
    "content": "In previous posts, we explored the core of the Project Weather , focusing on its basic backend-frontend platform and its AI prediction capabilities. Once these points were achieved and the application was operational, certain improvement aspects arose concerning a more fluid and intuitive frontend experience. This post unveils the \"extras\"‚Äîthe significant enhancements and design decisions that transformed Project Weather from a basic pilot into a comprehensive, production-ready weather dashboard. These improvements, although not directly related to AI prediction, were crucial for building a reliable, scalable, and delightful user experience. 1. Expanding Metrics and Data Collection: From Basic to Comprehensive Our initial weather dashboard, while functional, only provided basic metrics like temperature, wind, and humidity. To evolve into a truly useful weather station, we needed more data. This required significant modifications in both our backend data collection script and the frontend visualization. New Metrics & Dynamic Visualizations We revamped our Python read_weather.py script to fetch and store four new critical weather variables: Cloudiness (%): Percentage of sky covered. Visibility (km): Distance of vision (crucial for fog or haze conditions). Rain (mm): Rainfall in the last hour. Snow (mm): Snowfall in the last hour. In the frontend, effectively visualizing these diverse data types was key. We implemented conditional logic within Chart.js to adapt the chart type based on the data: Line charts for continuous variables like temperature and wind speed. Area (filled) charts for humidity and cloudiness, providing a sense of accumulation or coverage. Bar charts for precipitation (rain/snow), as bars visually represent accumulated amounts more intuitively. Automatic Data Migration A common challenge when adding new data fields to an existing system is managing historical data. We designed a robust backend system to automatically detect and migrate older CSV files (with 11 columns) to the new 15-column format. Crucially, this system elegantly handled the missing values for the new metrics, filling them with sensible default values (e.g., 0 for rain/snow), thus preventing any loss of historical context and ensuring data integrity across the entire dataset. This automated process was vital for a smooth, fluid transition without requiring manual intervention. 2. Comprehensive Internationalization (i18n) System: A Truly Global Application To make the Project Weather accessible to a wider audience, a complete internationalization (i18n) system was a top priority. We transformed the web application into a fully bilingual platform (Spanish/English) with dynamic language switching, without the need for page reloads. Translation Architecture We built a centralized const translations dictionary in JavaScript, containing all text strings used in the application. This approach ensured consistency and simplified maintenance. Static Texts: HTML elements requiring translation were tagged with a data-i18n-key attribute. A JavaScript function then automatically iterated through these elements, replacing their content with the corresponding translation from the active dictionary. Dynamic Texts: Crucially, all JavaScript logic responsible for generating dynamic strings (e.g., \"Feels like,\" \"Wind speed,\" KPI labels, and chart tooltips) was refactored to read directly from the currently selected language dictionary. This ensured that every piece of text, regardless of whether it was static HTML or dynamically generated, was correctly localized. Date and Number Formatting Beyond text, cultural formatting for dates and numbers is essential. We leveraged Intl.DateTimeFormat and integrated it with Chart.js to ensure that dates on chart axes and in data cards were displayed in the culturally correct format (e.g., \"10 nov\" vs. \"Nov 10\"). Number formats (e.g., decimal separators) were also adapted accordingly. Persistence To enhance user experience, the language preference is stored in localStorage , so the application remembers the user's chosen language across visits. 3. Advanced Interactivity: Custom Time Range Filtering Empowering users with precise data exploration capabilities was a key objective. We significantly improved the dashboard's interactivity by introducing advanced time range filtering. Custom Range Selector Alongside the predefined ranges (1H, 1D, 1W, 1M, 1Y, All), we added a \"Custom\" option. Activating this option dynamically reveals two date input fields ( start-date and end-date ). Dynamic Filtering Logic The core rendering engine for our charts was modified to accept arbitrary start and end dates. This allows users to define any time window. Upon selection, the system recalculates all statistics (maximums, minimums, and averages) and re-renders the graphs based exclusively on the user-defined period. This granular control dramatically improves the analytical utility of the dashboard. 4. UI/UX Enhancements: Pol",
    "url": "https://datalaria.com/en/posts/app_openweather_part4_extras_ux/",
    "slug": "app_openweather_part4_extras_ux",
    "lang": "en",
    "categories": [
      "Project Showcase",
      "Web Development",
      "UI/UX"
    ],
    "tags": [
      "project-weather",
      "weather-dashboard",
      "python",
      "javascript",
      "frontend",
      "backend",
      "i18n",
      "data-visualization",
      "chart.js",
      "ui-ux"
    ],
    "date": "2025-12-06",
    "timestamp": 0,
    "domain": "Projects",
    "image": "cover.png"
  },
  {
    "objectID": "en_app_unit_converter",
    "title": "Building My Digital 'Swiss Army Knife': A Custom Unit Converter with AI",
    "description": "A practical case study of how I used Gemini Canvas to develop a multi-category unit conversion web application from scratch, tailored to my professional needs.",
    "content": "In any technical or engineering discipline, our daily work is filled with small but constant unit conversion tasks. We switch from Pascals to PSI, Kilowatts to Horsepower, Gigabytes to Terabytes, or from frequency to wavelength. In my university days, we had so-called cheatsheets , which we used to keep handy on our calculators, folders, or notebooks for daily use (not for exams, of course üòÑ). Now, in the professional world, with the proliferation of all kinds of apps and websites, it's no longer necessary to rely on these resources, as we have a multitude of options available that allow us to quickly look up any conversion or unit, no matter how obscure. However, given the overwhelming number of options and this being such a recurrent and necessary task, I set myself a challenge: could I create my own unit converter? One that was fast, clean, visually appealing, and, above all, a living project ‚Äîa kind of \"digital Swiss army knife\" that I could expand over time according to my personal and professional needs. The answer, as in other blog projects, I found in AI. This post describes my own unit converter, covering its development from scratch with the support of Gemini's Canvas feature to create a web application with multiple pages and categories. The Process: From Idea to Conception The goal was to create a modern, intuitive web application with clear navigation by categories, and that was smooth and easy to use. Instead of starting to write code from scratch, I began a conversation with Gemini, describing my vision: \"Create the structure for a multi-page HTML web application. The application will be a format and unit converter. The main page should display cards for the categories: Dimension, Energy, Time, Mechanics, Computing, and Radio Frequency with basic conversions. Each card should link to a dedicated conversion page for the corresponding category from a homepage where, in addition to the navigation cards for each category, there should be a format converter for the '.' and ',' symbols for thousands and decimals in numbers.\" Gemini's Canvas feature allowed me to see in real-time how the AI generated not just the code, but the complete project structure. After the initial draft and a couple of additional iterations focused on refining the design and fine-tuning the units, a first operational version of the application came to life: My unit conversion application A Look at the Categories and Their Units What makes a tool like this useful is understanding what we are converting . That's why the \"Universal Converter Pro\" not only calculates but also aims to be educational. These are the initial categories and the science behind their units: 1. Dimension: Measuring the Space Around Us This category groups the fundamental measures of physical space. Length: What is it? It is the measure of one dimension, the distance between two points. How is it calculated? All conversions are calculated using the meter (m) as the reference unit. The formula converts the initial value to meters and then to the final unit. Area: What is it? It is the measure of a two-dimensional surface. How is it calculated? Similarly, the calculation is standardized using the square meter (m¬≤) as the base unit. Volume: What is it? It is the measure of the space an object occupies in three dimensions. How is it calculated? The base unit for volume in the application is the liter (l) , making it easy to convert between metric units and others like gallons or cups. 2. Energy: The Capacity to Do Work Here we group the units that describe how energy is transferred and used. Energy: What is it? It is the capacity of a system to perform work. How is it calculated? The Joule (J) is the base unit of the International System. From it, conversions to calories, watt-hours, etc., are performed. Power: What is it? It is the rate at which energy is transferred or work is done. It's not the same to have energy as to be able to use it quickly. How is it calculated? The base unit is the Watt (W) , which is equivalent to one Joule per second. Temperature: What is it? It is a measure of the thermal energy or heat of a body. How is it calculated? Unlike others, temperature does not use a simple conversion factor. The application uses specific formulas, always converting the input to degrees Celsius (¬∞C) as an intermediate step to then calculate the output unit (Fahrenheit or Kelvin). 3. Computing: The World of Bits and Bytes The units that define our digital world. Data Storage: What is it? It measures the capacity to store digital information. How is it calculated? The fundamental unit is the Byte (B) . It is important to note that in computing, multiples are not decimal (x1000), but binary (x1024). Thus, 1 Kilobyte is 1024 Bytes. Bandwidth: What is it? It measures the data transfer rate in a network. How is it calculated? Its base unit is bits per second (bps) . In this case, the multiples are decimal (kbps, mbps, gbps), as they refer to transmission spe",
    "url": "https://datalaria.com/en/posts/app_unit_converter/",
    "slug": "app_unit_converter",
    "lang": "en",
    "categories": [
      "Projects",
      "Tools"
    ],
    "tags": [
      "gemini",
      "ai",
      "web development",
      "unit converter",
      "html",
      "css",
      "javascript",
      "engineering"
    ],
    "date": "2025-10-04",
    "timestamp": 0,
    "domain": "Projects",
    "image": "cover.png"
  },
  {
    "objectID": "en_Basic-Visualizations",
    "title": "Data Visualization - Basics",
    "description": "Learn about the most common basic visualizations used to analyze your data",
    "content": "Building on the concepts from our post on \"Descriptive Statistics,\" we will now explore the most common charts for data visualization: \"Descriptive Statistics\" , below, we will know the most basic ways to visualize the data or information to be analyzed. Specifically: Pie chart Line chart Treemap & Subburst Histograms & Bar charts Density plot Boxplots Violin plots Scatter plots Pareto chart Note: All the examples (except for the line graphs due to its features) are based on the train.csv dataset file, which is a subset of 891 representative individuals of the population of the Titanic used for the training of models of Machine Learning. Pie charts Pie charts are useful to represent the numerical proportions of the values of the data of a variable or feature with respect to its total. This type of graphs present a vast catalogue of variants and are widely used due to their simplicity of preparation and interpretation. However, these charts are often overused and misused. For more on this topic, see this excellent article on why pie charts are so controversial about the right use of pie charts and why these charts are so hated ). Example: Proportions of Titanic passengers by socioeconomic status Line charts Line charts are ideal for displaying how a variable changes over a continuous interval, such as time. Generally, they are used to represent trends or relationships (in a group with more variables) between the data during a continuous time interval. Note: In order to visualize a use case in a continuous time interval, the following dataset from the Open data initiative of the Government of Spain has been used the population data by Autonomous Communities from 1997 to 2017. * Example: Spanish population since 1997 Example: Spanish population by Autonomous Communities since 1997 Treemaps & Sunbursts Treemaps and sunburst graphs are visualizations to represent hierarchical data and summarize the information in areas proportional to the corresponding values. In the case of treemaps, the representation consists of rectangles with an area proportional to the value of the corresponding data with respect to the total. Example: Proportion of passengers according to their socioeconomic status classified by gender (treemap) In the case of sunburst, the representation consists groups of rings according to the level represented, which are divided into proportional sections corresponding to the value of the data they represent. Example: Proportion of passengers according to their socioeconomic status classified by gender (sunburst) On the example represented, you can quickly see information about the hierarchical data represented as for example that the number of male passengers was almost 2 / 3 of the crew, as well as that male passengers and low socioeconomic status represented something more than 1 / 3 of the crew. Histograms & Bar charts Looking over the information already collected in the post \"Descriptive Statistics\" , where these two type of representations widely used in data analysis are explained, we can check that their main objective is to represent the distribution of the data with respect to a variable or characteristic of the dataset. Thus, the histograms allow to visualize continuous data and the bar diagrams represent discrete data. Example: Passenger distribution by price of the ticket Example: Passenger distribution by socioeconomic status As we saw in the post \"Descriptive Statistics\" , representing on these graphs measures of descriptive statistics (such as the mean or the median), we can characterize the distribution of the variable under analysis and thereby extract more information about it. Density plot As known as Kernel density plots, they serve to visualize the distribution of data in a continuous interval. This type of graphics is a variation of the histograms where the shape of the distribution is smoothly represented. Example: Distribution of the age of Titanic passengers Boxplots The boxplots or whisker plots serve to represent a variable or characteristic depending on its range and its quartiles. Note: The quartiles are the values that divide a list of ordered numbers into 4 parts in equal percentages The boxplot consists of a box that includes the data from the first quartile (Q1 - 25% of the data under it) to the third quartile (Q3 - 75% of the data under it) of the values ‚Äã‚Äãof the variable represented (Interquartile Range - IQR). Within said box, the median is represented with an horizontal line and optionally, the average is represented by an asterisk or a star. In addition to the box, this plot also shows 2 horizontal lines (or \"fences\") that represent the maximum and minimum typical values ‚Äã‚Äãof our variable, and from which the outliers ‚Äã‚Äãof the distribution are visualized using points. Statistical note: The outliers are those that are numerically distant from the rest of the data of the variable. Taking the Interquartile Range (IQR), outliers ‚Äã‚Äãare considered those that are below ",
    "url": "https://datalaria.com/en/posts/Basic-Visualizations/",
    "slug": "Basic-Visualizations",
    "lang": "en",
    "categories": [
      "foundations"
    ],
    "tags": [
      "statistics",
      "visualizations",
      "distribution",
      "graphs",
      "charts"
    ],
    "date": "2025-08-12",
    "timestamp": 0,
    "domain": "General",
    "image": "visualizaciones_basicas.png"
  },
  {
    "objectID": "en_BPMN_SOP",
    "title": "From Narrative to Diagram: Designing S&OP Processes with AI and BPMN",
    "description": "How to leverage Artificial Intelligence as a Business Analyst to transform complex narratives into structured flowcharts, comparing tools like Miro, Mermaid, and BPMN.io.",
    "content": "In the world of engineering and industrial management, we often face a \"translation\" problem. Business experts describe complex processes through dense narratives or endless text documents, while systems engineers and developers require structured logic and precise diagrams. This gap between business narrative and technical specification is where most errors occur: misinterpreted requirements, invisible bottlenecks hidden in text, and unidentified dependencies. Today, we will explore how Generative Artificial Intelligence can act as our virtual Business Analyst , transforming a complex paragraph describing a Sales and Operations Planning (S&OP) process into a standardized visual diagram. Additionally, we will analyze the technological strategy for visualizing it: When to use Miro , when Mermaid , and when BPMN.io ? The Use Case: A Comprehensive S&OP Imagine you receive the following description to digitize a planning process. It's a dense block of text, rich in detail but difficult to visualize at a glance: \"The process begins with the early detection of commercial opportunities . Engineering must identify the solution, analyzing the product's maturity and its manufacturability (obsolescence, usage restriction blocks, ROHS, REACH, high lead times,...). If there are issues, change management (ECR/ECO - Engineering Change Request/Engineering Change Order) or alternative validation is triggered; if new developments are required, systems engineering gets involved. Once the technical solution is validated, the flow branches: on one side, Operations performs factory load-capacity analysis based on what's already planned; in parallel, Procurement reviews sourcing lead times . Finally, everything converges in Finance to analyze resources, costs, and economic viability before approving the project.\" The human brain struggles to process all these conditionals and parallelisms simultaneously. This is where AI comes in. The Tool Strategy: The Visualization Triangle Not all diagrams serve the same purpose. Depending on the project phase, AI can help us generate outputs for three distinct tools. At Datalaria , we propose the following workflow: | Tool | Project Phase | AI's Role | | :---------------- | :--------------- | :----------------------------------------------------------------------------------------------------- | | Miro / Mural | Discovery | Generate task lists and decisions for \"sticky notes\" in collaborative brainstorming sessions. | | Mermaid.js | Documentation | Generate \"Diagrams as Code\" for living documentation, wikis, and technical blogs. Fast and versionable. | | BPMN.io / Camunda | Execution | Structure strict BPMN 2.0 XML files for actual process orchestration engines. | For this article, we will focus on the intermediate option: Mermaid.js . It's the perfect choice for agile technical documentation because it lives alongside your code and renders natively on the web. From Text to Code: The Engineering Prompt To achieve a quality result, it's not enough to ask AI to \"draw me a picture.\" We must ask it to reason about the logical structure. The prompt flow should be: 1. Role: Act as a BPMN expert. 2. Analysis: Identify Actors (Swimlanes), Activities, and Gateways. 3. Output: Generate Mermaid code with graph syntax. The Visual Result Below, I present the diagram automatically generated after processing the S&OP narrative. I instructed the model to use a BPMN 2.0-like aesthetic (horizontal orientation, defined lanes, and rounded nodes) to facilitate professional readability. flowchart LR %% --- DATALARIA MODERN STYLES --- %% Tasks: Clean white background with technical blue border classDef task fill:#ffffff,stroke:#2962ff,stroke-width:1px,rx:5,ry:5,color:#333; %% Gateways (Decisions): Soft orange background to highlight classDef gateway fill:#fff3e0,stroke:#ff6d00,stroke-width:1px,rotation:45,color:#333; %% Start Event: Subtle green classDef event fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px,color:#333; %% End Event: Subtle red classDef endEvent fill:#ffebee,stroke:#c62828,stroke-width:3px,color:#333; %% --- POOL / SWIMLANES --- subgraph S_OP_Process [Comprehensive S&OP Process] direction LR %% COMMERCIAL LANE subgraph COM [Sales & Business] Start((Start)):::event --> Opp(Detect Early Commercial Opportunity):::task Opp --> Reqs(Define Requirements):::task end %% ENGINEERING LANE subgraph ENG [Engineering & Product] Reqs --> Ident(Identify Solution):::task Ident --> CheckMat{Product Maturity &<br>Manufacturability Check?}:::gateway %% Exclusive Gateway (X) CheckMat -- No: Obs./Restrictions --> ECR/ECO(Manage Changes ECR/ECO):::task ECR/ECO --> ValAlt(Validate Alternatives):::task CheckMat -- No: New Dev. --> SysEng(Systems Eng: New Developments):::task SysEng --> Proto(Prototyping):::task ValAlt --> JoinEng(( )):::gateway Proto --> JoinEng CheckMat -- Yes --> JoinEng JoinEng --> SolValid(Solution Validated):::task end %% OPERATIONS LANE (PARALLEL) subgraph OPS [Operations & Supply Chain Management] SolV",
    "url": "https://datalaria.com/en/posts/BPMN_SOP/",
    "slug": "BPMN_SOP",
    "lang": "en",
    "categories": [
      "Artificial Intelligence",
      "Industry 4.0",
      "Productivity"
    ],
    "tags": [
      "S&OP",
      "BPMN",
      "Mermaid",
      "Process Design",
      "GenAI",
      "Business Analysis"
    ],
    "date": "2025-12-19",
    "timestamp": 0,
    "domain": "S&OP",
    "image": "/images/posts/ai-bpmn-sop-process.jpg"
  },
  {
    "objectID": "en_carto",
    "title": "Carto: From a UN Invoice to Conquering the Geospatial Cloud",
    "description": "The story of how Carto, a Spanish startup, evolved from a consulting project into a global leader in Location Intelligence by betting on a cloud-native strategy that has redefined data analysis.",
    "content": "In the competitive world of technology, success stories often follow a predictable script. However, every now and then, a company emerges whose journey breaks the mold‚Äîone that isn't born from a preconceived idea in a Silicon Valley garage, but from a need as pragmatic as issuing an invoice. This is the story of Carto , the chronicle of how a small data consulting studio in Madrid transformed into a global leader in geospatial analysis. It is a success story founded on a key strategic revelation: instead of fighting against the giants, they decided to become the indispensable intelligence layer that all of them needed. The Origin: A UN Invoice and a Gap in the Market The story of Carto begins in 2007, not with a product, but with Vizzuality , a data visualization consultancy founded by Javier de la Torre and Sergio √Ålvarez Leiva. Their first major assignment came from a UN agency, and the creation of the company was, initially, a mere formality to be able to invoice for that project. While working on niche projects for scientists, the founders repeatedly faced the same challenges: handling and visualizing massive volumes of spatial data. This direct experience led them to identify a \"critical need\" in the market: powerful spatial analysis techniques were inaccessible to mainstream industries like retail, finance, or telecommunications. That was the \"eureka!\" moment. They saw an opportunity to democratize geolocated data analysis . Thus, within Vizzuality, an internal software project called CartoDB was born, designed to solve their own problems. After its official launch in 2012, they made the crucial decision in 2014: Carto spun off from Vizzuality, pivoting from a service-based model to a scalable, product-centric SaaS model, ready to attract venture capital. The Strategic Pivot: We Are Not GIS, We Are the Cloud's Intelligence Layer Carto's most brilliant move was not trying to be a \"better GIS\" (Geographic Information System). Instead, they created a new category: Location Intelligence (LI) . Traditional GIS : For specialists. It answers \"where are things?\". Location Intelligence : For data and business analysts. It answers \" why do things happen in a certain place?\" and \" what if...? \". Their true competitive advantage, however, lies in their cloud-native architecture. Instead of creating another data silo and forcing companies to move their information, Carto brings the analysis directly to where the data already resides: major cloud data warehouses like Google BigQuery, Snowflake, AWS Redshift, and Databricks . This strategy turned the cloud giants, potential competitors, into their biggest allies and distribution channels. Carto positioned itself as the \"Switzerland\" of geospatial data, an agnostic analysis layer that enhances the value of the investments companies have already made in the cloud. This shift in focus was consolidated in 2016 with the rebranding from CartoDB to CARTO , dropping the \"DB\" suffix to signal its evolution from a \"database for maps\" to a complete intelligence platform under the tagline: \"Predict through location\" . The Engine of Growth: Funding Rounds, Acquisitions, and Talent Carto's trajectory validates its strategy, showing a clear progression in both investor confidence and its ability to attract talent. Key Funding Rounds The company's financing has evolved from local capital to top-tier global funds, accumulating a total of $92 million . | Round | Date | Amount (USD) | Lead Investor(s) | | :-------- | :------- | :------------- | :------------------------ | | Series A | Sep 2014 | $8M | Earlybird Venture Capital | | Series B | Sep 2015 | $23M | Accel Partners | | Series C | Dec 2021 | $61M | Insight Partners | Team Growth The team's growth reflects the company's expansion from a founding core to a global organization. | Date / Period | Employee Milestone | | :--------------- | :---------------------- | | 2012 | 2 Founders | | Nov 2021 | > 150 Employees | | Early 2024 | > 160 Experts | | Est. 2024/2025 | ~ 313 Employees | Strategic Maturity In 2019, the company matured its leadership by appointing Luis Sanz as the new CEO, an experienced operator with a successful track record of scaling businesses. Shortly after, Carto acquired Geographica , a Seville-based consultancy with extensive experience in spatial solutions for clients like Mastercard, BBVA, and Telef√≥nica. This move was a masterstroke: it provided the company with a seasoned professional services team, essential for addressing the complex needs of large enterprise accounts and executing its market strategy. The Future is Agentic: AI and the Next Geospatial Frontier Carto's vision for the future is centered on the concept of \"Agentic GIS\" : using Artificial Intelligence to make spatial analysis more accessible, conversational, and automated. The idea is to allow any user to ask complex questions in natural language, such as \"Which neighborhoods will grow the fastest?\" , and get visual answers and insights withou",
    "url": "https://datalaria.com/en/posts/carto/",
    "slug": "carto",
    "lang": "en",
    "categories": [
      "case-studies",
      "AI"
    ],
    "tags": [
      "carto",
      "geospatial",
      "ai",
      "cloud",
      "startup",
      "big data",
      "location intelligence"
    ],
    "date": "2025-10-28",
    "timestamp": 0,
    "domain": "General",
    "image": "cover.png"
  },
  {
    "objectID": "en_datalaria-blog",
    "title": "Building Datalaria: Technologies and Tools",
    "description": "Datalaria: building its foundations. A look at a modern stack vs. traditional solutions.",
    "content": "The fundamental goal of Datalaria is to experiment with and explore different topics, tools, and technologies. After a few initial posts that were more theoretical and educational to lay the groundwork for future analysis and experiments, now is a good time to discuss the inner workings of this first project: the blog itself. Let's examine the technology behind it and the reasons for choosing it over what many might expect to hear: \"WordPress.\" This post is a personal journey through the technical decision-making process that led me to build Datalaria with a modern stack: Hugo , GitHub , and Netlify . The goal isn't to demonize other platforms, but to explain why this combination was the perfect fit for this project's objectives, which are based on experimentation and exploration. Defining the Objectives Based on the main interests I mentioned in my welcome post , namely experimentation and long-term learning , I defined the core pillars of this project: Learning and Control (Priority #1) : I wanted a system that would force me to understand how things work under the hood, use version control (Git), and become familiar with continuous integration/continuous deployment (CI/CD) workflows. Performance : The website had to be very fast, with minimal loading times for a better user experience and SEO. Security : I wanted to minimize potential attack vectors and not have to worry about constant security updates for plugins or themes. Low Initial Cost and Scalability : The project is not commercial, so maintenance costs needed to be close to zero, but with the ability to scale if traffic increased or new features were integrated. With these criteria in mind, I evaluated the two main philosophies for web development. The Traditional Path: Why Not WordPress? WordPress is the undisputed king of content management, powering over 40% of the entire web. It's a powerful, flexible tool with a massive ecosystem. Its architecture is dynamic . This means that every time you visit a page, the server executes PHP code, queries a MySQL database to get the content, assembles the page into an HTML template, and sends it to you. Disadvantages for My Objectives Despite its strengths, WordPress had several drawbacks for Datalaria's goals: Different Learning Curve : Learning WordPress is largely about learning its interface and plugin ecosystem. It's about learning the tool itself. It doesn't natively expose you to web design processes, nor to tools like Git, the command line, or modern build processes. Performance : The dynamic nature, with its database calls, introduces unavoidable latency. While this can be mitigated with complex caching systems, the foundation is inherently slower than serving a static file. Security : Being so popular and dependent on a database and server-side code, it's a constant target for attacks. It requires proactive maintenance: updating the core, themes, and plugins is an ongoing task. Cost : You need a hosting provider that supports PHP and MySQL, which is usually more expensive than simple static file hosting. WordPress is a fantastic solution for non-technical users, for sites with complex functionalities (e-commerce, forums), or for those who want to quickly deploy a solution focused on content rather than on the underlying web development. However, WordPress didn't align with my main objective of deep technical learning and understanding what goes on behind the scenes. The Modern Path: Jamstack to the Rescue And then came Jamstack (JavaScript, APIs, and Markup), an architecture that decouples the frontend from the backend. The idea is revolutionary in its simplicity: instead of building the page every time a user requests it, you build it once during deployment. The result is a set of pre-generated HTML, CSS, and JavaScript files‚Äîin other words, a static site. These files are then distributed through a global Content Delivery Network (CDN). Advantages for My Objectives This philosophy was a perfect match for my goals: Performance : Serving an HTML file from a CDN is the fastest way to deliver a webpage. The results in tools like PageSpeed Insights are spectacular. Security : With no database or server-side code running at runtime, the attack surface is drastically reduced. Scalability and Almost Zero Cost : CDNs are designed to handle massive traffic spikes effortlessly. Services like Netlify or Vercel offer incredibly generous free tiers for personal projects. Developer Experience and Learning : The key point! This approach forces you to use the tools I wanted to learn. Content is written in Markdown , versioned with Git , and every git push triggers an automated process that builds and deploys the site. It's the workflow of a modern software developer applied to content creation. My Stack: Hugo + GitHub + Netlify Once I decided on the Jamstack approach, I just had to choose the tools: Hugo (The Static Site Generator - SSG) : This is the brain. It takes my content files in Markdown, combines them ",
    "url": "https://datalaria.com/en/posts/datalaria-blog/",
    "slug": "datalaria-blog",
    "lang": "en",
    "categories": [
      "Projects"
    ],
    "tags": [
      "hugo",
      "jamstack",
      "netlify",
      "github",
      "wordpress",
      "web development",
      "ci/cd"
    ],
    "date": "2025-08-17",
    "timestamp": 0,
    "domain": "Projects",
    "image": ""
  },
  {
    "objectID": "en_descriptive-analysis",
    "title": "Descriptive Analysis",
    "description": "Learn the fundamental math and statistics concepts you need to work with your data",
    "content": "As a starting point in data analysis, it is necessary to know some mathematical and statistical foundations in order to work with the information to be analyzed. Thus, we will start learning some basic concepts: Population, Samples, Individuals and Variables: Population : The set of all the elements under study Sample : A subset of elements of the population (it should be representative of the population) Individuals : Every individual element in the popularion Variables : Characteristics of the individuals Example: Titanic Dataset Population: 2.224 individuals Samples: files: train.csv : Set of 891 representative individuals of the population used for the training of models of Machine Learning. test.csv : Set of 418 representative individuals of the population used for the testing of models of Machine Learning. Individuals: Each passenger and crew of the population (every row of the sample or population data collected). Variables: The characteristics collected for each individual (every column of the sample or population data, ej.: survival (survivals), sex (sex) o Age (age)). Types of Variables The variables can be classified depending on the type of data they contain and, more specifically: Numerical : the data is represented by numbers that are metric or measures of said variable. According to their values, these could be classified in: Continuous : they could take a countless or indeterminate number of values. For example, in the data set of the Titanic example, the variable fare could be considered a continuous variable by taking different values with up to 4 decimals of precision. Discretes : they could take a countable number within a list of values (the data of continuous variables can be truncated to discrete variables for easier analysis). For example, the variable Sibsp (# of siblings) would be a discrete variable. Categorical : the data is represented by texts or numbers whose meaning is not a metric, but associated with a specific datum or category. Depending on the relationship between their values, the data would be classified in: Nominals : data that have no relation or intrinsic order in themselves. An example of this type would be the embarked variable that represents the port of embarkation of passengers. Discretes : data with a natural order or classification among themselves. As for example it could be the variable Pclass that represents the class in the Titanic according to the social status. Note of interest: The typology of the variables is not always clear and they could be considered of different types depending on the criteria of each person and the objectives sought. An example could be the representation of age, which could be correctly considered both numerical (continuous or discrete depending on the precision) and ordinal, depending on whether you want to work with formulas to treat age as a continuous value or simply work with specific age ranges according to the relationship of the rest of the variables. Histograms and Bar charts Although I will make a specific post for the data visualization, it is important to briefly mention the histograms and bar graphs. These are two of the most useful representations in the data analysis and help to understand how the data are distributed with respect to a variable or characteristic of the dataset (continuous in the case of histograms and discrete in the case of bar graphs). Example: Bar chart Example: Histogram Thus, significant information about the data distribution can be extracted, classifying it in symmetric or asymmetric, as we will see below. Measures of descriptive statistics A key step when analyzing a dataset is what is called \"descriptive statistics\", that is, in describing the data through a series of measures that allow to characterize its distribution and its dispersion. Measures of central tendency The measures of central tendency serve to give us an idea of the distribution of our data. Arithmetic Mean (Average) This measure seeks to obtain the average value of a dataset. That is, all the values in the dataset are summed and divided by the total number of values in that data. Population mean : $$ \\mu = \\frac{1}{N}\\displaystyle\\sum_{i=1}^Nx_i $$ Sample mean : $$ \\bar x = \\frac{1}{n}\\displaystyle\\sum_{i=1}^Nx_i $$ Being N the total of values in the population and n the total of values in the sample. Example: If we have the following set of 5 data: [1, 2, 2, 5, 10], the average or mean of these values is their sum (20) divided by the total of data (5), that is, the average or mean is 4. Example of the Titanic dataset: Following up on our example of the Titanic dataset, and taking the Age variable in the sample train.csv , we visualize its distribution and its average or mean: Median This measure represents the middle value in the dataset. That is, for its calculation, we take the data point in the middle of all the dataset x m : $$ \\displaystyle x_{m} = x (\\frac{n+1}{2}) $$ The median, due to its dependence on the posit",
    "url": "https://datalaria.com/en/posts/descriptive-analysis/",
    "slug": "descriptive-analysis",
    "lang": "en",
    "categories": [
      "foundations"
    ],
    "tags": [
      "statistics",
      "maths",
      "distribution",
      "normal",
      "gaussian"
    ],
    "date": "2025-08-09",
    "timestamp": 0,
    "domain": "General",
    "image": "estadistica_descriptiva.png"
  },
  {
    "objectID": "en_florence-nightingale",
    "title": "Florence Nightingale - The Mother of Nursing",
    "description": "Florence Nightingale, a British nurse, writer, and statistician, the mother of nursing and one of the first data scientists.",
    "content": "We continue our series on data analysis success stories to talk about Florence Nightingale (May 12, 1820 ‚Äì August 13, 1910), a revolutionary figure. A British nurse, writer, and statistician, she is considered the mother of modern nursing and, as we will see, one of the first and most influential data scientists . Her case is particularly relevant because, in addition to fighting for women's emancipation by opening up new qualified professional paths, she stood out in the field of nursing for being the pioneer of modern professional nursing through the creation of the first conceptual model . To develop this model, Florence applied her statistical and mathematical knowledge, as well as all the data and information related to nursing care that she gathered during a learning period in which she visited different hospitals, prisons, asylums, and orphanages in France, Germany, Italy, Switzerland, Ireland, and England. The success of this new professionalized nursing model was validated by its study and application to the high mortality rate of the wounded in the Crimean War , which had broken out in 1853 following Russia's ambitions to occupy the Danubian territories. Mortality rates within the British army were high, not as a result of the war itself, but due to the chaotic healthcare system that governed them. The field hospitals barely had any resources, and the filth was indescribable. Faced with this situation, the British Government decided to send Florence Nightingale to completely reform the army's sanitary administration. Upon her arrival, Florence began the arduous task of collecting and gathering data with the aim of analyzing the existing health conditions and presenting her findings to Parliament. To do this, she created a chart, known as the polar area diagram or rose diagram , which made it easier to visualize and interpret all the collected data. It starkly highlighted that the mortality rate in English hospitals was higher than on the battlefield itself. With this data, she convinced Parliament of the need to improve sanitary conditions in hospitals. As a result, Florence revolutionized aspects such as patient care, hygiene, and nutrition. After the war, she returned to London and focused on the civilian sector, where she wrote a textbook with her knowledge and founded a nursing school to train these professionals. Among her contributions, it is also worth highlighting the formulation of a model for Hospital Statistics so that hospitals could collect and generate consistent data and statistics. Therefore, in summary, Florence Nightingale represents a clear case of success and disruption within the healthcare field, thanks to her application of data collection and analysis to achieve greater clarity in its interpretation and, above all, to drive continuous improvement. Sources of Interest CODEM - Official College of Nursing of Madrid Master Telef√≥nica in Big Data & Business Analytics Did Nightingale‚Äôs ‚ÄòRose Diagram‚Äô save millions of lives?",
    "url": "https://datalaria.com/en/posts/florence-nightingale/",
    "slug": "florence-nightingale",
    "lang": "en",
    "categories": [
      "case-studies"
    ],
    "tags": [
      "nursing",
      "florence nightingale",
      "healthcare",
      "crimea"
    ],
    "date": "2025-09-06",
    "timestamp": 0,
    "domain": "General",
    "image": "FlorenceNightingale.png"
  },
  {
    "objectID": "en_founderz-coding-chatGPT",
    "title": "Copiloting with AI: Learning with Founderz and ChatGPT",
    "description": "A chronicle of how the AI and Prompt Engineering course by Founderz opened my eyes to the possibility of creating functional web applications using only natural language with ChatGPT.",
    "content": "In our daily lives, we constantly face barriers when trying to implement good ideas, often due to a lack of technical knowledge and the time needed to figure out where to start. The AI models available today have brought about a revolution in this regard, accelerating timelines and removing learning obstacles. Thanks to these models, we can do more in less time and delve into highly complex fields like programming, allowing us to bring simple applications that implement good ideas to life. In this context, I recently completed the Introduction to AI and Prompt Engineering course by Founderz , which is no longer available though you can find more similar courses in their website ( https://founderz.com/ ), taught by Pau Garc√≠a-Mill√° (Founder and Co-CEO of Founderz), Anna Cejudo (Founder and Co-CEO of Founderz), and Magda Teruel (Client Account Manager at Microsoft). It was easy to follow, very educational, well-structured, and it truly made an impact, offering glimpses of what can be achieved with AI through brief and effective instructions. The course consisted of short videos, between 5 and 15 minutes long, that explained and developed the idea of AI models as \"Copilots\" in our daily lives. Pau, Anna, and Magda presented examples of how to \"talk\" to these \"Copilots\" and how to use them in our day-to-day tasks, showing some advanced use cases and practical examples of how to turn our project ideas into web applications without knowing how to program, thanks to the good work of our \"Copilots.\" To finish, they proposed that you try creating a basic project yourself with these models based on what you've learned, providing a very interesting feedback mechanism from both themselves and other students. Besides recommending this kind of courses to anyone who wants to get started with AI models and learn more about having these \"Copilots,\" I'm going to use this post to illustrate one of the key takeaways from the course, focusing on how to create a functional application through a simple conversation with ChatGPT and the right instructions. The \"Aha!\" Moment of the Founderz Course The Founderz course was tremendously educational, and its greatest lesson was presenting AI (in this case, Copilot and ChatGPT) not just as a simple search assistant but as a creation partner (a copilot) . The clearest example of this copilotage is how a language model can help us create structured and functional code from an initial idea through a series of simple instructions. Through several practical cases, it teaches how to create self-contained web applications in a single HTML file, unleashing our ideas to improve our daily lives. The Challenge: Creating a Useful Tool from Scratch with ChatGPT To put this skill to the test, we're going to create a learning game for basic math operations aimed at preschool and elementary school children between 5 and 7 years old . The goal was simple: a webpage that would teach and allow practice of basic mathematical operations (addition, subtraction, multiplication, and division) by learning levels, making it engaging for users. The Recipe: The Detailed Prompt (and its Refinement) is Key This is where Prompt Engineering comes into play. It's not about asking ChatGPT to \"make me an app,\" but about acting as a product manager who gives the development team (the AI) clear and detailed specifications. The Initial Prompt This was the first prompt I used, inspired by the course's methodology, where I defined the entire structure and functionality I wanted (I encourage anyone to experiment and reproduce it themselves on ChatGPT): Create a simple web application to practice basic math functions: addition, subtraction, multiplication, and division. The goal is to create a simple application that makes it easier for preschool and elementary school children (between 5 and 7 years old) to learn these operations. The application must: 1. Start with a learning page that explains the 4 mathematical operations. 2. Allow the selection of a learning level from 4 levels (preschool 1 & 2, elementary 1 & 2). 3. Allow the selection of a difficulty level (easy with 1 digit, hard with 2). 4. Have a \"Start Game\" button that leads to a screen with operations. 5. Check if the answer is correct and display a motivating message and an emoticon. 6. Keep track of correct and incorrect answers. 7. Have an attractive and responsive design using Tailwind CSS. 8. Include all HTML, CSS, and JavaScript code in a single file. ChatGPT processed the request and, in seconds, returned a single block of code containing a fully functional application. The Refinement: The Conversation Continues The first result worked quite well, but there's always room for improvement üòÑ. This is where the true power of conversational AI lies: the ability to iterate and improve thanks to the coherence it can maintain in its results. As a test, I asked for a series of changes to make the application more dynamic and attractive: It works! Now I want you to mak",
    "url": "https://datalaria.com/en/posts/founderz-coding-chatGPT/",
    "slug": "founderz-coding-chatGPT",
    "lang": "en",
    "categories": [
      "Tools",
      "Learning"
    ],
    "tags": [
      "ai",
      "chatgpt",
      "prompt engineering",
      "founderz",
      "web development",
      "no-code",
      "html",
      "maths"
    ],
    "date": "2025-09-11",
    "timestamp": 0,
    "domain": "General",
    "image": "Founderz_AI_Introduction.png"
  },
  {
    "objectID": "en_freepik",
    "title": "Freepik: The Story of the Giant from M√°laga that Tamed AI to Conquer the Creative World",
    "description": "An in-depth analysis of how Freepik, the startup from M√°laga, went from an image search engine to a global leader, facing the disruption of AI with a bold strategy that is redefining the future of creativity.",
    "content": "In 2022, the explosion of generative artificial intelligence sent an earthquake through the creative industry. Models like Midjourney or DALL-E 2 posed an existential question that shook the foundations of stock content giants: \"What's the point of an image bank when users can create exactly what they need from scratch?\" . Many companies froze. One, however, saw the threat coming and decided to bet \"all or nothing\" on a total reinvention. That company wasn't born in Silicon Valley, but in M√°laga, Spain. This is the story of Freepik , the journey of a Spanish startup that, thanks to a unique culture and astonishing strategic agility, has not only survived the disruption of AI but is positioning itself to lead the new era of creativity. The DNA: A Google Mindset with a \"Homegrown\" Heart Freepik's success was no accident. Founded in 2010, its culture was forged from a unique combination of talents: the brothers Alejandro (the frustrated designer who had the original idea) and Pablo Blanes, along with Joaqu√≠n Cuenca, a serial entrepreneur who had already sold his company Panoramio to Google. Cuenca imported Google's discipline and data-driven methodology, but operating from M√°laga, far from major venture capital, they couldn't afford \"star signings.\" In their own words, they were like the \"Athletic Bilbao\" football club: their strength lay in their \"youth academy,\" in developing local talent and an obsessive focus on the product. Their initial business model was a disruptive stroke of genius. They started as a simple search engine indexing free resources. Their freemium model eliminated the cost barrier, attracting millions of users. Their first source of revenue? A cunning alliance with their main competitor, Shutterstock, to whom they sent traffic in exchange for a commission. At the same time, the attribution requirement for free resources turned their massive user base into an unpaid global marketing army , generating millions of links that catapulted their dominance on Google. The Existential Pivot: Sacrificing the Present to Win the Future Freepik was already a giant before AI. After being acquired by the private equity firm EQT in 2020 for an estimated 250 million euros, it doubled its revenue to 61.5 million in 2021 and surpassed Shutterstock in the number of premium subscribers. But then came the \"earthquake\" of generative AI. The leadership understood that a lukewarm response was not enough; the threat required a complete reinvention. They made a courageous decision: to bet \"all or nothing\" on AI , massively reallocating their resources to build a new ecosystem of creative tools. This decision came at a deliberate cost. After growing by 45% in 2022, the company's growth dramatically slowed to just 11% in 2023 . It was a conscious slowdown: they sacrificed short-term results to invest in their long-term survival and leadership. | Fiscal Year | Revenue (‚Ç¨M) | Annual Growth (%) | Key Strategic Context | | :--- | :--- | :--- | :--- | | 2019 | 31 | - | Organic growth pre-acquisition. | | 2021 | 61.5 | - | Accelerated growth post-EQT; surpasses Shutterstock. | | 2022 | 79 | 45% | Strong growth; beginning of AI disruption. | | 2023 | 88 | 11% | Year of the AI pivot : Growth slowed to invest in the AI Suite. | | 2024 (E) | > 100 | > 13.6% | Return to growth driven by new AI tools. | The result of this investment is the Freepik AI Suite , an ecosystem that includes everything from an image and video generator to innovative tools like Pikaso (which turns sketches into images in real-time) and the upscaling technology of the recently acquired Magnific . The Secret Weapon: A Hybrid Ecosystem for the Real World Freepik's strategy is not to beat Midjourney at creating conceptual art. Its goal is to win in the commercial workflow . While Midjourney serves the \"Artist/Explorer,\" Freepik has positioned itself to dominate the much larger \"Creator/Communicator\" segment: the marketing professional, the educator, or the small business owner who needs to create professional-looking content quickly and, above all, with legal safety. Its competitive advantage lies in a unique hybrid model: Integrated Workflow : A user can generate an image with AI, remove its background, add an icon from its stock library, and add text with its editor, all within the same platform and subscription. This is impossible in pure AI tools. Stock Library as a Strategic Moat : Its gigantic library of over 200 million assets is no longer just a product but a defensive advantage. It offers a fast and safe route for users (\"it's easier to tweak a stock photo than to generate the perfect one from scratch\") and serves as a potential \"clean\" dataset for training future proprietary AI models, avoiding copyright issues. Legal Security : By offering legal indemnification in its enterprise plans, Freepik directly addresses the biggest pain point for corporate clients, providing a security that tools trained on internet data cannot guarantee. Conclusion: Defi",
    "url": "https://datalaria.com/en/posts/freepik/",
    "slug": "freepik",
    "lang": "en",
    "categories": [
      "case-studies",
      "AI"
    ],
    "tags": [
      "freepik",
      "ai",
      "generative ai",
      "malaga",
      "startup",
      "business model",
      "disruption"
    ],
    "date": "2025-10-11",
    "timestamp": 0,
    "domain": "General",
    "image": ""
  },
  {
    "objectID": "en_game_snake",
    "title": "From Zero to Hero: Create a Cyberpunk Snake Game with Real-Time Ranking using Supabase and Vanilla JS",
    "description": "Learn to create a neon arcade-style web game using only HTML5 Canvas and Vanilla JS. Discover how to integrate a global real-time leaderboard using Supabase's PostgreSQL database.",
    "content": "Do you remember the classic Snake from the old Nokias? Now imagine it with neon lights, synthesized music, explosive particles, and most importantly: connected to the cloud . Web game development has experienced a renaissance thanks to the power of HTML5 Canvas and Serverless tools. In the past, if you wanted to save your players' scores to create a global leaderboard, you needed to set up a server, configure a REST API, manage databases, and pay for hosting. Today, thanks to Backend-as-a-Service (BaaS), we can do it in minutes. In this tutorial, we are going to build \"Neon Snake\" , an aesthetic and functional web arcade game that uses Supabase (an Open Source alternative to Firebase) to persist data in PostgreSQL. üõ†Ô∏è The Tech Stack We're going to keep it simple but powerful. No heavy frameworks, just the purity of the web: Frontend: HTML5 Canvas + Vanilla JavaScript (ES6+). Styles: Tailwind CSS (via CDN) for the user interface. Backend: Supabase (PostgreSQL + Realtime). Audio: Web Audio API for code-generated sound effects. Step 1: Setting up the Backend with Supabase Before writing a single line of JavaScript, we need our brain in the cloud. Supabase offers us a real PostgreSQL database with an automatically generated API. Create a free account at supabase.com . Create a new project called NeonSnake . Go to the Table Editor section and create a new table called snake_scores . The Magic of SQL To streamline the process, Supabase allows you to execute SQL commands directly. Go to the SQL Editor and paste the following code. This will create the table structure and configure the security policies (RLS - Row Level Security) so your game can read and write data publicly. [code block] Security Note: In a commercial game, we would use full user authentication. For this arcade-style demo, we allow public (anonymous) access to reduce friction so anyone can play and log their record instantly. Finally, go to Settings > API and copy your Project URL and your anon public key . You will need them shortly. Step 2: The Cyberpunk Frontend (HTML5 + Canvas) Our game will live in a single index.html file. We will use Tailwind CSS for the floating menus (Game Over, Start Screen) and the <canvas> element to render the game at 60 FPS. The basic structure is as follows: [code block] Game Logic: Beyond Eating Apples To give it that \"Pro\" touch and differentiate it from a basic tutorial, we implement several key mechanics in JavaScript: Animation Loop: Using requestAnimationFrame to maintain fluidity at 60 FPS. Particle System: An array of objects that spawn at the \"food\" coordinates when eaten and fade gradually, simulating a data explosion. Dynamic Firewalls: From level 5 onwards, we generate static obstacles that the player must dodge, progressively increasing the difficulty. Hybrid Controls: PC: We listen for keydown events for directional arrows. Mobile: We capture touchstart and touchend to calculate the gesture direction ( swipe ) and move the snake. Step 3: Connecting the Dots (Integration with Supabase) Here is where the Backend-as-a-Service magic happens. We are going to connect our Canvas to the PostgreSQL database. Inside our <script type=\"module\"> tag: 1. Client Initialization [code block] 2. The \"Game Over\" Flow When the player loses and hits \"UPLOAD DATA\", we need to ensure the data is saved before showing the leaderboard. We will use async/await to control this asynchronous flow. [code block] 3. Fetching the Ranking (Select) We retrieve the Top 20 sorted by score in descending order to show who rules the server. [code block] Bonus: Realtime ‚ö° Do you want the ranking to update on everyone's screen if someone breaks the record at that precise moment? Supabase makes this trivial with its subscriptions: [code block] The Final Result By putting it all together, you get a fluid and highly competitive user experience: The user plays frantically dodging neon \"Firewalls\". Upon crashing, the game stops with a visual \"glitch\" effect. They enter their name (e.g., \"Neo\"). Upon clicking send, in milliseconds , their name appears on the leaderboard. If their friend is playing on another mobile at the same time, they will see \"Neo's\" name appear on their screen without reloading the page. And once created... all that remains is to enjoy and play üêçüòä Datalaria Snake Game Conclusion We have moved from a simple static canvas to a Fullstack application in real-time without touching a traditional backend server (Node.js, PHP, Python) or managing infrastructure. The combination of creativity in game development with the power of BaaS tools like Supabase opens a world of possibilities for frontend developers. The next step? I invite you to clone the project and improve it: add GitHub authentication to avoid duplicate names, create snake skins that unlock with high scores, or implement a multiplayer mode using Supabase presence channels. The code is yours! üêçüíª Did you like this tutorial? Share it on your networks with your High Scores and ",
    "url": "https://datalaria.com/en/posts/game_snake/",
    "slug": "game_snake",
    "lang": "en",
    "categories": [
      "Web Development",
      "GameDev",
      "Tutorials"
    ],
    "tags": [
      "JavaScript",
      "Supabase",
      "PostgreSQL",
      "Canvas",
      "HTML5",
      "TailwindCSS"
    ],
    "date": "2026-02-07",
    "timestamp": 0,
    "domain": "General",
    "image": "imagen_snake_game.png"
  },
  {
    "objectID": "en_graphext",
    "title": "Graphext: The Spanish Startup That Spent 7 Years Building the 'Formula 1' of Data Analysis",
    "description": "The story of Graphext, the Spanish startup that defied norms by investing ‚Ç¨7M in R&D before scaling, creating a unique Explainable AI platform that redefines data analysis.",
    "content": "In the fast-paced world of startups, the motto is often \"grow fast or die.\" Launching a minimum viable product, gaining traction, and raising multi-million dollar funding rounds seems to be the obligatory path. But what if there was another way? A patient, almost artisanal strategy, focused on building technology so advanced it creates an insurmountable competitive moat before hitting the commercial accelerator? This is the story of Graphext , the startup founded by two Spanish computer engineers who dedicated seven years and approximately 7 million euros to perfecting their platform before seeking significant funding. Today, armed with cutting-edge technology and a clear vision centered on Explainable Artificial Intelligence (XAI) , they are ready to redefine how companies interact with their data. The Origin: From Analyzing Twitter to Understanding the World Graphext's spark didn't come from a business plan, but from curiosity. The founders, Victoriano Izquierdo and Miguel Cant√≥n , computer engineers with an entrepreneurial spirit since childhood, started with a tool called contexto.io , focused on analyzing connections on Twitter. They soon realized that the true potential lay in going further, in creating broader information contexts, in visualizing the hidden networks that connect people and organizations in any dataset. Thus, Graphext was born in 2015, merging 'graph' and 'context'. Their mission: to democratize data science , bridging the gap between coding experts and business analysts who have important questions but lack the tools to answer them directly. They wanted to overcome the limitations of Excel (too basic) and programming notebooks like Jupyter (too complex for non-technical profiles), aspiring to create something new: a tool \"as interactive as Figma, but for data science.\" The Technology: A \"Formula 1\" in Your Browser What radically differentiates Graphext is its architecture. After years of intensive R&D, they have built what their CEO describes as a \"Formula 1\": an incredibly powerful analysis machine. Their secret lies in leveraging cutting-edge web technologies like WebAssembly (Wasm) , WebGL , and Apache Arrow . Thanks to Wasm, a large part of data processing (up to 80-90%!) happens directly in the user's browser , not on a remote server. The result is astonishing fluidity: exploring and filtering millions of data rows feels instantaneous. They have developed their own compression libraries and an internal \"low-code\" language. This deep investment in proprietary technology creates, according to its founders, a very difficult competitive moat to replicate. The Platform: From Raw Data to Explainable Models Graphext is not just a visualization tool; it's a comprehensive \"no-code/low-code\" platform covering the entire data analysis lifecycle: Universal Connection : Import from a simple CSV or connect directly to modern data warehouses (Snowflake, BigQuery, Databricks, Redshift). Interactive Visual Exploration (EDA) : The heart of the tool. Filter, group, cross-reference variables, and enrich data on the fly. Advanced No-Code Modeling : Apply machine learning algorithms (clustering, NLP for text analysis, image analysis) with clicks, not code. Prediction with Explainability (XAI) : Create predictive models (to forecast customer churn or identify promising sales leads ) and, crucially, understand why the model makes that prediction. This commitment to Explainable AI is at the core of their future strategy. The \"Formula 1\" Dilemma: Power vs. Accessibility Despite its \"no-code\" approach, Graphext acknowledges a tension: its tool is so powerful that it requires a skilled \"driver\" to unlock its full potential. It's not for absolute beginners, but for business analysts, data scientists, and power users seeking superpowers. This duality is reflected in its hybrid business model: Self-service (Free and Pro) : To attract users and enable organic dissemination (Product-Led Growth). Enterprise : With customized pricing and data engineering and training services, recognizing that large corporations need support. The company is evolving from selling ‚Ç¨1,000 tickets to closing six or seven-figure contracts with corporations like McDonald's and Roche . An Atypical and Patient Financial Strategy Graphext defied venture capital norms. For its first 7 years, it funded itself with a clever combination: Modest Seed Capital : Small rounds, including one led by K Fund . Key Public Grants : Approximately 2 million euros from European funds (Horizon 2020, EIC Fund), crucial for financing R&D without excessive dilution. This strategy allowed them to patiently build their technological \"Formula 1.\" Once the product matured and technological risk was significantly reduced, they became highly attractive to top-tier venture capital. The turning point came in June 2023 with a $4.6 million seed round led by Hoxton Ventures (London), marking the beginning of their commercial scaling phase. The backing of over 80 angel i",
    "url": "https://datalaria.com/en/posts/graphext/",
    "slug": "graphext",
    "lang": "en",
    "categories": [
      "success_stories",
      "AI"
    ],
    "tags": [
      "graphext",
      "ai",
      "explainable ai",
      "no-code",
      "startup",
      "big data",
      "visualization"
    ],
    "date": "2025-11-28",
    "timestamp": 0,
    "domain": "General",
    "image": "cover.png"
  },
  {
    "objectID": "en_john-snow",
    "title": "The authentic John Snow, a precursor of Geolocation",
    "description": "John Snow: The father of epidemiology and a pioneer of geolocation.",
    "content": "Despite the fact that nowadays, for the great majority of people, the name of John Snow brings to his mind to Aegon Targaryen or the heir to the iron throne, there is actually another, perhaps less popular, John Snow (United Kingdom 15/03/1813 - 16/06/1858). This another John Snow should be known as a precursor or forerunner of epidemiology and geolocation and one of the references in the field of dissemination of data thanks to its Cholera map of 1854. John Snow was an English doctor who was able to demonstrate in an innovative way that the Cholera was driven by the consumption of water contaminated with feces, breaking with the then-dominant miasma theory which assured that the origin of the disease was based on the air or the foul odors of soils or impure waters. For that purpose, John Snow analyzed in detail the cases of the cholera outbreak in the city of London in 1854 and detected that approximately 700 people deceased in the area of Soho during the same week. To make a clearer view of this issue, Dr. Snow developed a geographic information system based on information layers over a map of the neighborhood. Specifically, John Snow represented a layer with the water pumps (points in the map labeled as \"pump\") and another layer visualizing the deaths occurred in every building of this area (stacked black lines in each house). As a result of this representation, Dr. Snow identified the water pump located on Broad Street, in the heart of the epidemic, as the source of the problem because more than 70% of deaths were caused in its service area. However, and in spite of the clairvoyance of the conclusions, the authorities did not taken into account immediately and John Snow had to clear some confusing cases. For example, some workers in a workshop and others in a distillery had hardly suffered cholera cases due to the fact that they had private well pumps. In addition, certain cases of deaths far from the area of ‚Äã‚Äãthe water pump were also clarified based on visits to the area for work or family reasons. Finally, and thanks to the fact that the origin of the epidemic was also detected (the contamination of the waters by the filtration of a baby's diapers that had been thrown into a black hole days before the epidemic began), John Snow got the authorities to remove the handle of the pump and then, the cholera cases began to reduce. Today, there are several references to Dr. John Snow in the current neighborhood of Soho, on the famous Broad Street renamed today as Broadwick Street. A pub with its name where the John Snow Society meets, a sculpture of the famous pump of water, or even a small plate in the building where said pump was. Below, and for more information, you can find the link to the TED talk \"The Ghost Map\" by Steven Johnson about the impact that John Snow and his map had in science, cities and modern societies. Sources of Interest TED Talks - Steven Johnson - The Ghost Map Wikipedia - John Snow Victorian Web - John Snow Master Telef√≥nica en Big Data & Business Analytics All3dform - John Snow y el mapa que salv√≥ Londres psanxiao - El mapa del c√≥lera de John Snow",
    "url": "https://datalaria.com/en/posts/john-snow/",
    "slug": "john-snow",
    "lang": "en",
    "categories": [
      "success_stories"
    ],
    "tags": [
      "cholera",
      "john snow",
      "map",
      "gis",
      "geolocation",
      "London"
    ],
    "date": "2025-08-26",
    "timestamp": 0,
    "domain": "General",
    "image": "John_Snow.png"
  },
  {
    "objectID": "en_kilometer-0-why-datalaria-is-born",
    "title": "Kilometer 0: Why Datalaria is Born",
    "description": "",
    "content": "Welcome to Datalaria! This is the very first post, the starting point of a project born from a dual need: professional and personal. Professionally, I operate in a high-demand industrial and technological environment. I see every day how data and artificial intelligence are ceasing to be abstract concepts and becoming critical tools. This blog is my laboratory to experiment with those tools, to understand them deeply, and hopefully, to apply that knowledge in my career. Personally, I am naturally curious. This is my public logbook, my commitment to structured learning and sharing the journey. Here I will document my experiments, my mistakes, and my discoveries. Thank you for joining me at this kilometer 0.",
    "url": "https://datalaria.com/en/posts/kilometer-0-why-datalaria-is-born/",
    "slug": "kilometer-0-why-datalaria-is-born",
    "lang": "en",
    "categories": [],
    "tags": [
      "Meta",
      "Introduction"
    ],
    "date": "2025-08-06",
    "timestamp": 1754487000,
    "domain": "General",
    "image": ""
  },
  {
    "objectID": "en_meetings_ai",
    "title": "Excellent Meetings in the AI Era: The Definitive Decalogue",
    "description": "A decalogue of 10 practical steps to transform your meetings, making them shorter, more effective, and agile, with a special focus on how AI can be your best ally.",
    "content": "If there's one universal element in the professional world that generates both love and hate, it's meetings. When effective, they can align teams, unlock ideas, and accelerate projects. But when they're not, they become black holes of time and energy that devour our productivity. In an era where remote and hybrid work is the norm and artificial intelligence tools are redefining our capabilities, mastering the art of the effective meeting is no longer just a desirable skill‚Äîit's a strategic necessity. The good news? It's not magic, it's method. Below, I present a decalogue of 10 practical steps to transform our meetings, with a special focus on how technology, particularly AI, can become our best copilot to achieve this. The Decalogue for Excellent Meetings 1. Meetings Only for the Essential The first rule of a good meeting system is... to have fewer meetings. Before scheduling, ask yourself: Is it really necessary? Can it be resolved with an email, a shared document, or a quick message? Reserve synchronous time for discussions, complex decision-making, or brainstorming sessions that genuinely require real-time interaction. 2. Invite the Right People (and Respect Their Schedule) A meeting with too many attendees is a recipe for inefficiency. Invite only those who are essential for decision-making or whose contribution is indispensable. Before setting a time, use calendar tools to check the availability of key attendees and avoid unnecessary conflicts. 3. Define Objectives and a Clear Agenda (24h in Advance) A meeting without an agenda is like a ship without a rudder. The invitation should always include the main objective, a detailed agenda, and any necessary pre-reading materials. Sending this at least 24 hours in advance allows attendees to arrive prepared, making the meeting much more productive. 4. Start and End on Time (and Shorten if Possible) Punctuality shows respect for everyone's time. Start at the scheduled time, even if not everyone has arrived, and end on time. A pro tip is to schedule 25 or 50-minute meetings instead of 30 or 60, leaving a small buffer between commitments. 5. No Interruptions: Absolute Focus For a meeting to be effective, it requires the full attention of its participants. Establish a rule to silence mobile phones and close messaging applications (Slack, Teams, etc.). The cost of multitasking and distractions is much higher than we think. 6. Contribute Ideas and Encourage Active Participation A meeting is not a lecture. The facilitator must ensure that all voices are heard. Foster an environment of psychological safety where ideas, not people, can be debated. Active listening is crucial. 7. Summarize Conclusions and Actions at the End The last 5 minutes of the meeting are the most important. Use them to verbally summarize key decisions, concrete actions, assigned owners, and deadlines. This ensures everyone leaves with the same understanding. 8. Avoid Calendar \"Dead Zones\" When a meeting is scheduled directly impacts attendee energy and focus. Avoid late afternoon meetings when fatigue sets in, or very early morning meetings when people need time to organize their day. Mid-morning slots are often the most productive. 9. Establish a \"No Meeting Day\" An increasingly popular and highly effective practice. Agree with your team on one day a week or month (e.g., Wednesdays) when no internal meetings are scheduled. This frees up an uninterrupted block of time for \"deep work.\" 10. Use Technology (and AI) as Your Ultimate Copilot Technology is no longer just for connecting remote people; it's for making the entire meeting lifecycle smarter and more agile. Here's an arsenal of tools and practical use cases. Before the Meeting: Smart Planning A successful meeting starts long before it begins. Automatic Scheduling : Forget the endless email chains trying to find a slot. Tools like Clockwise or Motion analyze the entire team's calendars, find the best time for everyone while respecting their deep work blocks, and even intelligently reschedule meetings if conflicts arise. AI-Powered Agenda Creation : Instead of starting from scratch, you can ask an AI assistant for help. Practical Case (Prompt for Gemini/ChatGPT) : \"Generate a detailed agenda for a 60-minute meeting. The objective is to decide the marketing strategy for the 'X' product launch. Attendees are the Marketing Director, a social media specialist, and a data analyst. Include timings for each point and key questions to discuss.\" During the Meeting: Focus on Conversation, Not Notes The goal is for participants to focus on discussion, not transcription. Real-Time Transcription and Translation : Major platforms now integrate this. Google Meet (with Gemini) and Microsoft Teams (with Copilot) not only transcribe the conversation but can translate it in real-time, removing language barriers in international teams. Dedicated Assistants : Tools like Otter.ai or Fireflies.ai act like an extra participant in the meeting. They join the call, r",
    "url": "https://datalaria.com/en/posts/meetings_ai/",
    "slug": "meetings_ai",
    "lang": "en",
    "categories": [
      "Productivity",
      "Tools"
    ],
    "tags": [
      "ai",
      "meetings",
      "productivity",
      "remote work",
      "prompt engineering",
      "best practices"
    ],
    "date": "2025-11-21",
    "timestamp": 0,
    "domain": "People",
    "image": "cover.png"
  },
  {
    "objectID": "en_multiverse_computing",
    "title": "Multiverse Computing: The Spanish Startup Solving AI's Bottleneck with Quantum Physics (Without Quantum Computers)",
    "description": "Strategic analysis of Multiverse Computing, the Spanish startup that has raised $326M by applying quantum physics mathematics (Tensor Networks) to compress AI models on classical hardware, solving the critical problem of LLM cost and efficiency.",
    "content": "In the current technological race, two narratives dominate the headlines: the Generative Artificial Intelligence revolution and the future promise of Quantum Computing. However, both face significant barriers. AI models (LLMs) are becoming increasingly larger, expensive, and energetically unsustainable. On the other hand, useful and fault-tolerant quantum computers remain a promise years away. At the intersection of these two challenges emerges Multiverse Computing , a startup founded in San Sebasti√°n that has found an ingenious and pragmatic solution. Instead of waiting for the quantum hardware of the future, they are using the mathematical tools of quantum physics today to solve the most pressing AI problems on classical computers. This approach, known as \"quantum-inspired computing,\" recently allowed them to close a $215 million Series B investment round, reaching a valuation of $500 million and solidifying their position as one of Europe's most promising deep tech companies. The Problem: The AI Scalability Crisis The deployment of models like GPT-4 or Llama-3 is hitting a wall of reality: the cost of inference. Running these models requires immense computational power and energy, limiting their adoption and sustainability. Traditional compression techniques, such as \"quantization\" (reducing numerical precision) or \"pruning\" (removing less important neurons), have limits before significantly degrading model quality. The industry needs a paradigm shift to make AI economically viable at scale. The Solution: Tensor Networks, the Mathematical \"Trick\" This is where Multiverse Computing shines. Its core innovation is not hardware, but the industrial application of a complex mathematical structure called Tensor Networks . Originally developed in condensed matter physics to simulate complex quantum states, Tensor Networks allow decomposing problems of enormous dimensionality into manageable pieces. Imagine an AI model as an immensely complex network of connections. Tensor Networks allow identifying and retaining only the essential correlations that contribute to precision, discarding noise and redundant connections. The key advantage: This can run on current GPUs and CPUs, emulating quantum efficiency without needing a quantum computer. CompactifAI: Compressing the Future of AI The flagship product that has catapulted their valuation is CompactifAI . Unlike other methods, CompactifAI fundamentally restructures the neural network layers using Tensor Networks. The reported results are impressive: Compression Rate: Reduction of model size by up to 95% . Accuracy Retention: Maintains the original model's accuracy with marginal loss (2-3%). Inference Speed: Acceleration between 4x and 12x . Cost Savings: Reduction of inference costs between 50% and 80% . Beyond cloud savings, this technology is a critical enabler for Edge AI . It allows powerful models to run directly on devices with limited resources such as smartphones, autonomous cars, or drones, without relying on a cloud connection, improving latency and privacy. Business Strategy: Quantum Pragmatism Since its founding in 2019, Multiverse adopted a philosophy of \"Value-Based Quantum Computing.\" They moved away from speculation and focused on generating immediate ROI for their clients. Their strategy is based on several pillars: Hardware Agnosticism: Their software platform, Singularity , runs on both current quantum processors (from IBM, D-Wave, etc.) and classical hardware. This mitigates the risk of betting on immature hardware technology. Focus on Inference: They have strategically pivoted to address the AI inference market, projected at over $100 billion, where client pain (cost and efficiency) is acute and current. Deep Strategic Partnerships: They don't just sign marketing deals. They have deep technical collaborations that validate their technology in critical environments: Finance: With the Bank of Canada , simulating cryptocurrency adoption in complex economic networks intractable for classical computing. Energy: With Iberdrola , optimizing battery placement in the power grid to integrate renewables, outperforming classical benchmarks. Industry 4.0: With Bosch , integrating quantum algorithms into digital twins for defect detection. Consulting: With PwC Spain , which integrates CompactifAI into its service offering, acting as both a client and a distribution channel. Conclusion: A Bridge to the Quantum Era Multiverse Computing has achieved what few deep tech startups manage: navigating the \"Valley of Death\" between academic research and a viable commercial product. They have solved the dilemma of how to monetize quantum science before the hardware is ready. By applying quantum physics mathematics to solve today's most urgent AI bottleneck, they have not only built a solid business but have also positioned themselves as a critical piece of infrastructure in the global technology stack. They are a pragmatic bridge between today's supercomputers and tomorr",
    "url": "https://datalaria.com/en/posts/multiverse_computing/",
    "slug": "multiverse_computing",
    "lang": "en",
    "categories": [
      "Artificial Intelligence",
      "Deep Tech",
      "Business Strategy",
      "Quantum Computing"
    ],
    "tags": [
      "Multiverse Computing",
      "Generative AI",
      "Model Compression",
      "Tensor Networks",
      "Quantum-Inspired",
      "LLM",
      "Startup",
      "Investment"
    ],
    "date": "2026-01-31",
    "timestamp": 0,
    "domain": "General",
    "image": "cover.png"
  },
  {
    "objectID": "en_nanobanana-engineering",
    "title": "From Systems Engineering to Formula 1: Using AI (Nanobana) to Illustrate a Complex Roadmap",
    "description": "A case study on how I used the AI tool Nanobanana to transform the systems engineering V-Model into an understandable visual story using a Formula 1 analogy.",
    "content": "In the world of engineering, one of our biggest challenges isn't solving the technical problem, but communicating the solution . We often find ourselves in front of managers, clients, or other departments trying to explain complex processes with dry diagrams or indigestible, hundred-page documents. The result is usually blank stares and a disconnect between the technical team and stakeholders, leading to widespread confusion. Recently, I faced this very challenge: I needed to present a status and progress report for the implementation of a complex engineering process whose roadmap closely resembled the traditional V-Model of systems engineering . Instead of resorting to the usual diagram, I decided to experiment and find a more engaging analogy. What if I could use AI not just to analyze data, but to tell a story? This post is the chronicle and result ( Downloadable template ) of how I used Google's AI tool, Nanobanana , to transform an engineering roadmap into an attractive visual presentation, using an analogy that almost everyone understands and admires: the design of a Formula 1 car . The Challenge: Making the V-Model Understandable For those unfamiliar, the V-Model is a development model that represents the sequence of phases in a product's design or development lifecycle. Its great virtue is that it connects each design and development phase with its corresponding testing phase, ensuring that everything specified is verified. It sounds logical, but its standard graphical representation can be abstract and uninspiring for those who are not familiar with it. In my case, for the simile of the process I was presenting, I used a variation of the basic V-diagram as illustrated below, instead of the traditional V-diagram used in Hardware or Software design which has a deeper level of detail: The V-diagram would help make the initiative's roadmap easier to understand, but for the concept to truly sink in and leave a lasting image in each stakeholder's mind, I needed a powerful metaphor. And few things combine design, testing, and a clear performance objective like the creation of an F1 car. The Tool: Nanobanana to the Rescue To bring this idea to life, I turned to Nanobanana , an AI tool specializing in generating images and visuals with a clean and consistent style. Its strength lies in its ability to create images that are true to the requested context and to maintain coherence across different interactions‚Äîas was my case, where I sought to create an evolutionary flow of images for the design that followed the metaphor's narrative thread, instead of using loose, disconnected images.. The Visual Roadmap: Designing an F1 Car Phase by Phase The plan was simple: for each phase of the V-Model, I would generate an image representing the equivalent stage in the design of a race car. Here is the detail of each phase: Phase 1: Requirements Management In Engineering : This is the starting point. Here, the client's needs and the system's constraints are defined and documented. What must the product do? The F1 Analogy : This is the team's initial meeting. The engineers study the strict FIA regulations (the constraints), analyze data from the previous season, and listen to the driver's requests (the client). The goal is clear: to build a winning car. The Prompt for Nanobanana : \"Create an illustrative image of a pair of F1 engineers in a meeting room, analyzing a technical rulebook and sketching initial concepts on a whiteboard.\" Phase 2: Preliminary Design (System Architecture) In Engineering : The high-level architecture is defined. How will the system be structured? What are its main components and how do they interact? The F1 Analogy : The engineers design the overall concept of the car: weight distribution, aerodynamic philosophy, engine integration with the chassis. It's the conceptual skeleton. The Prompt for Nanobanana : \"Maintain a similar scene to the previous one but change the whiteboard in the background to a screen where a conceptual 3D model of an F1 car is being projected, showing its main interconnected systems (engine, chassis, aerodynamics).\" Phase 3: Detailed Design In Engineering : The individual components of the system are designed in detail. The F1 Analogy : The moment of ultra-specialization. The aerodynamics team designs every millimeter of the front wing; the suspension team, every titanium piece. These are the detailed blueprints for each component. The Prompt for Nanobanana : \"On this same stage, create an image in which an engineer is working on a detailed CAD design or blueprint of the F1 vehicle, visualizing it on the background projection.\" Phase 4: Implementation In Engineering : The \"coding\" or physical component construction phase. The F1 Analogy : The factory machines start running. Parts are machined, carbon fiber monocoques are molded. The blueprints become tangible reality. The Prompt for Nanobanana : \"Generate a scene with a similar illustration style but depicting the manufactur",
    "url": "https://datalaria.com/en/posts/nanobanana-engineering/",
    "slug": "nanobanana-engineering",
    "lang": "en",
    "categories": [
      "Tools",
      "Engineering"
    ],
    "tags": [
      "ai",
      "nanobanana",
      "systems engineering",
      "v-model",
      "visualization",
      "formula 1",
      "communication",
      "ppt",
      "pptx",
      "engineering",
      "development",
      "roadmap"
    ],
    "date": "2025-09-20",
    "timestamp": 0,
    "domain": "Product",
    "image": "cover.png"
  },
  {
    "objectID": "en_netflix",
    "title": "Netflix: How Data Forges an Empire in the Midst of the Streaming Wars",
    "description": "An analysis of Netflix as a data-driven company that uses Big Data and AI not only to recommend series but to make key strategic decisions and dominate in the 'Streaming Wars'.",
    "content": "In a world where a new streaming platform with a multitude of content seems to be born every month, the question is no longer how Netflix managed to revolutionize television, but how it manages to stay on the throne. The answer, though complex in its execution, is simple in its concept: an obsessive and radically data-centric culture. Long gone are the days when Netflix was the sole disruptor. Today, with nearly 300 million subscribers (mid-2024 data) and a content budget exceeding $17 billion annually , the company doesn't just create series and movies; it fights a daily battle for every minute of our attention. And in this war, its arsenal isn't just scripts, but algorithms. The Netflix DNA: A Data Obsession Since the DVD Era To understand the Netflix of today, one must travel back to its origins. Founded in 1997 as a DVD-by-mail rental service, the company showed its data-driven DNA very early on. In 2000, long before \"Big Data\" was a buzzword, it was already developing a recommendation system based on its users' ratings. This obsession culminated in 2006 with the legendary \"Netflix Prize\" : a million dollars for the team that could improve the accuracy of its recommendation algorithm by 10%. Although the winning solution was never implemented due to its complexity, the message was clear: Netflix was willing to invest massively to understand its users on an unprecedented level. The Evolution of Intelligence: Beyond \"House of Cards\" The production of House of Cards in 2011 was a milestone, a multi-million dollar bet based on data. It was a resounding success, but the current sophistication of Netflix makes that look like child's play. Extreme Personalization: The Art of the Perfect Thumbnail One of the most fascinating examples of data use at Netflix is the personalization of thumbnails . The artwork you see for Stranger Things is not the same as what I see. Netflix A/B tests dozens of images and, based on your viewing history, shows you the one you are most likely to interact with. Do you watch a lot of romantic comedies? You'll see an image of the main couple. Are you an action fan? You'll see a thumbnail with an explosion or a chase scene. This is a massive-scale application of psychology and statistics, designed to win your click in under two seconds. The Adam Sandler Anecdote: Ignoring the Critics In the mid-2010s, Adam Sandler's movies were receiving terrible reviews. However, Netflix's data told a different story: his older comedies had a massive and recurring global viewership on the platform. Based exclusively on this behavioral data and not on critics' opinions, Netflix signed Sandler to a multi-million dollar deal for several exclusive films. The result was a resounding audience success, proving that user data was a more reliable indicator of success than Hollywood reviews. The Button That Saves 195 Years a Day: \"Skip Intro\" The famous \"Skip Intro\" button wasn't a creative idea; it was a conclusion drawn from data. The Netflix team analyzed user behavior and detected that a large percentage of viewers manually skipped the first few minutes of episodes. After confirming this pattern across thousands of series, they developed the feature. According to a former executive, this button is pressed 136 million times a day, saving users a collective 195 years of time... every single day . The Global Phenomenon Hunter: The \"Squid Game\" Case This is perhaps the best example of the power of its global intelligence. Squid Game was not a Hollywood project. It was a South Korean series that Netflix's algorithms detected as a regional hit with an extremely high completion rate. By identifying this pattern, the platform promoted it globally, turning it into a cultural phenomenon and its most-watched series in history. Data as a Weapon in the \"Streaming Wars\" In today's saturated market, the battle is fierce. To understand the context, one need only look at the market distribution in the United States, one of the most representative battlegrounds. According to data from Statista for the first quarter of 2025, the market share of streaming platforms in the U.S. is divided as follows: Amazon Prime Video : 22% Netflix : 21% Max : 13% Disney+ : 12% Hulu : 11% Others : 21% With this landscape, Netflix's main objective is to reduce the cancellation rate ( churn rate ). To do this, it has made bold and controversial business decisions, all of them based on data. And although it seems that Amazon Prime Video has overtaken them in the U.S. market share, Netflix is still the king in terms of subscribers and revenue, with a very slight margin over its biggest rival. You will find more Statistics at Straits Research The Crackdown on Shared Accounts The decision to end password sharing was a high-risk gamble. For years, Netflix analyzed billions of data points (locations, devices, viewing patterns) to define what constituted a \"Netflix Household.\" Armed with this model, they launched a strategy that, against the predictions",
    "url": "https://datalaria.com/en/posts/netflix/",
    "slug": "netflix",
    "lang": "en",
    "categories": [
      "case-studies"
    ],
    "tags": [
      "netflix",
      "streaming",
      "big data",
      "ai",
      "disney",
      "churn",
      "algorithms"
    ],
    "date": "2025-09-15",
    "timestamp": 0,
    "domain": "General",
    "image": "cover_twitter.jpg"
  },
  {
    "objectID": "en_notebooklm-sql",
    "title": "Learning with AI - NotebookLM: From a Book to a Multimedia Learning Platform",
    "description": "My hands-on experience generating a multimedia learning and study platform from a book about SQL using Google's AI tool, NotebookLM.",
    "content": "In the current age of information and knowledge, we face a colossal challenge whenever we want to learn something new: mountains of raw information. Books, academic papers, documents of any kind, websites, videos, podcasts... Extracting genuinely useful knowledge from all these sources is an arduous task that consumes our most valuable resource: time. While there is inherent value in drifting through these seas of information to build and consolidate a foundation on any topic, I have always sought tools to optimize this process, allowing me to pinpoint key concepts and guidelines along the way. In this article, I'm going to talk about the most revolutionary tool I've encountered to date, one that is set to completely change my workflow: Google's NotebookLM . To illustrate what it is and to understand its potential, I've based this on a practical example using the very interesting book shared on freeCodeCamp by Daniel Garc√≠a Solla , How to Design Structured Database Systems Using SQL . With the help of NotebookLM, I transformed it into a complete multimedia learning arsenal. What is NotebookLM and Why Is It a Silent Revolution? Imagine a personal research assistant that has perfectly read and understood only the sources (documents, videos, podcasts, etc.) you provide . That is the essence of NotebookLM. Unlike other AIs that dive into the entire internet (regardless of the reliability of the sources), NotebookLM is exclusively based on your sources . This simple premise is its superpower. By being \"grounded\" in your documents, it eliminates hallucinations and generic answers . Its knowledge is your knowledge, making it a custom-built expert on any topic you feed it (PDFs, Google Docs, URLs, etc.). My Proving Ground: A Technical Book on SQL To put it to the test, I chose a detailed and specialized source: the complete book How to Design Structured Database Systems Using SQL shared by Daniel Garc√≠a Solla on freeCodeCamp . My goal was not just to \"summarize\" it, but to deconstruct and reconstruct it into formats adapted to different learning styles that would facilitate the process of assimilating its concepts. The first step was as simple as going to the NotebookLM tool and creating a new \"notebook\" by linking the URL with the book's content. In seconds, NotebookLM had not only processed it but was already offering me an initial summary and key questions. But this was just the beginning... From Information to Knowledge: My 4 Creations This is where the magic begins to happen. Using a series of specific, pre-defined prompts within the tool itself, I had NotebookLM generate several high-value learning resources: 1. The Mind Map: The Structure at a Glance üó∫Ô∏è To understand the book's architecture, I needed a bird's-eye view. For that, NotebookLM presents a very prominent button called Mind Map . By pressing it, as if by magic... A perfectly organized structure appeared, allowing me to navigate the book's concepts, from the relational model to complex queries, understanding how each piece fit into the whole. Ideal for structuring my study. 2. The Video Summary: The Educational Script üé¨ Next trick: could it generate a visual resource like an explanatory video of all this content? The idea was to create a short introductory video on the main concepts from the book, allowing me to see the forest before diving into the trees. To this end, NotebookLM offers a Video Summary button. With this, it puts on its video editor hat and, in a few minutes, generates an impressive video summary of key aspects of the book, presented clearly and didactically in a narrative format that keeps the viewer's attention. Video summary for the book \"How to Design Structured Database Systems Using SQL\" : Video summary generated by NotebookLM 3. The Audio Summary: The Podcast for Learning on the Move üéß Still amazed by NotebookLM's capabilities and wanting to review the book's concepts while traveling or working out, I launched the Audio Summary feature. Once again, as if by magic, it generated a natural-sounding audio podcast between two people that was easy to follow. The most surprising part was how the tool's AI could adapt the tone, format, and content in a very natural and engaging way to hook the listener. Audio summary for the book \"How to Design Structured Database Systems Using SQL\" : Audio summary generated by NotebookLM 4. The Study Guide Report: My Ultimate Review Companion üìù Finally, after consuming the other resources and having delved deeper into the parts of the book that most interested me, I tried the Reports feature offered by NotebookLM to consolidate and self-assess my knowledge. Specifically, I launched the Study Guide report, and the result was the creation of a series of questions, exercises, and a glossary, all perfectly extracted and formulated, creating an incredibly high-value review tool. Final Verdict: Who is NotebookLM For? After trying it in detail, I believe that NotebookLM is the definitive tool for deep work ",
    "url": "https://datalaria.com/en/posts/notebooklm-sql/",
    "slug": "notebooklm-sql",
    "lang": "en",
    "categories": [
      "Tools"
    ],
    "tags": [
      "ai",
      "notebooklm",
      "google",
      "learning",
      "productivity",
      "sql",
      "mind map"
    ],
    "date": "2025-08-31",
    "timestamp": 0,
    "domain": "Product",
    "image": "notebooklm_hero.png"
  },
  {
    "objectID": "en_onboarding",
    "title": "From '24 Days' to Immediate Success: Revolutionizing Onboarding with Artificial Intelligence",
    "description": "Traditional onboarding is broken, with an average time of 24 days and a cost of $4,000 per hire. This post explores how Generative AI and Intelligent Document Processing (IDP) transform bureaucratic 'infoxication' into a seamless integration, offering a 24/7 virtual mentor that can improve retention by 82%.",
    "content": "Do you remember your first day at your last job? The excitement probably quickly mixed with a mountain of forms, many documents in different inaccessible paths, outdated or incomplete manuals, work tools not available... or even lack of equipment and desk space in the office? If this sounds familiar, you are not alone. I am not an HR expert, but I have had to deal with avoiding all these problems with the onboarding of new talent, faced with a process that is currently painful, expensive, and, frankly, inefficient. The traditional onboarding process is broken. But there is a new frontier where Generative Artificial Intelligence (GenAI) and Intelligent Document Processing (IDP) are not only streamlining paperwork but are redefining the human experience of joining a company. Based on a recent analysis of document and information management in onboarding, we explore how to transform \"infoxication\" into effective integration. The Harsh Reality of Traditional Onboarding: Numbers That Hurt The current landscape regarding the incorporation of new talent is worrying in large tech companies. Despite the massive investment in recruitment, the critical \"landing\" phase remains a costly bottleneck, apart from also dampening the spark of motivation and limiting the potential of new workers. The data is overwhelming: The cost of time: The average time to complete an onboarding process is 24 days . That's almost a month of reduced productivity. The financial cost: The average cost per new hire hovers around $4,000 . The wrong focus: 58% of organizations admit that their onboarding focuses mainly on paperwork and processes, rather than on culture or development. The disconnect: Perhaps the most alarming figure is that only 12% of employees believe their company does a good job with onboarding. The consequence of this poor management is clear: 31% of workers have quit a job within the first six months. We are losing talent almost as fast as we hire it. The Problem: \"Drinking from a Firehose\" There are two critical pain points that saturate the new employee: The Document Burden (Bureaucracy): These are repetitive, manual processes prone to human error that consume HR team time. Information Overload (\"Infoxication\"): The new employee receives company policies, regulations, tool guides, and corporate culture all at once. It is impossible to retain so much information at once. The result is an overwhelmed employee who spends their first few weeks navigating bureaucracy instead of adding value. The Solution: A 24/7 Mentor Powered by AI The revolution in onboarding is not limited to digitizing existing documents; it is about fundamentally redefining how new employees interact with the company from the very first moment. The key proposal is to implement AI as a \"copilot\" or virtual mentor, a constant guide that accompanies the employee from day zero, facilitating their integration and accelerating their productivity. This transformation is achieved through the strategic combination of two powerful technologies: IDP (Intelligent Document Processing): Capable of extracting, understanding, and processing information from structured and semi-structured documents. GenAI (Generative Artificial Intelligence): Uses Large Language Models (LLMs) to understand natural language, context, and generate human-like responses. This technological duo addresses the two main challenges of traditional onboarding: 1. Automation of Bureaucracy (Document Management) The traditional process of collection and verification of documentation, equipment requests, access, and permissions is usually a bottleneck that involves multiple actors (the employee themselves, their manager, HR, IT, etc.), consuming valuable time and generating friction. AI transforms this manual and tedious process into an agile and automated workflow: Automatic Extraction and Validation of Data: The employee's documents (ID, passport, degrees, etc.) are uploaded to a secure platform. The IDP automatically extracts the relevant data, verifies its authenticity, and compares it with existing information, flagging any discrepancies for review. Tools: Platforms like DocuSign CLM , Abbyy FlexiCapture , or Kofax use IDP to streamline document management. Automatic Generation of Forms and Contracts: Based on the extracted information and position data, AI can automatically fill out tax forms, employment contracts, confidentiality agreements, and other legal documents, ready for digital signature. Orchestration of Requests (IT, Facilities, etc.): AI can automatically initiate workflows to request necessary computer equipment, create user accounts, assign access permissions to specific systems, request building access cards, etc., all based on the position profile. Example: A new software developer, upon being hired, automatically triggers the request for a laptop with specific specifications, access to the code repository, development tools, and team communication platforms, without manual interv",
    "url": "https://datalaria.com/en/posts/onboarding/",
    "slug": "onboarding",
    "lang": "en",
    "categories": [
      "Artificial Intelligence",
      "Human Resources",
      "Productivity"
    ],
    "tags": [
      "Onboarding",
      "GenAI",
      "IDP",
      "Employee Experience",
      "Talent Retention",
      "Document Management"
    ],
    "date": "2026-01-17",
    "timestamp": 0,
    "domain": "People",
    "image": "cover.png"
  },
  {
    "objectID": "en_sop_engineering-data-hygiene",
    "title": "S&OP: Why Your Excel Is Lying to You (and How to Interrogate It with Python)",
    "description": "Let's stop cleaning data manually. In this first chapter of the S&OP Engineering series, we automate data hygiene using Python, Supabase, and Statistics to detect the truth hidden behind the noise.",
    "content": "In S&OP (Sales & Operations Planning) meetings, opinions are often discussed instead of facts. \"I think we'll sell more\" , \"Last month was weird\" . The root problem is not the lack of business vision, it's the lack of signal integrity . Most supply chains are managed on spreadsheets that accept anything: dates as text, blank spaces, and typos that turn a 100-unit order into 100,000. When you feed your prediction algorithm with that \"garbage,\" you get amplified garbage (the financial Bullwhip effect). Today we kick off the S&OP Engineering series. We're not going to talk theory; we're going to build a data architecture that audits your business automatically. The Problem: Signal-to-Noise Ratio In telecommunications (my original background), noise is any interference that corrupts the signal. In Supply Chain, \"noise\" is dirty data. If you don't filter the noise before planning demand, you're immobilizing capital . An undetected outlier is money on fire. If your algorithm sees a false spike of 100,000 units, it will order raw materials you don't need, burning cash and taking up warehouse space. Data hygiene is not 'cleaning', it's protecting your operating margin. The Visual Evidence Before seeing a single line of code, look at the difference between what your ERP exports (top) and the statistical reality of your demand (bottom). Top: Raw data with human errors. Bottom: The clean signal ready for AI algorithms. The Solution: \"Quality Valve\" Architecture To solve this, we apply First Principles Thinking . We don't need to \"be more careful\" with Excel. We need a system that mathematically prohibits the entry of dirty data into our \"Single Source of Truth.\" We've designed an automated pipeline with the following stack: Brain: Python (Pandas + Scipy) for statistical logic. Warehouse: Supabase (PostgreSQL) as the \"Single Source of Truth.\" Agent: A script that runs automatically when new files arrive. The Code: Statistics > Intuition We don't use fixed rules (\"if greater than 1000, delete\"). We use statistics. We implement the Z-Score , which measures how many standard deviations a data point is from the mean. If a sale has a Z-Score > 3 (it's more than 3 sigmas from normality), it's mathematically unlikely to be standard behavior. The system doesn't delete it (it could be a real sale), but it flags it for audit and excludes it from automatic prediction. Note : We use Z-Score assuming normality to simplify this example. In production scenarios with intermittent demand, we use methods like IQR (Interquartile Range) or MAD (Median Absolute Deviation) which are more robust for non-Gaussian distributions. Here's the core logic from our SupplyChainSanitizer class: [code block] Open Kitchen: Try It Yourself As an engineer, I distrust what I can't execute. That's why I've isolated the cleaning logic in an interactive Colab Notebook . You don't need to install Python or configure databases. I've prepared an ephemeral environment where you can: Generate a corrupt sales dataset (simulated). Run the SupplyChainSanitizer cleaning engine. See how the algorithm detects and separates the noise. Click the button, hit \"Play\" on the cells, and observe data engineering in action. Production Architecture (Behind the Scenes) For technical profiles interested in how this scales in a real company (Datalaria Core): Ingestion: CSVs are uploaded to a private Bucket in Supabase Storage or a local database. Trigger: A Python worker detects the file. Processing: Executes the cleaning in memory (Docker Container). Persistence: Clean data is injected into PostgreSQL using Row Level Security (RLS) to ensure nobody can manually alter the financial history. Security Note: In production, we never connect scripts with superuser permissions. We use specific Service Roles and strict RLS policies to ensure supply chain integrity. Data Flow Visualization The following diagram shows how \"dirty\" data passes through our Quality Valve before reaching the Single Source of Truth: flowchart LR subgraph ORIGIN[\"üìÇ Origin\"] A[\"ERP CSV (Dirty Data)\"] end subgraph PIPELINE[\"üß† Quality Valve (Python)\"] B[\"structural_clean()<br/>Dates ¬∑ Nulls ¬∑ Duplicates\"] C[\"detect_outliers()<br/>Z-Score œÉ > 3\"] D[\"get_audit_report()<br/>Hygiene Metrics\"] end subgraph DESTINATION[\"üóÑÔ∏è Single Source of Truth\"] E[(\"Supabase<br/>PostgreSQL\")] F[\"Clean Data<br/>(Pure Signal)\"] G[\"Flagged Outliers<br/>(For Audit)\"] end A --> B --> C --> D D --> E E --> F E --> G style A fill:#ff6b6b,stroke:#c0392b,color:#fff style F fill:#2ecc71,stroke:#27ae60,color:#fff style G fill:#f39c12,stroke:#d35400,color:#fff style E fill:#3498db,stroke:#2980b9,color:#fff Legend: - üî¥ Red: Raw data with noise (the problem) - üü¢ Green: Clean signal ready for prediction - üü† Orange: Anomalies labeled for human review - üîµ Blue: Centralized warehouse (Supabase) Next Step: Scientific Prediction Now that we have a clean database (a pure signal), we're ready to look into the future. In the next chapter of the series, we'll co",
    "url": "https://datalaria.com/en/posts/sop_engineering-data-hygiene/",
    "slug": "sop_engineering-data-hygiene",
    "lang": "en",
    "categories": [
      "S&OP Engineering",
      "Data Engineering",
      "Python"
    ],
    "tags": [
      "Supply Chain",
      "S&OP",
      "Supabase",
      "Pandas",
      "Z-Score"
    ],
    "date": "2026-02-14",
    "timestamp": 0,
    "domain": "S&OP",
    "image": "cover.png"
  },
  {
    "objectID": "en_the-goal",
    "title": "The Goal is Not (Just) About Factories: Synchronizing Your Enterprise in the Age of AI",
    "description": "An in-depth analysis of Eli Goldratt's 'The Goal' and how the Theory of Constraints (TOC) applies beyond production, to engineering, project management, and the supply chain, empowered by AI and Industry 4.0.",
    "content": "In 1984, Eliyahu M. Goldratt published \"The Goal\" , a management novel that, disguised as a story about a struggling factory manager, sparked a quiet revolution. Many still associate this book exclusively with optimizing production lines. However, its underlying philosophy, the Theory of Constraints (TOC) , is an incredibly powerful systemic thinking framework that extends far beyond manufacturing. Today, in an era defined by high technology, software project management, global supply chains, and the explosion of AI, the principles of \"The Goal\" are, paradoxically, more relevant than ever. This post explores how TOC applies to the entire modern tech enterprise ‚Äî from engineering and procurement to program management ‚Äî and how it serves as a strategic compass to guide the powerful, yet costly, tools of Industry 4.0. The Dilemma: \"Cost World\" vs. \"Throughput World\" The first and greatest hurdle \"The Goal\" breaks down is traditional cost accounting. Goldratt argues that this metric is misleading, as it incentivizes \"local efficiencies\" that often harm the overall system. In a \"Cost World\" , a procurement manager is rewarded for finding a 5% cheaper supplier, and a production manager is incentivized to keep all machines running at 100% efficiency to \"absorb overhead.\" TOC demonstrates that this logic is fatally flawed in an interdependent system. Optimizing a resource that is not a bottleneck does not improve overall system performance; in fact, it often makes it worse, generating excess inventory (I) that consumes cash and increases operating expenses (OE). To escape this trap, Goldratt redefines the goal of any commercial enterprise (\"make money now and in the future\") with three simple operational metrics: Throughput (T): The rate at which the system generates money through sales (sales minus totally variable costs, such as raw materials). Inventory (I): All the money the system invests in purchasing things it intends to sell. TOC treats it as a liability, not an asset. Operating Expenses (OE): All the money the system spends to convert Inventory into Throughput (fixed costs, salaries, etc.). The true objective of the enterprise, therefore, is to: Increase Throughput (T) while simultaneously reducing Inventory (I) and Operating Expenses (OE). Under this new perspective (the \"Throughput World\" ), the decision of that purchasing manager changes. If that cheaper supplier is less reliable and causes a stoppage at the system's constraint, the lost Throughput will be immensely greater than the nominal cost saving. Throughput Accounting (the financial application of TOC) gives us the financial language to prioritize reliability and speed over unit purchase price at critical links in our chain. The Core of TOC: The 5 Focusing Steps (POOGI) TOC is not just a theory; it's a Process of Ongoing Improvement (POOGI) . The method for executing it is based on 5 Focusing Steps : IDENTIFY the Constraint: What resource, policy, or process dictates the pace of the entire system? It's not always a machine; it can be an overburdened senior engineer, market demand, or an absurd internal policy (like prohibiting overtime at the bottleneck). EXPLOIT the Constraint: Get the most out of the limiting resource without spending money. Ensure the bottleneck never idles for unnecessary reasons (waiting for materials, unnecessary meetings, setups). SUBORDINATE everything else: This is the most radical step. The entire system must operate at the pace of the constraint. Running non-constrained resources at 100% capacity is wasteful, as it only generates inventory (WIP) that the constraint cannot process. ELEVATE the Constraint: If, after exploiting and subordinating, we still need more capacity, only then do we invest capital (CAPEX) to improve that resource (buy another machine, hire another senior engineer). REPEAT (Prevent Inertia): As soon as we break one constraint, another part of the system will become the new bottleneck. The cycle must immediately restart at Step 1. This cycle is tactically implemented through Drum-Buffer-Rope (DBR) : Drum: The constraint, which sets the pace (the \"drumbeat\") for the entire system. Buffer: A time buffer (not inventory) strategically placed just before the constraint to ensure it never runs out of work. Rope: A communication signal that \"ties\" the release of new materials at the beginning of the process to the pace of the drum, thus preventing the system from being flooded with excessive Work In Progress (WIP). TOC as a GPS for Lean and Six Sigma A common confusion is viewing TOC, Lean , and Six Sigma as competing methodologies. In reality, they are synergistic. Lean focuses on eliminating waste (Muda). Six Sigma focuses on eliminating variability (defects). TOC focuses on managing the constraint to increase Throughput. The mistake is applying Lean and Six Sigma everywhere . What's the point of optimizing and reducing variability in a process that is not the constraint? We are only \"improving\" a resour",
    "url": "https://datalaria.com/en/posts/the-goal/",
    "slug": "the-goal",
    "lang": "en",
    "categories": [
      "Strategy",
      "Project Management",
      "AI"
    ],
    "tags": [
      "toc",
      "the-goal",
      "theory-of-constraints",
      "project-management",
      "supply-chain",
      "ai",
      "industry-4.0",
      "lean",
      "six-sigma",
      "critical-chain"
    ],
    "date": "2025-12-13",
    "timestamp": 0,
    "domain": "Projects",
    "image": "cover.png"
  },
  {
    "objectID": "en_the_thinking_game",
    "title": "The Thinking Game: How DeepMind Turned Video Games into History's Greatest Scientific Tool",
    "description": "A deep dive into the documentary 'The Thinking Game', chronicling the odyssey of Demis Hassabis and DeepMind. From Atari pixels to solving protein folding with AlphaFold, we explore how the quest for AGI is rewriting the rules of modern science.",
    "content": "In the era of unbridled hype for Generative Artificial Intelligence, it is easy to lose sight of the ultimate goal. While Silicon Valley competes to see who has the most eloquent chatbot, a lab in London has been pursuing a different, almost esoteric goal: to solve intelligence and then use it to solve everything else. The new documentary \"The Thinking Game\" (selected by the Tribeca Festival) is not just a film about technology; it is the chronicle of an intellectual obsession. Directed by Greg Kohs, the film follows Demis Hassabis and the Google DeepMind team over the course of a decade, narrating the transition from a cash-strapped startup to the architect of one of the 21st century's greatest scientific breakthroughs: AlphaFold [1] . This post analyzes the technical and philosophical pillars exposed by the documentary, breaking down how video games served as a training ground for hard science and why the quest for Artificial General Intelligence (AGI) is the ultimate \"Thinking Game.\" 1. The Architect: From Child Prodigy to AGI Visionary To understand DeepMind, one must understand Demis Hassabis. The documentary does an excellent job of connecting the dots of his biography, revealing that the architecture of his AIs is not accidental, but a reflection of his own multidisciplinary mind [2] . Hassabis is not your typical tech CEO. He was a chess prodigy (ranked second in the world in his category at age 13) and began programming at age 8. By 17, he was already working at Bullfrog Productions designing the AI for the iconic video game Theme Park . There, Hassabis experimented with a fascinating premise: simulating human behavior through autonomous agents. If you placed a food stall with too much salt next to a roller coaster, the virtual visitors would vomit. No one explicitly programmed the vomit; it was an emergent consequence [3] . After his stint in the video game industry, Hassabis pivoted toward neuroscience at Cambridge. His thesis: the human brain is the only \"existence proof\" we have that general intelligence is possible. DeepMind was born from that intersection: biological inspiration to build intelligent silicon [4] . 2. The Proving Ground: From Pixels to Intuition DeepMind's strategy was always clear: use games not as an end, but as a safe sandbox to train Reinforcement Learning algorithms. The documentary structures this progress into three critical phases that defined the last decade of AI: Phase 1: The Atari Era and Deep Learning The first major milestone was teaching an AI to play Atari games simply by \"looking\" at the pixels on the screen, without knowing the rules. The \"Eureka\" moment came with the game Breakout . After hours of training, the DQN agent not only learned to play but discovered an optimal strategy: digging a side tunnel to send the ball behind the wall of bricks. The developers did not program that tactic; the machine invented it [5] . Phase 2: AlphaGo and the \"Sputnik Moment\" The showdown against Lee Sedol in 2016 is the dramatic axis of the film's first half. Here, the documentary highlights the famous Move 37 . With a probability of 1 in 10,000 of being played by a human, that move was the definitive proof that AI had transcended brute calculation to enter the realm of creativity and intuition . Even more fascinating was the evolution into AlphaZero . Unlike its predecessor, which studied human games, AlphaZero learned from a tabula rasa (blank slate), playing against itself millions of times. It became its own teacher, eliminating centuries of human bias and discovering strategies that no Grandmaster had ever conceived [6] . Phase 3: The Fog of War with AlphaStar The leap to StarCraft II represented the challenge of imperfect information. Unlike Go, where the entire board is visible, in StarCraft there is \"fog of war.\" AlphaStar had to learn to explore, plan for the long term, and make decisions in real-time, reaching a Grandmaster level and demonstrating that AI could handle uncertainty [7] . 3. The Scientific Pivot: The AlphaFold Telescope This is where the documentary and DeepMind's mission reach their climax. Hassabis uses a brilliant analogy that helps understand the magnitude of the leap from games to biology: \"Building systems like AlphaGo was like learning to polish glass lenses with perfect precision. It was a difficult and technical art, but the ultimate goal wasn't to have pretty lenses on a shelf. AlphaFold was the moment they finally took those lenses, built a telescope, and pointed it at the biological sky, revealing galaxies of protein structures that were previously invisible to the human eye.\" The protein folding problem (how a 1D chain of amino acids folds into a functional 3D structure) had baffled biology for 50 years. Form determines function: if you know the form, you can understand diseases and design drugs [8] . The documentary shows with stark honesty the initial failure at the CASP13 competition, where they won but without the precision necessary ",
    "url": "https://datalaria.com/en/posts/the_thinking_game/",
    "slug": "the_thinking_game",
    "lang": "en",
    "categories": [
      "Artificial Intelligence",
      "Deep Tech",
      "Science",
      "Documentaries"
    ],
    "tags": [
      "DeepMind",
      "Demis Hassabis",
      "AlphaFold",
      "AGI",
      "AlphaGo",
      "The Thinking Game",
      "Computational Biology"
    ],
    "date": "2026-01-01",
    "timestamp": 0,
    "domain": "General",
    "image": "the_thinking_game.jpg"
  },
  {
    "objectID": "es_abraham_wald",
    "title": "La Arquitectura del Silencio: Abraham Wald y la Epistemolog√≠a de los Datos Ausentes",
    "description": "En 1943, el ej√©rcito estadounidense quer√≠a blindar los agujeros de bala de sus B-17. Un matem√°tico les dijo que blindaran los espacios vac√≠os. Esta es la historia de c√≥mo el Sesgo del Superviviente destruye los modelos de IA y las estrategias empresariales modernas.",
    "content": "En el entorno de alta presi√≥n de la ingenier√≠a de datos y la toma de decisiones ejecutiva, a menudo nos dejamos seducir por el dashboard. Confiamos en las filas de nuestras bases de datos SQL y en los logs de nuestros SIEMs porque son tangibles. Son lo que vemos. Pero en mi experiencia dise√±ando sistemas‚Äîdesde log√≠stica de la industria de defensa hasta infraestructuras cloud modernas‚Äîlos datos m√°s peligrosos no son los valores at√≠picos; son los datos que nunca llegaron a la base de datos. Hoy, nos alejamos del c√≥digo para examinar un \"Primer Principio\" fundamental del an√°lisis de datos: el Sesgo del Superviviente . Para ello, debemos remontarnos a 1943 y a la mente de un hombre que vio lo invisible. La Guerra de los N√∫meros La Segunda Guerra Mundial fue el primer conflicto donde la victoria dependi√≥ en gran medida del procesamiento de informaci√≥n y la aplicaci√≥n del rigor matem√°tico a la incertidumbre del campo de batalla. No solo se libr√≥ en las playas; se libr√≥ en oficinas donde las ecuaciones eran la munici√≥n y el enemigo era el error cognitivo. Las Fuerzas A√©reas Aliadas enfrentaban una crisis. Sus bombarderos estrat√©gicos, las Fortalezas Volantes B-17, sufr√≠an tasas de bajas catastr√≥ficas sobre Europa. La soluci√≥n intuitiva para el mando militar era simple: blindar los aviones. Pero el blindaje es un juego de suma cero; cada kilogramo de acero reduce la carga de bombas y la maniobrabilidad, parad√≥jicamente haciendo que el avi√≥n sea m√°s f√°cil de derribar. Los militares recurrieron a los datos. Analizaron los bombarderos que regresaban de las misiones y mapearon cada agujero de bala. Los datos hablaban con claridad seductora: las alas, el fuselaje central y la cola estaban plagados de da√±os. La l√≥gica militar, guiada por la evidencia emp√≠rica visible, dictaba reforzar estas √°reas \"heridas\". Tiene sentido, ¬øverdad? Refuerzas donde te impactan. Aqu√≠ es donde intervino Abraham Wald , un matem√°tico jud√≠o-h√∫ngaro del Statistical Research Group (SRG) de la Universidad de Columbia. Abraham Wald La Inversi√≥n de la L√≥gica Wald era un outsider. Excluido de la educaci√≥n formal en su juventud debido a su religi√≥n, desarroll√≥ una independencia intelectual radical. Mir√≥ los mismos diagramas que los generales pero lleg√≥ a la conclusi√≥n diametralmente opuesta. Su argumento era elegante y contraintuitivo: \"El blindaje no va donde est√°n los agujeros de bala. Va donde no est√°n los agujeros: en los motores y la cabina\" . ¬øC√≥mo lleg√≥ a esta conclusi√≥n? Haciendo la √∫nica pregunta que nadie m√°s hizo: ¬øD√≥nde est√°n los aviones que faltan? Wald asumi√≥ que el fuego antia√©reo alem√°n era aleatorio. No ten√≠a un sistema de gu√≠a que buscara las alas. Por lo tanto, los impactos deb√≠an estar uniformemente distribuidos. Si los aviones que regresaban a la base ten√≠an agujeros en el fuselaje pero motores intactos, no era porque los motores no fueran alcanzados. Era porque los aviones alcanzados en los motores nunca regresaron . Los \"puntos rojos\" en los diagramas no indicaban da√±o cr√≠tico; indicaban da√±o sobrevivible. Los espacios vac√≠os eran las zonas letales. Wald nos ense√±√≥ que la verdad a menudo reside no en los datos que tenemos, sino en el silencio de los datos que nos faltan. Las Matem√°ticas de la Supervivencia Aunque la historia se cuenta a menudo como un momento \"¬°Eureka!\", la contribuci√≥n de Wald fue un modelo estad√≠stico riguroso que involucraba complejas probabilidades condicionales. Ve√°moslo desde una perspectiva de ingenier√≠a simplificada. Wald esencialmente estableci√≥ una relaci√≥n inversa entre la densidad de da√±o observada y la vulnerabilidad. Si definimos la vulnerabilidad como la probabilidad de que un avi√≥n sea derribado dado un impacto en una zona espec√≠fica, y observamos la densidad de impactos en los supervivientes, la l√≥gica fluye de la siguiente manera: Si vulnerabilidad(motor) ‚âà 1 (Letal) ‚Üí Impactos en Motor en Supervivientes ‚âà 0 En otras palabras: si ser alcanzado en el motor casi siempre significa la muerte, entonces los aviones supervivientes casi nunca tendr√°n da√±os en el motor‚Äîno porque los motores no fueran alcanzados, sino porque esos aviones se estrellaron. Wald demostr√≥ que un B-17 que regresa con 100 agujeros en sus alas proporciona evidencia estad√≠stica de la robustez del ala. Por el contrario, la ausencia de datos sobre da√±os en la bomba de combustible indica un umbral de fallo cr√≠ticamente bajo. El Fantasma en la M√°quina: Implicaciones Modernas ¬øPor qu√© importa esto a un CTO o Ingeniero de Datos moderno? Porque el Sesgo del Superviviente es una epidemia en la econom√≠a digital. Estamos construyendo algoritmos y estrategias basadas en conjuntos de datos filtrados, a menudo con resultados desastrosos. 1. El Cementerio de Startups En el ecosistema de startups, estamos obsesionados con el \"Mito del Garaje\". Vemos a Bill Gates o Mark Zuckerberg abandonar la universidad y tener √©xito, as√≠ que inferimos que la educaci√≥n formal es un obst√°culo. Este es un error puramente Waldiano. Estamos analizando los \"bombard",
    "url": "https://datalaria.com/es/posts/abraham_wald/",
    "slug": "abraham_wald",
    "lang": "es",
    "categories": [
      "casos_exito"
    ],
    "tags": [
      "sesgo del superviviente",
      "abraham wald",
      "teor√≠a de decisiones",
      "machine learning",
      "ciberseguridad"
    ],
    "date": "2026-02-21",
    "timestamp": 0,
    "domain": "General",
    "image": "wald-cover.png"
  },
  {
    "objectID": "es_app-openweather_part1_backend",
    "title": "Proyecto Weather Service (Parte 1): Construyendo el Recolector de Datos con Python y GitHub Actions o Netlify",
    "description": "Primera entrega de la serie sobre c√≥mo construir un servicio meteorol√≥gico. Nos centramos en el backend: conectar a la API de OpenWeatherMap, almacenar datos en CSV y automatizar todo 24/7 gratis con GitHub Actions o Netlify.",
    "content": "Como coment√© en un post anterior, uno de mis objetivos con Datalaria es \"ensuciarme las manos\" con proyectos que me permitan aprender y conectar diferentes tecnolog√≠as del mundo de los datos. Hoy empezamos una serie dedicada a uno de esos proyectos: la creaci√≥n de un servicio meteorol√≥gico global completo , desde la recolecci√≥n de datos hasta su visualizaci√≥n y predicci√≥n, todo ello sin servidores y con herramientas gratuitas. En esta primera entrega, nos centraremos en el coraz√≥n del sistema: el backend recolector de datos . Veremos c√≥mo construir un \"robot\" que trabaje por nosotros 24/7, conect√°ndose a una API externa, guardando la informaci√≥n de forma estructurada y haciendo todo esto de manera autom√°tica y gratuita. ¬°Vamos all√°! El Primer Paso: Hablar con la API de OpenWeatherMap Todo servicio meteorol√≥gico necesita una fuente de datos. Eleg√≠ OpenWeatherMap por su popularidad y su generoso plan gratuito. El proceso inicial es sencillo: Registrarse : Crear una cuenta en su web. Obtener la API Key : Generar una clave √∫nica que nos identificar√° en cada llamada. Es como nuestra \"llave\" para acceder a sus datos. Guardar la Clave : ¬° Nunca directamente en el c√≥digo! Hablaremos de esto m√°s adelante. Con la clave en mano (¬°o casi!), escrib√≠ un primer script test_clima.py para probar la conexi√≥n usando la maravillosa librer√≠a requests de Python: [code block] Primer Obst√°culo Superado (con Paciencia): Al ejecutarlo por primera vez, ¬°error 401: No Autorizado! üò± Resulta que las API Keys de OpenWeatherMap pueden tardar unas horas en activarse despu√©s de generarlas. La lecci√≥n: a veces, la soluci√≥n es simplemente esperar. ‚è≥ La \"Base de Datos\": ¬øPor Qu√© CSV y No SQL? Con los datos fluyendo, necesitaba almacenarlos. Podr√≠a haber montado una base de datos SQL (PostgreSQL, MySQL...), pero eso implicaba complejidad, un servidor (coste) y, para este proyecto, era matar moscas a ca√±onazos. Opt√© por la simplicidad radical: ficheros CSV (Comma Separated Values) . Ventajas : F√°ciles de leer y escribir con Python, perfectamente versionables con Git (podemos ver el historial de cambios), y suficientes para el volumen de datos que manejar√≠amos inicialmente. L√≥gica Clave : Necesitaba a√±adir una nueva fila cada d√≠a a cada fichero de ciudad, pero escribiendo la cabecera ( fecha_hora , ciudad , temperatura_c , etc.) solo la primera vez. La librer√≠a csv nativa de Python y os.path.exists lo hacen trivial: [code block] El Robot de Automatizaci√≥n: GitHub Actions al Rescate ü§ñ Aqu√≠ viene la magia: ¬øc√≥mo hacer que este script se ejecute solo todos los d√≠as sin tener un servidor encendido? La respuesta es GitHub Actions , el motor de automatizaci√≥n integrado en GitHub. Es como tener un peque√±o robot que trabaja gratis para nosotros. La Seguridad Primero: ¬°Nunca Subas tu API Key! El error m√°s grave ser√≠a subir registrar_clima.py con la API_KEY escrita directamente. Cualquiera podr√≠a verla en GitHub. * Soluci√≥n : Usar los Secretos de Repositorio de GitHub. 1. Ve a Settings > Secrets and variables > Actions en tu repositorio de GitHub. 2. Crea un nuevo secreto llamado OPENWEATHER_API_KEY y pega ah√≠ tu clave. 3. En el script Python, lee la clave de forma segura usando os.environ.get(\"OPENWEATHER_API_KEY\") . El Cerebro del Robot: El Fichero .github/workflows/actualizar-clima.yml Este fichero YAML le dice a GitHub Actions qu√© hacer y cu√°ndo: [code block] ¬°Este √∫ltimo paso es crucial! La propia Action act√∫a como un usuario, haciendo git add , git commit y git push de los ficheros CSV que el script Python acaba de modificar. As√≠, los datos actualizados quedan guardados en nuestro repositorio cada d√≠a. La Alternativa Serverless: Despliegue y Automatizaci√≥n con Netlify üöÄ Aunque GitHub Actions es una herramienta fant√°stica para la automatizaci√≥n, para este proyecto he decidido explorar una alternativa a√∫n m√°s integrada con el concepto de \"serverless\": Netlify . Netlify no solo nos permite desplegar nuestro frontend est√°tico (como GitHub Pages), sino que tambi√©n ofrece funciones serverless y, lo que es clave para nuestro backend, funciones de ejecuci√≥n programadas (Scheduled Functions o Cron Jobs) . Desplegando el Frontend Est√°tico con Netlify Conectar tu Repositorio : El proceso es incre√≠blemente sencillo. Inicia sesi√≥n en Netlify, haz clic en \"Add new site\" y selecciona \"Import an existing project\". Conecta con tu cuenta de GitHub y elige el repositorio de tu proyecto Weather Service. Configuraci√≥n B√°sica : Netlify detectar√° autom√°ticamente tu proyecto. Aseg√∫rate de que la \"Build command\" est√© vac√≠a (ya que es un sitio est√°tico sin proceso de build) y que el \"Publish directory\" sea la ra√≠z de tu repositorio ( ./ ). Despliegue Continuo : Netlify configurar√° autom√°ticamente el despliegue continuo. Cada vez que hagas un git push a la rama main (o la que hayas configurado), Netlify reconstruir√° y desplegar√° tu sitio. Automatizando el Backend con Netlify Functions (y Cron Jobs) Aqu√≠ es donde Netlify Serverless Functions brillan para nuestro recolector de d",
    "url": "https://datalaria.com/es/posts/app-openweather_part1_backend/",
    "slug": "app-openweather_part1_backend",
    "lang": "es",
    "categories": [
      "Proyectos",
      "Herramientas"
    ],
    "tags": [
      "python",
      "api",
      "github actions",
      "automatizacion",
      "serverless",
      "datos",
      "backend",
      "netlify"
    ],
    "date": "2025-10-31",
    "timestamp": 0,
    "domain": "Projects",
    "image": "cover.png"
  },
  {
    "objectID": "es_app_conversor_unidades",
    "title": "Creando mi 'Navaja Suiza' Digital: Un Conversor de Unidades a Medida con la IA",
    "description": "Un caso pr√°ctico de c√≥mo utilic√© Gemini Canvas para desarrollar desde cero una aplicaci√≥n web de conversi√≥n de unidades de medida, organizada por categor√≠as y adaptada a mis necesidades profesionales.",
    "content": "En cualquier disciplina t√©cnica o de ingenier√≠a, nuestro d√≠a a d√≠a est√° lleno de peque√±as pero constantes tareas de conversi√≥n de unidades de medida. Pasamos de Pascales a PSI, de Kilovatios a Caballos de potencia, de Gigabytes a Terabytes o de frecuencias a longitudes de onda.. En mi etapa universitaria dispon√≠amos de las llamadas chuletas o cheatsheets , las cuales sol√≠amos tener a mano ya sea pegadas a las calculadoras, carpetas, cuadernos... para nuestro d√≠a a d√≠a (no para los ex√°menes por supuesto üòÑ). Una vez ya en el √°mbito profesional y con la proliferaci√≥n de todo tipo de Apps y p√°ginas web, actualmente ya no es necesario tirar de estos recursos dado que tenemos multitud de opciones disponibles que nos permiten r√°pidamente consultar cualquier conversi√≥n o unidad de medida por rara que sea. No obstante, ante tal maremagnum de opciones y siendo esta una tarea tan recurrente y necesaria, me plante√© un reto: ¬øpodr√≠a crear mi propio conversor de unidades? Uno que fuera r√°pido, limpio, visualmente atractivo y, sobre todo, un proyecto vivo , una especie de \"navaja suiza digital\" que pudiera ampliar con el tiempo seg√∫n mis necesidades personales y profesionales. La respuesta, como en otros proyectos del blog, la encontr√© en la IA. Este post es la descripci√≥n de mi conversor de unidades propio recogiendo su desarrollo desde cero y apoyado en la funcionalidad Canvas de Gemini para crear una aplicaci√≥n web con m√∫ltiples p√°ginas y categor√≠as. El Proceso: De la Idea a la concepci√≥n El objetivo era crear una aplicaci√≥n web moderna, intuitiva con una navegaci√≥n clara por categor√≠as, fluida y f√°cil de usar. En lugar de empezar a escribir c√≥digo desde cero, inici√© una conversaci√≥n con Gemini, describiendo mi visi√≥n: \"Crea la estructura de una aplicaci√≥n web de varias p√°ginas HTML. La aplicaci√≥n ser√° un conversor de formatos y unidades de medida. La p√°gina principal debe mostrar tarjetas para las categor√≠as: Dimensi√≥n, Energ√≠a, Tiempo, Mec√°nica, Inform√°tica y Radiofrecuencia con conversiones b√°sicas. Cada tarjeta debe enlazar a una p√°gina de conversi√≥n dedicada a la categor√≠a correspondiente desde una p√°gina de inicio donde adem√°s de las tarjetas de navegaci√≥n a cada categor√≠a debe haber un conversor de formato de s√≠mbolo \".\" y \",\" para miles y decimales en n√∫meros\" La funcionalidad Canvas de Gemini me permiti√≥ ver en tiempo real c√≥mo la IA generaba no solo el c√≥digo, sino la estructura completa del proyecto. A partir del boceto inicial y tras un par de iteraciones adicionales centradas en refinar algo m√°s el dise√±o y afinar las unidades utilizadas, vio la luz una primera versi√≥n operativa de la aplicaci√≥n: Mi aplicaci√≥n de conversi√≥n de unidades de medida Un Vistazo a las Categor√≠as y sus Unidades Lo que hace √∫til a una herramienta como esta es entender qu√© estamos convirtiendo . Por eso, el \"Conversor Universal Pro\" no solo calcula, sino que busca ser did√°ctico. Estas son las categor√≠as iniciales y la ciencia detr√°s de sus unidades: 1. Dimensi√≥n: Midiendo el Espacio que Nos Rodea Esta categor√≠a agrupa las medidas fundamentales del espacio f√≠sico. Longitud: ¬øQu√© es? Es la medida de una dimensi√≥n, la distancia entre dos puntos. ¬øC√≥mo se calcula? Todas las conversiones se calculan tomando el metro (m) como unidad de referencia. La f√≥rmula convierte el valor inicial a metros y luego a la unidad final. √Årea: ¬øQu√© es? Es la medida de una superficie bidimensional. ¬øC√≥mo se calcula? De forma similar, el c√°lculo se estandariza usando el metro cuadrado (m¬≤) como unidad base. Volumen: ¬øQu√© es? Es la medida del espacio que ocupa un objeto en tres dimensiones. ¬øC√≥mo se calcula? La unidad base para el volumen en la aplicaci√≥n es el litro (l) , facilitando la conversi√≥n entre unidades m√©tricas y otras como los galones o las tazas. 2. Energ√≠a: La Capacidad de Realizar un Trabajo Aqu√≠ agrupamos las unidades que describen c√≥mo se transfiere y se utiliza la energ√≠a. Energ√≠a: ¬øQu√© es? Es la capacidad de un sistema para realizar un trabajo. ¬øC√≥mo se calcula? El Julio (J) es la unidad base del Sistema Internacional. A partir de √©l, se realizan las conversiones a calor√≠as, vatios-hora, etc. Potencia: ¬øQu√© es? Es la velocidad a la que se transfiere la energ√≠a o se realiza un trabajo. No es lo mismo tener energ√≠a que poder usarla r√°pidamente. ¬øC√≥mo se calcula? La unidad base es el Vatio (W) , que equivale a un Julio por segundo. Temperatura: ¬øQu√© es? Es una medida de la energ√≠a t√©rmica o el calor de un cuerpo. ¬øC√≥mo se calcula? A diferencia de otras, la temperatura no usa un factor de conversi√≥n simple. La aplicaci√≥n utiliza f√≥rmulas espec√≠ficas, convirtiendo siempre la entrada a grados Celsius (¬∞C) como paso intermedio para luego calcular la unidad de salida (Fahrenheit o Kelvin). 3. Inform√°tica: El Mundo de los Bits y los Bytes Las unidades que definen nuestro mundo digital. Almacenamiento de Datos: ¬øQu√© es? Mide la capacidad de guardar informaci√≥n digital. ¬øC√≥mo se calcula? La unidad fundamental es el Byte (B) . Es importante destacar q",
    "url": "https://datalaria.com/es/posts/app_conversor_unidades/",
    "slug": "app_conversor_unidades",
    "lang": "es",
    "categories": [
      "Proyectos",
      "Herramientas"
    ],
    "tags": [
      "gemini",
      "ia",
      "desarrollo web",
      "conversor de unidades",
      "html",
      "css",
      "javascript",
      "ingenier√≠a"
    ],
    "date": "2025-10-04",
    "timestamp": 0,
    "domain": "General",
    "image": "cover.png"
  },
  {
    "objectID": "es_app_flashcards",
    "title": "Programando con IA: Creando mi Propia App m√°gica de Flashcards para Estudiar",
    "description": "Un caso pr√°ctico de c√≥mo una necesidad familiar me llev√≥ a crear una aplicaci√≥n de estudio con flashcards desde cero, apoy√°ndome en la IA para hacer el aprendizaje m√°s ameno y eficaz.",
    "content": "La tecnolog√≠a brilla de verdad cuando resuelve un problema real, por peque√±o que sea. Hace unos d√≠as, me encontr√© en una situaci√≥n que muchos padres y madres reconocer√°n: ayudando a mi hijo mayor a repasar una lecci√≥n de ciencias. La dificultad a√±adida era que la asignatura es biling√ºe, por lo que no solo deb√≠a memorizar t√©rminos como \"articulaciones\" o \"columna vertebral\", sino tambi√©n su traducci√≥n y fon√©tica en ingl√©s. Nuestro m√©todo inicial fue el cl√°sico: el ancestral arte de \"tapar con la mano\" para adivinar la respuesta. Era funcional, pero mon√≥tono, poco motivador y no muy efectivo. Mientras luch√°bamos por mantener la concentraci√≥n, me surgi√≥ una idea: ¬øy si en lugar de luchar contra la distracci√≥n, la combatimos con una herramienta mejor? ¬øPodr√≠a crear, con la ayuda de la IA, una peque√±a aplicaci√≥n de estudio a medida en cuesti√≥n de minutos? Este post es el resultado de ese experimento, la creaci√≥n de una aplicaci√≥n propia de estudio con flashcards (tarjetas de memoria) . Vamos a ver las especificaciones de partida, el proceso de creaci√≥n y, lo m√°s importante, el resultado final listo para su uso. M√°s All√° de Anki y Quizlet: La B√∫squeda de la Simplicidad a Medida Existen herramientas de estudio incre√≠blemente potentes como Quizlet, AnkiApp o ProProfs. Son fant√°sticas, y con much√≠simas posibilidades. No obstante, a menudo vienen con una curva de aprendizaje o una cantidad de opciones que, para una necesidad inmediata y concreta, pueden resultar abrumadoras. Principalmente yo no necesitaba un ecosistema social para los conceptos requeridos ni m√∫ltiples m√©todos espaciados de aprendizaje. La premisa prioritaria era disponer de una soluci√≥n r√°pida y con unos requisitos muy espec√≠ficos: Entrada de datos flexible : Poder crear listas de t√©rminos manualmente, pero tambi√©n la posibilidad de generar traducciones o definiciones de forma autom√°tica. Gamificaci√≥n simple : A√±adir un elemento de juego con puntos e im√°genes para mantener el inter√©s de un ni√±o. Enfoque biling√ºe : Facilitar el repaso de t√©rminos en varios idiomas. Sin distracciones : Una interfaz limpia, directa al grano. Con estos objetivos en mente, y apoy√°ndome en las mismas t√©cnicas de \"copilotaje con IA\" que he explorado en otros posts, naci√≥ la aplicaci√≥n \"Flashcards de Estudio\" . La Soluci√≥n: Presentando la App \"Flashcards de Estudio\" La aplicaci√≥n est√° dise√±ada para ser minimalista pero potente, ofreciendo tres formas de empezar a estudiar en segundos: 1. Modo Manual: El Control Total Es la forma m√°s directa. Permite a√±adir filas de \"T√©rmino\" y \"Definici√≥n\" una por una. Es perfecto para listas de repaso cortas o para cuando tienes el material ya preparado y solo quieres digitalizarlo r√°pidamente para empezar a estudiar. 2. Modo Autom√°tico (IA): El toque m√°gico Aqu√≠ es donde la magia de la IA entra en juego. En este modo, solo necesitas escribir un t√©rmino y la IA se encarga de generar la definici√≥n o la traducci√≥n autom√°ticamente en el idioma que elijas. Para la lecci√≥n de ciencias de mi hijo, simplemente introduje la lista de palabras en espa√±ol y la IA complet√≥ al instante su traducci√≥n al ingl√©s y una aproximaci√≥n de su fon√©tica. Es un ahorro de tiempo espectacular. Para poder utilizarlo, es necesario generar una Key gratuita en Google AI Studio e incluirla en el campo de \"Tu Clave API de Gemini\" para utilizar el motor de IA de Gemini para la generaci√≥n de contenido. La aplicaci√≥n dispone de traducciones y generaci√≥n de definiciones en varios idiomas: 3. Importaci√≥n desde Fichero: Para los Power Users Para listas m√°s largas (vocabularios de un tema completo, listas de capitales, etc.), la aplicaci√≥n permite subir un fichero de texto ( .txt ) o CSV ( .csv ) con los t√©rminos y definiciones. Simplemente preparas tu lista en un fichero, la subes y la aplicaci√≥n genera las tarjetas al instante. Esta funci√≥n es ideal para aquellos que quieran preparar material para sus hijos o para estudiantes que necesiten digitalizar temas enteros. Plantilla de importaci√≥n Funcionamiento: F√°cil y entretenido Una vez seleccionados los t√©rminos y el modo de funcionamiento, simplemente se trata de empezar a estudiar y durante su ejecuci√≥n se ir√°n mostrando tarjetas para que el usuario practique dichas traducciones o t√©rminos antes de voltearlas e indicar si ha acertado o fallado. Si acierta, la aplicaci√≥n sumar√° 2 puntos y aparecer√° un emoticono y un mensaje de felicitaci√≥n, si falla, no sumar√° puntos yaparecer√° un mensaje de √°nimo. En estos casos, los fallos son registrados para que una vez finalizada la ejecuci√≥n se puedan volver a repasar nuevamente hasta asegurarnos que ya los dominamos. Si todas las respuestas son acertadas con √©xito, al finalizar se mostrar√° un gif animado de celebraci√≥n y desde esta pantalla podemos volver a estudiar todos los t√©rminos de nuevo o volver a la pantalla de inicio para generar nuevos contenidos. Un Proyecto Vivo y Abierto a la Comunidad Lo que empez√≥ como una soluci√≥n para una tarde de estudio se ha convertido en un proyecto ",
    "url": "https://datalaria.com/es/posts/app_flashcards/",
    "slug": "app_flashcards",
    "lang": "es",
    "categories": [
      "Proyectos",
      "Herramientas"
    ],
    "tags": [
      "ia",
      "flashcards",
      "aprendizaje",
      "estudio",
      "desarrollo web",
      "no-code",
      "gamificacion"
    ],
    "date": "2025-10-17",
    "timestamp": 0,
    "domain": "General",
    "image": "Cover.PNG"
  },
  {
    "objectID": "es_app_openweather_part2_frontend",
    "title": "Proyecto Weather Service (Parte 2): Construyendo el Frontend Interactivo con GitHub Pages o Netlify y JavaScript",
    "description": "Segunda entrega del proyecto Weather Service. Nos adentramos en el frontend: sirviendo un dashboard din√°mico con GitHub Pages o Netlify, leyendo datos CSV con PapaParse.js y creando gr√°ficos interactivos con Chart.js.",
    "content": "En la primera parte de esta serie , sentamos las bases de nuestro servicio meteorol√≥gico global. Construimos un script de Python para obtener datos del clima de OpenWeatherMap, los almacenamos eficientemente en ficheros CSV separados por ciudad y automatizamos todo el proceso de recolecci√≥n utilizando GitHub Actions. Nuestro \"robot\" est√° diligentemente recopilando datos 24/7. Pero, ¬øde qu√© sirven los datos si no puedes verlos? Hoy, cambiamos nuestro enfoque al frontend : la construcci√≥n de un dashboard interactivo y f√°cil de usar que permita a cualquiera explorar los datos meteorol√≥gicos que hemos recopilado. Aprovecharemos el poder del alojamiento de sitios est√°ticos con GitHub Pages o Netlify , utilizaremos JavaScript \"vainilla\" para darle vida y nos apoyaremos en algunas excelentes librer√≠as para el manejo y la visualizaci√≥n de datos. ¬°Hagamos que nuestros datos brillen! Alojamiento Web Gratuito: GitHub Pages vs. Netlify El primer obst√°culo para cualquier proyecto web es el alojamiento. Los servidores tradicionales pueden ser costosos y complejos de gestionar. Siguiendo nuestra filosof√≠a \"serverless y gratis\", tanto GitHub Pages como Netlify son soluciones perfectas para alojar sitios web est√°ticos directamente desde tu repositorio de GitHub. Opci√≥n 1: GitHub Pages Permite alojar sitios web est√°ticos directamente desde tu repositorio de GitHub. La activaci√≥n es trivial: 1. Ve a Settings > Pages en tu repositorio. 2. Selecciona tu rama main (o la rama que contenga tu contenido web) como fuente. 3. Elige la carpeta /root (o una carpeta /docs si lo prefieres) como la ubicaci√≥n de tus archivos web. 4. Haz clic en Save . Y as√≠, tu archivo index.html (y cualquier recurso vinculado) se vuelve accesible p√∫blicamente en una URL como https://tu-usuario.github.io/tu-nombre-de-repositorio/ . ¬°Sencillo, efectivo y gratuito! üöÄ Opci√≥n 2: Netlify (¬°la elecci√≥n final para este proyecto!) Para este proyecto, finalmente he optado por Netlify por su flexibilidad, la facilidad para gestionar dominios personalizados y su integraci√≥n con el despliegue continuo. Adem√°s, me permite alojar el proyecto directamente bajo mi dominio de Datalaria ( https://datalaria.com/apps/weather/ ). Pasos para desplegar en Netlify: Conectar tu Repositorio : Inicia sesi√≥n en Netlify. Haz clic en \"Add new site\" y luego en \"Import an existing project\". Conecta tu cuenta de GitHub y selecciona el repositorio de tu proyecto Weather Service. Configuraci√≥n de Despliegue : Owner : Tu cuenta de GitHub. Branch to deploy : main (o la rama donde tengas tu c√≥digo frontend). Base directory : Deja esto vac√≠o si tu index.html y assets est√°n en la ra√≠z del repositorio, o especifica una subcarpeta si es el caso (ej., /frontend ). Build command : D√©jalo vac√≠o, ya que nuestro frontend es puramente est√°tico sin necesidad de un paso de build (sin frameworks como React/Vue). Publish directory : . (o la subcarpeta que contenga tus archivos est√°ticos, ej., /frontend ). Desplegar Sitio : Haz clic en \"Deploy site\". Netlify tomar√° tu repositorio, lo desplegar√° y te proporcionar√° una URL aleatoria. Dominio Personalizado (Opcional pero recomendado) : Para usar un dominio como datalaria.com/apps/weather/ : Ve a Site settings > Domain management > Domains > Add a custom domain . Sigue los pasos para a√±adir tu dominio y configurarlo con los DNS de tu proveedor (a√±adiendo registros CNAME o A ). Para la ruta espec√≠fica ( /apps/weather/ ), necesitar√°s configurar una \"subcarpeta\" o \"base URL\" en tu aplicaci√≥n si no est√° directamente en la ra√≠z del dominio. En este caso, nuestro index.html est√° dise√±ado para ser servido desde una subruta. Netlify gestiona esto de forma transparente una vez que el sitio est√° desplegado y tu dominio configurado. ¬°As√≠ de sencillo! Cada git push a tu rama configurada activar√° un nuevo despliegue en Netlify, manteniendo tu dashboard siempre actualizado. La Pila Tecnol√≥gica del Frontend: HTML, CSS y JavaScript (con una peque√±a ayuda) Para este dashboard, opt√© por un enfoque ligero: HTML puro para la estructura, un poco de CSS para los estilos y JavaScript \"vainilla\" (sin frameworks complejos) para la interactividad. Para manejar tareas espec√≠ficas, incorpor√© dos librer√≠as fant√°sticas: PapaParse.js : El mejor parser de CSV del lado del cliente para el navegador. Es el puente entre nuestros archivos CSV en bruto y las estructuras de datos de JavaScript que necesitamos para la visualizaci√≥n. Chart.js : Una potente y flexible librer√≠a de gr√°ficos JavaScript que facilita enormemente la creaci√≥n de gr√°ficos bonitos, responsivos e interactivos. La L√≥gica del Dashboard: Dando Vida a los Datos en index.html Nuestro index.html act√∫a como el lienzo principal, orquestando la obtenci√≥n, el parseo y la representaci√≥n de los datos meteorol√≥gicos. 1. Carga Din√°mica de Ciudades En lugar de codificar una lista de ciudades, queremos que nuestro dashboard se actualice autom√°ticamente si a√±adimos nuevas ciudades en el backend. Lo logramos obteniendo un simple archivo ciudades.",
    "url": "https://datalaria.com/es/posts/app_openweather_part2_frontend/",
    "slug": "app_openweather_part2_frontend",
    "lang": "es",
    "categories": [
      "Proyectos",
      "Herramientas"
    ],
    "tags": [
      "javascript",
      "frontend",
      "github pages",
      "html",
      "css",
      "papaparse",
      "chartjs",
      "serverless",
      "visualizacion-datos",
      "netlify"
    ],
    "date": "2025-11-08",
    "timestamp": 0,
    "domain": "Projects",
    "image": "cover.png"
  },
  {
    "objectID": "es_app_openweather_part3_ai_prediction",
    "title": "Proyecto Weather Service (Parte 3): Prediciendo el Futuro con IA y OpenWeatherMap",
    "description": "La entrega final de nuestro proyecto Weather Service. Nos adentramos en la adici√≥n de capacidades predictivas, combinando pron√≥sticos oficiales de OpenWeatherMap con nuestro modelo de IA (Regresi√≥n Lineal) personalizado para predecir el tiempo de ma√±ana y visualizar su precisi√≥n.",
    "content": "En la primera parte de esta serie , establecimos la columna vertebral de nuestro servicio meteorol√≥gico global, recolectando datos brutos utilizando Python y GitHub Actions. Luego, en la Parte 2 , transformamos esos datos en un hermoso dashboard interactivo, aprovechando GitHub Pages/Netlify, JavaScript, PapaParse.js y Chart.js. Ahora, es el momento del gran final: a√±adir capacidad predictiva a nuestro Weather Service. Exploraremos c√≥mo aumentar nuestra visualizaci√≥n de datos hist√≥ricos con pron√≥sticos reales. Esta entrega se centra en un enfoque dual: integrar un pron√≥stico oficial y fiable de un servicio profesional (OpenWeatherMap) y, lo que es m√°s emocionante, construir y entrenar nuestro propio modelo de IA simple (Regresi√≥n Lineal) para predecir el tiempo de ma√±ana bas√°ndonos en los datos hist√≥ricos que hemos recolectado meticulosamente. Finalmente, visualizaremos ambos pron√≥sticos en nuestro dashboard, permitiendo una comparaci√≥n directa y una prueba real de la precisi√≥n de nuestra IA. ¬°Convirtamos nuestros datos en una bola de cristal! üîÆ El N√∫cleo Predictivo: OpenWeatherMap y Nuestra IA Personalizada El objetivo de esta funcionalidad predictiva era doble: Pron√≥stico Oficial : Obtener un pron√≥stico fiable y a varios d√≠as de un servicio meteorol√≥gico profesional (OpenWeatherMap - OWM). Predicci√≥n de IA Personalizada : Crear nuestro propio modelo de IA simple (Regresi√≥n Lineal) entrenado con los datos hist√≥ricos que hemos recolectado, para predecir el tiempo del d√≠a siguiente. Visualizaci√≥n y Comparaci√≥n : Mostrar y comparar ambos pron√≥sticos para medir la precisi√≥n y el rendimiento de nuestro modelo de IA personalizado. 1. ‚öôÔ∏è L√≥gica Backend: read_weather.py se Vuelve m√°s Inteligente Nuestro script read_weather.py , anteriormente responsable de la recolecci√≥n de datos, ahora ampl√≠a su funci√≥n para recopilar datos tanto de OWM como de nuestros archivos hist√≥ricos, consolidando todo en un √∫nico fichero predicciones.json . Paso 1: Obtener el Pron√≥stico a 5 D√≠as de OpenWeatherMap Decidimos que, adem√°s de la predicci√≥n de IA a 1 d√≠a, un pron√≥stico a 5 d√≠as de OWM proporcionar√≠a un contexto valioso. API Endpoint : Optamos por la API gratuita data/2.5/forecast (ya que OneCall 3.0 requer√≠a un m√©todo de pago). Procesamiento de Datos : Esta API devuelve datos en bloques de 3 horas. Tuvimos que a√±adir l√≥gica en Python para: Iterar sobre la lista de ~40 pron√≥sticos. Agruparlos por d√≠a (ignorando el d√≠a actual). Para cada uno de los 5 d√≠as siguientes, calcular la temperatura m√°xima, m√≠nima y media de todos los bloques de 3 horas dentro de ese d√≠a. Resultado : Una lista de 5 objetos (uno por d√≠a) que contienen las predicciones de temperatura m√°xima, m√≠nima y media de OWM. Paso 2: Implementar Nuestro Modelo de IA (Predicci√≥n a 1 D√≠a) Esta es la parte central de nuestra \"IA casera\". Para cada ciudad: Carga de Datos : Utilizamos pandas para leer el fichero CSV hist√≥rico de la ciudad (ej. datos/Madrid.csv ). Ingenier√≠a de Caracter√≠sticas (Feature Engineering) : Como ten√≠amos m√∫ltiples lecturas por d√≠a, el paso m√°s crucial fue transformar estos datos: Remuestreo : Usamos df.resample('D') de pandas para agrupar los datos por d√≠a, calculando los agregados diarios reales (ej., temp_max , temp_min , temp_media , hum_media ). Creaci√≥n de Caracter√≠sticas (X) : Creamos nuevas columnas \"desplazadas\" ( .shift(1) ) para que cada fila (representando un d√≠a) contuviera los datos del d√≠a anterior (ej., temp_max_lag1 , hum_media_lag1 ). Tambi√©n a√±adimos dia_del_anio para capturar la estacionalidad. Creaci√≥n de Objetivos (y) : Definimos qu√© quer√≠amos predecir (ej., la temp_max real del d√≠a actual). Entrenamiento de 3 Modelos : En lugar de uno, entrenamos tres modelos de Regresi√≥n Lineal ( scikit-learn ) independientes: model_max : Entrenado con y = df_clean['temp_max'] . model_min : Entrenado con y = df_clean['temp_min'] . model_media : Entrenado con y = df_clean['temp_media'] . Predicci√≥n : Tomamos la √∫ltima fila de datos agregados (representando los datos de \"hoy\"). Alimentamos estos datos a los 3 modelos para predecir los valores de \"ma√±ana\". Incluimos una salvaguarda ( MIN_RECORDS_FOR_IA = 10 ) para que el modelo solo intente predecir si tiene suficientes datos hist√≥ricos (ej., 10 d√≠as limpios). Paso 3: Consolidar y Guardar El script combina los resultados de los Pasos 1 y 2 en una estructura JSON y la guarda en predicciones.json : [code block] 2. üé® L√≥gica Frontend: index.html Visualiza el Futuro El frontend es responsable de cargar este fichero predicciones.json y presentarlo de forma visualmente atractiva e informativa. Paso 1: Carga de Datos loadPredictions() : Creamos una nueva funci√≥n async que se ejecuta una vez durante la inicializaci√≥n (antes de updateDashboard ). allPredictionsCache : Esta funci√≥n carga predicciones.json y lo guarda en esta nueva variable global para que todas las funciones de visualizaci√≥n tengan acceso a √©l. Paso 2: Visualizaci√≥n en las \"Super-Cards\" (KPIs) Quer√≠amos una comparaci√≥n directa y clara. Pron",
    "url": "https://datalaria.com/es/posts/app_openweather_part3_ai_prediction/",
    "slug": "app_openweather_part3_ai_prediction",
    "lang": "es",
    "categories": [
      "Proyectos",
      "IA",
      "Herramientas"
    ],
    "tags": [
      "machine-learning",
      "regresion",
      "openweathermpa",
      "python",
      "pandas",
      "scikit-learn",
      "javascript",
      "frontend",
      "prediccion-datos",
      "pronostico-tiempo",
      "serverless"
    ],
    "date": "2025-11-15",
    "timestamp": 0,
    "domain": "General",
    "image": "cover.png"
  },
  {
    "objectID": "es_app_openweather_part4_extras_ux",
    "title": "Proyecto Weather (Extras): M√°s All√° de la IA - Construyendo un Dashboard Robusto y Centrado en el Usuario",
    "description": "Un an√°lisis profundo de las caracter√≠sticas 'extra' y las decisiones de dise√±o detr√°s de Project Weather, mostrando c√≥mo la recolecci√≥n avanzada de datos, la internacionalizaci√≥n, el filtrado personalizado y las mejoras de UI/UX transformaron un dashboard b√°sico en una aplicaci√≥n meteorol√≥gica completa, robusta y f√°cil de usar, complementando sus capacidades centrales de predicci√≥n por IA.",
    "content": "En las publicaciones anteriores, hemos explorado el n√∫cleo del Proyecto Weather , centrado en su plataforma b√°sica de backend-frontend y sus capacidades de predicci√≥n por IA. Una vez conseguidos estos puntos y con la aplicaci√≥n operativa, surgieron ciertos aspectos de mejora de cara a una experiencia de frontend m√°s fluida e intuitiva. Esta publicaci√≥n desvela las \"extras\", las mejoras significativas y las decisiones de dise√±o que transformaron el Proyecto Weather de una piloto b√°sico a un dashboard meteorol√≥gico integral y listo para producci√≥n. Estas mejoras, aunque no est√°n directamente relacionadas con la predicci√≥n por IA, fueron cruciales para construir una experiencia de usuario fiable, escalable y agradable. 1. Expansi√≥n de M√©tricas y Recolecci√≥n de Datos: De B√°sico a Completo Nuestro dashboard meteorol√≥gico inicial, aunque funcional, solo proporcionaba m√©tricas b√°sicas como temperatura, viento y humedad. Para evolucionar hacia una estaci√≥n meteorol√≥gica verdaderamente √∫til, necesit√°bamos m√°s datos. Esto requiri√≥ modificaciones significativas tanto en nuestro script de recolecci√≥n de datos del backend como en la visualizaci√≥n del frontend. Nuevas M√©tricas y Visualizaciones Din√°micas Renovamos nuestro script de Python read_weather.py para obtener y almacenar cuatro nuevas variables meteorol√≥gicas cr√≠ticas: Nubosidad (%): Porcentaje de cielo cubierto. Visibilidad (km): Distancia de visi√≥n (crucial para condiciones de niebla o bruma). Lluvia (mm): Lluvia ca√≠da en la √∫ltima hora. Nieve (mm): Nieve ca√≠da en la √∫ltima hora. En el frontend, visualizar estos diversos tipos de datos de manera efectiva era clave. Implementamos l√≥gica condicional dentro de Chart.js para adaptar el tipo de gr√°fico seg√∫n el dato: Gr√°ficos de l√≠neas para variables continuas como temperatura y velocidad del viento. Gr√°ficos de √°rea (rellenos) para humedad y nubosidad, proporcionando una sensaci√≥n de acumulaci√≥n o cobertura. Gr√°ficos de barras para precipitaciones (lluvia/nieve), ya que las barras representan visualmente las cantidades acumuladas de manera m√°s intuitiva. Migraci√≥n Autom√°tica de Datos Un desaf√≠o com√∫n al agregar nuevos campos de datos a un sistema existente es la gesti√≥n de datos hist√≥ricos. Dise√±amos un robusto sistema de backend para detectar y migrar autom√°ticamente ficheros CSV antiguos (con 11 columnas) al nuevo formato de 15 columnas. Fundamentalmente, este sistema manej√≥ con elegancia los valores faltantes para las nuevas m√©tricas, complet√°ndolos con valores por defecto sensatos (por ejemplo, 0 para lluvia/nieve), evitando as√≠ cualquier p√©rdida de contexto hist√≥rico y asegurando la integridad de los datos en todo el conjunto de datos. Este proceso automatizado fue vital para una transici√≥n fluida sin requerir intervenci√≥n manual. 2. Sistema Completo de Internacionalizaci√≥n (i18n): Una Aplicaci√≥n Verdaderamente Global Para hacer que el Proyecto Weather fuera accesible a una audiencia m√°s amplia, un sistema completo de internacionalizaci√≥n (i18n) fue una prioridad principal. Transformamos la aplicaci√≥n web en una plataforma totalmente biling√ºe (espa√±ol/ingl√©s) con cambio din√°mico de idioma, sin necesidad de recargar la p√°gina. Arquitectura de Traducci√≥n Construimos un diccionario central const translations en JavaScript, que conten√≠a todas las cadenas de texto utilizadas en la aplicaci√≥n. Este enfoque asegur√≥ la consistencia y simplific√≥ el mantenimiento. Textos Est√°ticos: Los elementos HTML que requer√≠an traducci√≥n se etiquetaron con un atributo data-i18n-key . Una funci√≥n de JavaScript luego iteraba autom√°ticamente a trav√©s de estos elementos, reemplazando su contenido con la traducci√≥n correspondiente del diccionario activo. Textos Din√°micos: Fundamentalmente, toda la l√≥gica de JavaScript responsable de generar cadenas din√°micas (por ejemplo, \"Sensaci√≥n t√©rmica\", \"Velocidad del viento\", etiquetas de KPI y tooltips de gr√°ficos) fue refactorizada para leer directamente del diccionario de idioma actualmente seleccionado. Esto asegur√≥ que cada fragmento de texto, independientemente de si era HTML est√°tico o generado din√°micamente, se localizara correctamente. Formato de Fecha y N√∫meros M√°s all√° del texto, el formato cultural para fechas y n√∫meros es esencial. Aprovechamos Intl.DateTimeFormat y lo integramos con Chart.js para asegurar que las fechas en los ejes de los gr√°ficos y en las tarjetas de datos se mostraran en el formato culturalmente correcto (por ejemplo, \"10 nov\" frente a \"Nov 10\"). Los formatos de n√∫meros (por ejemplo, separadores decimales) tambi√©n se adaptaron en consecuencia. Persistencia Para mejorar la experiencia del usuario, la preferencia de idioma se almacena en localStorage , de modo que la aplicaci√≥n recuerda el idioma elegido por el usuario entre visitas. 3. Interactividad Avanzada: Filtrado de Rango de Tiempo Personalizado Capacitar a los usuarios con capacidades precisas de exploraci√≥n de datos fue un objetivo clave. Mejoramos significativamente la interactividad del dashboard introduci",
    "url": "https://datalaria.com/es/posts/app_openweather_part4_extras_ux/",
    "slug": "app_openweather_part4_extras_ux",
    "lang": "es",
    "categories": [
      "Project Showcase",
      "Desarrollo Web",
      "UI/UX"
    ],
    "tags": [
      "project-weather",
      "dashboard-climatico",
      "python",
      "javascript",
      "frontend",
      "backend",
      "i18n",
      "visualizacion-datos",
      "chart.js",
      "ui-ux"
    ],
    "date": "2025-12-06",
    "timestamp": 0,
    "domain": "Projects",
    "image": "cover.png"
  },
  {
    "objectID": "es_BPMN_SOP",
    "title": "De la Narrativa al Diagrama: Dise√±ando Procesos S&OP con IA y BPMN",
    "description": "C√≥mo utilizar la Inteligencia Artificial como un Analista de Negocio para transformar narrativas complejas en diagramas de flujo estructurados, comparando herramientas como Miro, Mermaid y BPMN.io.",
    "content": "En el mundo de la ingenier√≠a y la gesti√≥n industrial, a menudo nos enfrentamos a un problema de \"traducci√≥n\". Los expertos de negocio describen procesos complejos mediante narrativas densas o documentos de texto interminables, mientras que los ingenieros de sistemas y desarrolladores necesitan l√≥gica estructurada y diagramas precisos. Esta brecha entre la narrativa de negocio y la especificaci√≥n t√©cnica es donde ocurren la mayor√≠a de los errores: requisitos malinterpretados, cuellos de botella invisibles en el texto y dependencias no identificadas. Hoy vamos a ver c√≥mo la Inteligencia Artificial Generativa puede actuar como nuestro Business Analyst virtual, transformando un p√°rrafo complejo de un proceso S&OP (Sales and Operations Planning) en un diagrama visual estandarizado. Adem√°s, analizaremos la estrategia tecnol√≥gica para visualizarlo: ¬øCu√°ndo usar Miro , cu√°ndo Mermaid y cu√°ndo BPMN.io ? El Caso de Uso: Un S&OP Integral Imagina que recibes la siguiente descripci√≥n para digitalizar un proceso de planificaci√≥n. Es un bloque de texto denso, rico en detalles pero dif√≠cil de visualizar de un vistazo: \"El proceso inicia con la detecci√≥n de oportunidades comerciales tempranas . Ingenier√≠a debe identificar la soluci√≥n, analizando la madurez del producto y su fabricabilidad (obsolescencias, bloqueos por restricciones de uso, ROHS, REACH, lead time elevados,...). Si hay problemas, se activa la gesti√≥n de cambios (ECR/ECO - Engineering Change Request/Engineering Change Order) o la validaci√≥n de alternativos; si se requieren nuevos desarrollos, entra ingenier√≠a de sistemas. Una vez validada la soluci√≥n t√©cnica, el flujo se divide: por un lado, Operaciones realiza el an√°lisis de carga-capacidad fabril sobre lo ya planificado; en paralelo, Compras revisa los lead times de acopio . Finalmente, todo converge en Finanzas para analizar recursos, costes y viabilidad econ√≥mica antes de aprobar el proyecto.\" El cerebro humano lucha para procesar todas esas condicionales y paralelismos simult√°neamente. Aqu√≠ es donde entra la IA. La Estrategia de Herramientas: El Tri√°ngulo de la Visualizaci√≥n No todos los diagramas tienen el mismo prop√≥sito. Dependiendo de la fase del proyecto, la IA puede ayudarnos a generar outputs para tres herramientas distintas. En Datalaria proponemos el siguiente flujo de trabajo: | Herramienta | Fase del Proyecto | Rol de la IA | | :--- | :--- | :--- | | Miro / Mural | Descubrimiento | Generar listas de tareas y decisiones para \"p√≥st-its\" en sesiones de brainstorming colaborativo. | | Mermaid.js | Documentaci√≥n | Generar c√≥digo (\"Diagrams as Code\") para documentaci√≥n viva, wikis y blogs t√©cnicos. R√°pido y versionable. | | BPMN.io / Camunda | Ejecuci√≥n | Estructurar archivos XML BPMN 2.0 estrictos para motores de orquestaci√≥n de procesos reales. | Para este art√≠culo, nos centraremos en la opci√≥n intermedia: Mermaid.js . Es la opci√≥n perfecta para la documentaci√≥n t√©cnica √°gil porque vive junto a tu c√≥digo y se renderiza nativamente en la web. De Texto a C√≥digo: El Prompt de Ingenier√≠a Para lograr un resultado de calidad, no basta con pedirle a la IA \"hazme un dibujo\". Debemos pedirle que razone la estructura l√≥gica. El flujo del prompt debe ser: 1. Rol: Actuar como experto en BPMN. 2. An√°lisis: Identificar Actores (Swimlanes), Actividades y Compuertas (Gateways). 3. Output: Generar c√≥digo Mermaid con sintaxis de grafo. El Resultado Visual A continuaci√≥n, presento el diagrama generado autom√°ticamente tras procesar la narrativa del S&OP. He instruido al modelo para que utilice una est√©tica tipo BPMN 2.0 (orientaci√≥n horizontal, carriles definidos y nodos redondeados) para facilitar la lectura profesional. flowchart LR %% --- ESTILOS MODERNOS DATALARIA --- %% Tareas: Fondo blanco limpio con borde azul t√©cnico classDef task fill:#ffffff,stroke:#2962ff,stroke-width:1px,rx:5,ry:5,color:#333; %% Compuertas (Decisiones): Fondo naranja suave para destacar classDef gateway fill:#fff3e0,stroke:#ff6d00,stroke-width:1px,rotation:45,color:#333; %% Evento Inicio: Verde sutil classDef event fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px,color:#333; %% Evento Fin: Rojo sutil classDef endEvent fill:#ffebee,stroke:#c62828,stroke-width:3px,color:#333; %% --- POOL / SWIMLANES --- subgraph S_OP_Process [Proceso S&OP Integral] direction LR %% CARRIL COMERCIAL subgraph COM [Ventas & Negocio] Start((Inicio)):::event --> Opp(Detecci√≥n Oportunidad):::task Opp --> Reqs(Definir Requisitos):::task end %% CARRIL INGENIER√çA subgraph ENG [Ingenier√≠a & Producto] Reqs --> Ident(Identificar Soluci√≥n):::task Ident --> CheckMat{¬øMadurez &<br>Fabrica.?}:::gateway %% Compuerta Exclusiva (X) CheckMat -- No: Obs./Restricciones --> ECR/ECO(Gesti√≥n Cambios ECR/ECO):::task ECR/ECO --> ValAlt(Validar Alternativos):::task CheckMat -- No: Nuevo Des. --> SysEng(Ing. Sistemas: Desarrollos):::task SysEng --> Proto(Prototipado):::task ValAlt --> JoinEng(( )):::gateway Proto --> JoinEng CheckMat -- S√≠ --> JoinEng JoinEng --> SolValid(Soluci√≥n Validada):",
    "url": "https://datalaria.com/es/posts/BPMN_SOP/",
    "slug": "BPMN_SOP",
    "lang": "es",
    "categories": [
      "Inteligencia Artificial",
      "Industria 4.0",
      "Productividad"
    ],
    "tags": [
      "S&OP",
      "BPMN",
      "Mermaid",
      "Process Design",
      "GenAI",
      "Business Analysis"
    ],
    "date": "2025-12-19",
    "timestamp": 0,
    "domain": "S&OP",
    "image": "/images/posts/ai-bpmn-sop-process.jpg"
  },
  {
    "objectID": "es_carto",
    "title": "Carto: De una Factura a la ONU a Conquistar la Nube Geoespacial",
    "description": "La historia de c√≥mo Carto, una startup espa√±ola, pas√≥ de ser un proyecto de consultor√≠a a un l√≠der mundial en Inteligencia de Localizaci√≥n, apostando por una estrategia nativa en la nube que ha redefinido el an√°lisis de datos.",
    "content": "En el competitivo mundo de la tecnolog√≠a, las historias de √©xito a menudo siguen un guion predecible. Sin embargo, de vez en cuando, surge una empresa cuya trayectoria rompe el molde. Una que no nace de una idea preconcebida en un garaje de Silicon Valley, sino de una necesidad tan pragm√°tica como emitir una factura. Esta es la historia de Carto , la cr√≥nica de c√≥mo un peque√±o estudio de consultor√≠a de datos de Madrid se transform√≥ en un l√≠der mundial en el an√°lisis geoespacial. Es un caso de √©xito fundamentado en una revelaci√≥n estrat√©gica clave: en lugar de luchar contra los gigantes, decidieron convertirse en la capa de inteligencia indispensable que todos ellos necesitaban. El Origen: Una Factura a la ONU y una Brecha en el Mercado La historia de Carto comienza en 2007, no con un producto, sino con Vizzuality , una consultora de visualizaci√≥n de datos fundada por Javier de la Torre y Sergio √Ålvarez Leiva. Su primer gran encargo fue de una agencia de la ONU, y la creaci√≥n de la empresa fue, en sus inicios, un mero tr√°mite para poder facturar ese proyecto. Mientras trabajaban en proyectos de nicho para cient√≠ficos, los fundadores se enfrentaban repetidamente a los mismos desaf√≠os: manejar y visualizar enormes vol√∫menes de datos espaciales. Esta experiencia directa les hizo identificar una \"necesidad cr√≠tica\" en el mercado: las potentes t√©cnicas de an√°lisis espacial eran inaccesibles para las industrias generalistas como el retail, las finanzas o las telecomunicaciones. Ese fue el momento \"¬°eureka!\". Vieron la oportunidad de democratizar el an√°lisis de datos geolocalizados . As√≠, dentro de Vizzuality, naci√≥ un proyecto de software interno llamado CartoDB , dise√±ado para resolver sus propios problemas. Tras su lanzamiento oficial en 2012, tomaron la decisi√≥n crucial en 2014: Carto se independiz√≥ de Vizzuality, pivotando de un modelo de servicios a un modelo de producto SaaS escalable, listo para atraer capital riesgo. El Pivote Estrat√©gico: No Somos un SIG, Somos la Capa Inteligente de la Nube El movimiento m√°s brillante de Carto fue no intentar ser un \"mejor SIG\" (Sistema de Informaci√≥n Geogr√°fica). En su lugar, crearon una nueva categor√≠a: la Inteligencia de Localizaci√≥n (Location Intelligence - LI) . SIG Tradicional : Para especialistas. Responde a \"¬ød√≥nde est√°n las cosas?\". Inteligencia de Localizaci√≥n : Para analistas de datos y de negocio. Responde a \" ¬øpor qu√© suceden las cosas en un lugar?\" y \" ¬øqu√© pasar√≠a si...? \". Su verdadera ventaja competitiva, sin embargo, reside en su arquitectura nativa en la nube (cloud-native) . En lugar de crear otro silo de datos, obligando a las empresas a mover su informaci√≥n, Carto lleva el an√°lisis directamente al lugar donde ya residen los datos: los grandes almacenes de datos en la nube como Google BigQuery, Snowflake, AWS Redshift y Databricks . Esta estrategia convirti√≥ a los gigantes de la nube, potenciales competidores, en sus mayores aliados y canales de distribuci√≥n. Carto se posicion√≥ como la \"Suiza\" de los datos geoespaciales, una capa de an√°lisis agn√≥stica que aumenta el valor de las inversiones que las empresas ya han hecho en la nube. Este cambio de enfoque se consolid√≥ en 2016 con el rebranding de CartoDB a CARTO , eliminando el sufijo \"DB\" para se√±alar su evoluci√≥n de una \"base de datos para mapas\" a una plataforma de inteligencia completa, bajo el lema: \"Predecir a trav√©s de la localizaci√≥n\" . El Motor del Crecimiento: Rondas de Financiaci√≥n, Adquisiciones y Talento La trayectoria de Carto valida su estrategia, mostrando una progresi√≥n clara tanto en la confianza de los inversores como en su capacidad para atraer talento. Rondas de Financiaci√≥n Clave La financiaci√≥n de la compa√±√≠a ha pasado de capital local a fondos de primer nivel mundial, acumulando un total de 92 millones de d√≥lares . | Ronda | Fecha | Cantidad (USD) | Inversor(es) Principal(es) | | :-------- | :------- | :------------- | :------------------------- | | Serie A | Sep 2014 | $8M | Earlybird Venture Capital | | Serie B | Sep 2015 | $23M | Accel Partners | | Serie C | Dic 2021 | $61M | Insight Partners | Crecimiento de la Plantilla El crecimiento del equipo refleja la expansi√≥n de la empresa, pasando de un n√∫cleo fundador a una organizaci√≥n global. | Fecha / Per√≠odo | Hito de Empleados | | :-------------- | :---------------- | | 2012 | 2 Fundadores | | Nov 2021 | > 150 Empleados | | Principios 2024 | > 160 Expertos | | Est. 2024/2025 | ~ 313 Empleados | Madurez Estrat√©gica En 2019, la compa√±√≠a madur√≥ su liderazgo nombrando a Luis Sanz como nuevo CEO, un operador experimentado con un historial de √©xito en escalar negocios. Poco despu√©s, Carto adquiri√≥ Geographica , una consultora sevillana con gran experiencia en soluciones espaciales y con clientes como Mastercard, BBVA y Telef√≥nica. Este movimiento fue una jugada maestra: dot√≥ a la empresa de un equipo de servicios profesionales consolidado, esencial para abordar las complejas necesidades de las grandes cuentas empresariales y ejecut",
    "url": "https://datalaria.com/es/posts/carto/",
    "slug": "carto",
    "lang": "es",
    "categories": [
      "casos_exito",
      "IA"
    ],
    "tags": [
      "carto",
      "geoespacial",
      "ia",
      "cloud",
      "startup",
      "big data",
      "inteligencia de localizaci√≥n"
    ],
    "date": "2025-10-25",
    "timestamp": 0,
    "domain": "General",
    "image": "cover.png"
  },
  {
    "objectID": "es_datalaria-blog",
    "title": "Construyendo Datalaria: Tecnolog√≠as y herramientas",
    "description": "Datalaria: construcci√≥n de sus cimientos. Stack moderno vs. soluciones tradicionales",
    "content": "El objetivo fundamental de Datalaria es la experimentaci√≥n y exploraci√≥n de diferentes temas, herramientas, tecnolog√≠as... Tras unos primeros art√≠culos m√°s bien te√≥ricos y did√°cticos para sentar las bases de futuros an√°lisis y experimentos, es un buen momento para plantear las entra√±as de este primer proyecto el cual es este blog. Vamos a examinar la tecnolog√≠a detr√°s y los motivos de su elecci√≥n frente a qui√©nes esperar√≠an o√≠r la palabra \"Wordpress\". Este post es un viaje personal por el proceso de decisi√≥n t√©cnica que me llev√≥ a construir Datalaria con un stack moderno: Hugo , GitHub y Netlify . El objetivo no es demonizar otras plataformas, sino explicar por qu√© esta combinaci√≥n era la perfecta para los objetivos de este proyecto basados en experimentar y explorar. Definiendo los Objetivos Partiendo de los principales aspectos de inter√©s que recog√≠a en mi post de bienvenida , los cuales son la experimentaci√≥n y el aprendizaje a largo plazo , defin√≠ los pilares de este proyecto: Aprendizaje y Control (Prioridad #1) : Quer√≠a un sistema que me forzara a entender c√≥mo funcionan las cosas por debajo, a usar control de versiones (Git) y a familiarizarme con flujos de despliegue continuo (CI/CD). Rendimiento : La web deb√≠a ser muy √°gil, con tiempos de carga m√≠nimos para la experiencia de usuario y el SEO. Seguridad : Quer√≠a minimizar los posibles puntos de ataque y no tener que preocuparme por actualizaciones constantes de seguridad de plugins o temas. Bajo Coste inicial y Escalabilidad : El proyecto no tiene un fin comercial, por lo que los costes de mantenimiento deb√≠an ser cercanos a cero, pero con la capacidad de escalar si el tr√°fico aumentara o integrara nuevas funcionalidades. Con estos criterios en mente, evalu√© las dos principales filosof√≠as de construcci√≥n web. El Camino Tradicional: ¬øPor Qu√© No WordPress? Wordpress es el rey indiscutible de la gesti√≥n de contenidos, moviendo m√°s del 40% de toda la web. Es una herramienta potent√≠sima, flexible y con un ecosistema gigantesco. Su arquitectura es din√°mica . Esto significa que cada vez que visitas una p√°gina, el servidor ejecuta c√≥digo PHP, realiza consultas a una base de datos MySQL para obtener el contenido, ensambla la p√°gina en una plantilla HTML y te la env√≠a. Desventajas para mis Objetivos Pese a sus fortalezas, para los pilares de Datalaria, WordPress presentaba varios inconvenientes: Curva de aprendizaje distinta : Aprender WordPress es, en gran medida, aprender a usar su interfaz y su ecosistema de plugins. Es decir, es un aprendizaje de la herramienta en s√≠. No te expone de forma nativa a procesos de dise√±o web como tal, ni a herramientas como Git, la l√≠nea de comandos o procesos de build modernos. Rendimiento : La naturaleza din√°mica, con sus llamadas a la base de datos, introduce una latencia inevitable. Aunque se puede mitigar con complejos sistemas de cach√©, la base es inherentemente m√°s lenta que servir un fichero est√°tico. Seguridad : Al ser tan popular y depender de una base de datos y c√≥digo que se ejecuta en el servidor, es un objetivo constante de ataques. Requiere un mantenimiento proactivo: actualizar el core, los temas y los plugins debe ser una tarea continua. Coste : Necesitas un hosting compatible con PHP y MySQL, que suele ser m√°s caro que un simple alojamiento de ficheros est√°ticos. WordPress es una soluci√≥n fant√°stica para usuarios no t√©cnicos, para sitios con funcionalidades complejas (e-commerce, foros) o para aquellas personas que quieran desplegar una soluci√≥n r√°pida centrada en el contenido y no en todo lo que hay detr√°s del desarrollo web. Con todo esto, Wordpress no se alineaba con mi objetivo principal de aprendizaje t√©cnico y entender lo que hay detr√°s. El Camino Moderno: Jamstack al Rescate Y entonces apareci√≥ el Jamstack (JavaScript, APIs y Markup) como una arquitectura que desacopla el frontend del backend. La idea es revolucionaria en su simplicidad: en lugar de construir la p√°gina cada vez que un usuario la pide, la construyes una sola vez durante el despliegue. El resultado es un conjunto de ficheros HTML, CSS y JavaScript pre-generados, es decir, un sitio est√°tico . Estos ficheros se distribuyen despu√©s a trav√©s de una Red de Distribuci√≥n de Contenidos (CDN) global. Ventajas para mis Objetivos Esta filosof√≠a encajaba como un guante con mis metas: Rendimiento : Servir un fichero HTML desde un CDN es la forma m√°s r√°pida de entregar una p√°gina web. Los resultados en herramientas como PageSpeed Insights son espectaculares. Seguridad : Al no haber base de datos ni c√≥digo ejecut√°ndose en el servidor en tiempo de ejecuci√≥n, los posibles puntos de riesgo de ataque se reducen dr√°sticamente. Escalabilidad y coste casi nulos : Los CDNs est√°n dise√±ados para manejar picos de tr√°fico masivos sin esfuerzo. Servicios como Netlify o Vercel ofrecen planes gratuitos incre√≠blemente generosos para proyectos personales. Experiencia de desarrollo y aprendizaje : ¬°El punto clave! Este enfoque te obliga a usar las herramientas que quer",
    "url": "https://datalaria.com/es/posts/datalaria-blog/",
    "slug": "datalaria-blog",
    "lang": "es",
    "categories": [
      "Proyectos"
    ],
    "tags": [
      "hugo",
      "jamstack",
      "netlify",
      "github",
      "wordpress",
      "desarrollo web",
      "ci/cd"
    ],
    "date": "2025-08-17",
    "timestamp": 0,
    "domain": "Projects",
    "image": "hugo_vs_wordpress.png"
  },
  {
    "objectID": "es_Estadistica-descriptiva",
    "title": "Estad√≠stica Descriptiva",
    "description": "Conoce los conceptos y nociones b√°sicas en matem√°ticas y estad√≠stica para poder trabajar con tus datos",
    "content": "Como punto de partida en el an√°lisis de datos, es necesario conocer unas nociones matem√°ticas y estad√≠sticas para poder trabajar con la informaci√≥n a analizar. Para ello, vamos a comenzar viendo unos conceptos b√°sicos: Poblaci√≥n, Muestras, Individuos y Variables Poblaci√≥n : Total de elementos bajo estudio Muestra : Subconjunto de elementos de la poblaci√≥n (deber√≠a ser representativo de la poblaci√≥n) Individuos : Elementos individuales de la poblaci√≥n Variables : Caracter√≠sticas de los individuos Ejemplo: Dataset Titanic Poblaci√≥n: 2.224 individuos Muestras: ficheros train.csv : Conjunto de 891 individuos representativo de la poblaci√≥n utilizado para el entrenamiento de modelos de Aprendizaje autom√°tico. test.csv : Conjunto de 418 individuos representativo de la poblaci√≥n utilizado para las pruebas de los modelos entrenados de Aprendizaje autom√°tico. Individuos: Cada pasajero y tripulante de la poblaci√≥n (cada fila de los datos de las muestras o poblaci√≥n recogidas). Variables: Las caracter√≠sticas recogidas para cada individuo (cada columna de los datos de las muestras o poblaci√≥n recogidas, ej.: survival (superviviente), sex (sexo) o Age (edad)). Tipos de Variables Las variables se pueden clasificar dependiendo del tipo de datos que contengan y, m√°s concretamente: Num√©ricas : los datos se representan mediante n√∫meros que son m√©tricas o medidas de dicha variable. Seg√∫n los valores que pueda tomar ese n√∫mero se clasificar√≠an en: Continuas : podr√≠an tomar un n√∫mero incontable o indeterminado de valores. Por ejemplo, en el conjunto de datos del ejemplo del Titanic, la variable ‚Äúfare‚Äù podr√≠a considerarse una variable continua al tomar valores diferentes con hasta 4 decimales de precisi√≥n. Discretas : tomar√≠an un n√∫mero contable dentro de una lista de valores (los datos de variables continuas pueden ser truncados a variables discretas por mayor facilidad de an√°lisis). Por ejemplo, la variable Sibsp (# de hermanos) ser√≠an variables discretas. Categ√≥ricas : los datos se representan con textos o n√∫meros cuyo significado no es una m√©trica, sino que se asocian a un dato concreto o categor√≠a. Seg√∫n la relaci√≥n entre los valores que puedan tomar los datos se clasificar√≠an en: Nominales : datos que no tienen ninguna relaci√≥n u orden intr√≠nseco en s√≠ mismas. Un Ejemplo de este tipo ser√≠a la variable ‚Äúembarked‚Äù que representa el puerto de embarque de los pasajeros. Ordinales : datos que siguen un orden natural o clasificaci√≥n entre s√≠. Como por ejemplo podr√≠a ser la variable Pclass que representa la clase en el Titanic seg√∫n el estatus social. Nota de inter√©s: No siempre est√° clara la tipolog√≠a de las variables y √©stas podr√≠an ser consideradas de diferentes tipos dependiendo del criterio de cada persona y de los objetivos que se est√©n buscando. Un ejemplo podr√≠a ser la representaci√≥n de la edad, la cual podr√≠a ser correctamente considerada tanto num√©rica (continua o discreta dependiendo de la precisi√≥n) como ordinal, dependiendo de si se desea trabajar con f√≥rmulas para tratar la edad como un valor continuo o simplemente se trabaja con intervalos de edad espec√≠ficos seg√∫n la relaci√≥n del resto de variables. Histogramas y Gr√°ficos de barras Aunque m√°s adelante har√© una entrada dedicada a la visualizaci√≥n de datos y los principales gr√°ficos a utilizar, es importante citar brevemente a los histogramas y los gr√°ficos de barras, los cuales son dos de las representaciones m√°s √∫tiles en el an√°lisis de datos y sirven para entender c√≥mo se distribuyen los datos respecto a una variable o caracter√≠stica del conjunto de datos (continua en el caso de los histogramas y discreta en el caso de los gr√°ficos de barras). Ejemplo: Diagrama de Barras Ejemplo: Histograma As√≠ pues, gracias a estos gr√°ficos se puede extraer informaci√≥n significativa sobre la distribuci√≥n de los datos representados y seg√∫n su distribuci√≥n se pueden clasificar en sim√©tricos o asim√©tricos como veremos a continuaci√≥n. Medidas de Estad√≠stica Descriptiva Un paso clave a la hora de analizar un conjunto de datos es lo que se denomina ‚Äúestad√≠stica descriptiva‚Äù y que consiste, como el nombre indica, en describir los datos mediante una serie de medidas que permitan caracterizar principalmente su distribuci√≥n y su dispersi√≥n. Medidas de la tendencia central Las medidas de la tendencia central sirven para darnos una idea de la distribuci√≥n de nuestros datos. Media aritm√©tica (Promedio) Esta medida busca obtener el valor medio de un conjunto de datos. Es decir, para ello se suman todos los valores del conjunto de datos y se dividen por el total de datos que lo componen. Media de la poblaci√≥n : $$ \\mu = \\frac{1}{N}\\displaystyle\\sum_{i=1}^Nx_i $$ Media de la muestra : $$ \\bar x = \\frac{1}{n}\\displaystyle\\sum_{i=1}^Nx_i $$ Siendo N el total de datos de la poblaci√≥n y n el total de datos de la muestra. Ejemplo: Si tenemos el siguiente conjunto de 5 datos: [1, 2, 2, 5, 10], la media de dichos valores ser√° por lo tanto la suma de los mismos (20) dividido por el total de datos (5), e",
    "url": "https://datalaria.com/es/posts/Estadistica-descriptiva/",
    "slug": "Estadistica-descriptiva",
    "lang": "es",
    "categories": [
      "fundamentos"
    ],
    "tags": [
      "estad√≠stica",
      "matem√°ticas",
      "distribucion",
      "normal",
      "gaussiana"
    ],
    "date": "2025-08-09",
    "timestamp": 0,
    "domain": "General",
    "image": "estadistica_descriptiva.png"
  },
  {
    "objectID": "es_florence-nightingale",
    "title": "Florence Nightingale - La madre de la Enfermer√≠a",
    "description": "Florence Nightingale, enfermera, escritora y estad√≠stica brit√°nica, madre de la enfermer√≠a y una de las primeras cient√≠ficas de datos.",
    "content": "Continuamos con los casos de √©xito en el an√°lisis de datos para hablar de Florence Nightingale (12/05/1820 ‚Äì 13/08/1910), una figura revolucionaria. Enfermera, escritora y estad√≠stica brit√°nica, es considerada la madre de la enfermer√≠a moderna y, como veremos, una de las primeras y m√°s influyentes cient√≠ficas de datos . Su caso es especialmente relevante dado que, adem√°s de luchar por la emancipaci√≥n de la mujer mediante la apertura de nuevos rumbos profesionales cualificados, destac√≥ en el √°mbito de la enfermer√≠a por ser considerada la precursora de la enfermer√≠a profesional moderna gracias a la creaci√≥n del primer modelo conceptual . Para la confecci√≥n de este modelo, Florence aplic√≥ sus conocimientos estad√≠sticos y matem√°ticos, as√≠ como todos los datos e informaci√≥n relativa a cuidados de enfermer√≠a que fue recopilando durante un per√≠odo de aprendizaje en el que visit√≥ diferentes hospitales, prisiones, asilos y orfelinatos de Francia, Alemania, Italia, Suiza, Irlanda e Inglaterra. El √©xito de este nuevo modelo profesionalizado de enfermer√≠a, se vio refrendado bajo el √°mbito de estudio y aplicaci√≥n del mismo en el alto √≠ndice de mortandad de los heridos de la guerra de Crimea , la cual hab√≠a estallado en el a√±o 1853 tras las pretensiones rusas de ocupar los territorios danubianos. Las mortalidades dentro del ej√©rcito brit√°nico eran elevadas, fruto no de la guerra sino del ca√≥tico sistema sanitario por el que se reg√≠an. Los hospitales de campa√±a apenas dispon√≠an de medios y la suciedad era indescriptible. Ante esta situaci√≥n, el Gobierno Brit√°nico decidi√≥ enviar a Florence Nightingale para reformar completamente la administraci√≥n sanitaria del ej√©rcito. A su llegada Florence comenz√≥ una ardua labor de recolecci√≥n y recogida de datos con el objetivo de analizar el estado sanitario existente y presentar dichas conclusiones al Parlamento. Para ello, Florence cre√≥ un gr√°fico, conocido como diagrama polar o de la rosa , que facilit√≥ la labor de visualizaci√≥n e interpretaci√≥n de todos los datos recopilados y en el que destacaba mayor grado de mortalidad en los hospitales ingleses que en la propia guerra. Ante estos datos, convenci√≥ al Parlamento de la necesidad de mejorar las condiciones sanitarias de los hospitales y como resultando Florence revolucion√≥ aspectos como la atenci√≥n a los enfermos, la higiene y la mejora de la alimentaci√≥n Al finalizar la guerra regres√≥ a Londres y se centr√≥ en el √°mbito civil, donde escribi√≥ un libro de texto con sus conocimientos y fund√≥ una escuela de enfermer√≠a para la formaci√≥n de estas profesionales. Dentro de dichos conocimientos tambi√©n cabe destacar la Formulaci√≥n de un modelo de Estad√≠stica Hospitalaria para que los hospitales recolecten y generen datos y estad√≠sticas consistentes. Por lo tanto, y en resumen, Florence Nightingale representa un claro caso de √©xito y de disrupci√≥n dentro del √°mbito sanitario gracias a la aplicaci√≥n de la recogida y an√°lisis de los datos con el objetivo de obtener una mayor claridad en la interpretaci√≥n de los mismos y sobre todo de mejora continua. Fuentes de inter√©s CODEM - Colegio Oficial de Enfermer√≠a de Madrid Master Telef√≥nica en Big Data & Business Analytics Did Nightingale‚Äôs ‚ÄòRose Diagram‚Äô save millions of lives?",
    "url": "https://datalaria.com/es/posts/florence-nightingale/",
    "slug": "florence-nightingale",
    "lang": "es",
    "categories": [
      "casos_exito"
    ],
    "tags": [
      "enfermer√≠a",
      "florence nightingale",
      "sanidad",
      "crimea"
    ],
    "date": "2025-09-06",
    "timestamp": 0,
    "domain": "General",
    "image": "FlorenceNightingale.png"
  },
  {
    "objectID": "es_founderz-programando-chatGPT",
    "title": "Copilotando con IA: Aprendiendo con Founderz y ChatGPT",
    "description": "Cr√≥nica de c√≥mo el curso de IA y Prompt Engineering de Founderz me abri√≥ los ojos a la posibilidad de crear aplicaciones web funcionales usando solo lenguaje natural con ChatGPT.",
    "content": "En nuestro d√≠a a d√≠a, nos enfrentamos constantemente a barreras en la implementaci√≥n de buenas ideas y su ejecuci√≥n por falta de conocimiento t√©cnico y el tiempo necesario para saber por donde empezar e investigar como resolverlo. Los modelos de IA existentes actualmente han supuesto una revoluci√≥n en este sentido gracias a la aceleraci√≥n del tiempo y eliminaci√≥n de los obst√°culos de aprendizaje y resoluci√≥n de problemas en este sentido. Gracias a estos modelos podemos hacer m√°s cosas en menos tiempo, y adentrarnos en campos de gran complejidad como el mundo de la programaci√≥n permiti√©ndonos hacer realidad aplicaciones sencillas que implementen buenas ideas de nuestro d√≠a a d√≠a. En este sentido, recientemente complet√© el curso Introducci√≥n a la IA y Prompt Engineering de Founderz impartido por Pau Garc√≠a-Mill√° (Fundador y Co-CEO de Founderz), Anna Cejudo (Fundadora y Co-CEO de Founderz) y Magda Teruel (Manager de cuentas de cliente en Microsoft) el cual ya no est√° disponible en su plataforma, pero pod√©is encontrar otros cursos similares en su p√°gina web Founderz . El curso fue muy f√°cil de seguir, muy did√°ctico, muy bien hilado en sus contenidos y que no te deja indiferente, sino que Pau nos ofrece varias pinceladas de lo que se puede llegar a hacer con la IA con unas breves y buenas instrucciones. Este curso consist√≠a en videos breves de entre 5 y 15 minutos donde se iba explicando y desarrollando la idea de los modelos de IA como \"Copilotos\" en nuestro d√≠a a d√≠a. Pau, Anna y Magda nos iban presentando ejemplos de como \"hablarles\" a estos \"Copilotos\" y como usarlos para nuestro d√≠a a d√≠a, ense√±√°ndonos algunos casos de uso m√°s avanzados y llegando a varios casos pr√°cticos sobre como materializar nuestras ideas de proyectos en aplicaciones web sin saber programar gracias al buen hacer de nuestros \"Copilotos\". Para finalizar, te propon√≠an probar t√∫ mismo a crear un proyecto b√°sico con estos modelos basado en lo aprendido durante el curso y proporcionan un mecanismo de feedback propio y de otros alumnos que me ha parecido muy interesante. Al margen de recomendar realizar este tipo de cursos a qui√©n quiera introducirse en el uso de los modelos de IA y quiera saber m√°s sobre como contar con estos \"Copilotos\", voy a aprovechar esta entrada para ilustrar una de las pinceladas dadas en este curso y centrada en como aprender a crear una aplicaci√≥n funcional mediante una simple conversaci√≥n con ChatGPT y unas instrucciones adecuadas para tal efecto. El \"¬°Aj√°!\" Moment del Curso de Founderz El curso de Founderz es tremendamente did√°ctico, y su gran aprendizaje es presentar a la IA (en su caso, Copilot y ChatGPT) no solo como un simple asistente de b√∫squeda sino como un compa√±ero de creaci√≥n (un copiloto) . El ejemplo m√°s claro de este copilotaje consiste en como un modelo de lenguaje puede ayudarnos a crear c√≥digo estructurado y funcional a partir de una idea inicial a trav√©s de una serie de instrucciones sencillas. Y, para ello, a trav√©s de varios casos pr√°cticos se ense√±a como crear aplicaciones web autocontenidas en un solo fichero HTML que den rienda suelta a nuestras ideas para mejorar nuestro d√≠a a d√≠a. El Desaf√≠o: Como crear una Herramienta √ötil desde Cero con ChatGPT Para poner a prueba esta habilidad, vamos a crear un juego de aprendizaje de operaciones matem√°ticas b√°sicas enfocada a ni√±os de infantil y primaria de entre 5 y 7 a√±os . El objetivo era simple: una p√°gina web que permitiera ense√±ar y practicar las operaciones matem√°ticas b√°sicas (suma, resta, multiplicaci√≥n y divisi√≥n) por niveles de aprendizaje haci√©ndolo atractivo para los usuarios. La Receta: El Prompt Detallado (y su Refinamiento) es la Clave Aqu√≠ es donde entra en juego el Prompt Engineering . No se trata de pedirle a ChatGPT \"hazme una app\", sino de actuar como un responsable de producto que le da al equipo de desarrollo (la IA) unas especificaciones claras y detalladas. El Prompt Inicial Este fue el primer prompt que utilic√©, inspirado en la metodolog√≠a del curso, donde defin√≠ toda la estructura y funcionalidad que quer√≠a (animo a quienquiera a experimentar y reproducirlo por si mismo sobre ChatGPT): Crea una aplicaci√≥n web simple para practicar las funciones de matem√°ticas b√°sicas: sumas, restas, multiplicaciones y divisiones. El objetivo es crear una aplicaci√≥n sencilla que facilite el aprendizaje de estas operaciones a ni√±os de infantil y primaria (entre 5 y 7 a√±os). Para ello la aplicaci√≥n debe: 1. Empezar con una p√°gina de aprendizaje donde se expliquen las 4 operaciones matem√°ticas. 2. Permitir seleccionar el nivel de aprendizaje entre 4 niveles (infantil 1 y 2, primaria 1 y 2). 3. Permitir seleccionar un grado de dificultad (f√°cil con 1 d√≠gito, dif√≠cil con 2). 4. Disponer de un bot√≥n de \"Empezar juego\" que pase a una pantalla de operaciones. 5. Comprobar si la respuesta es correcta y mostrar un mensaje motivador y un emoticono. 6. Llevar la cuenta de aciertos y fallos. 7. Tener un dise√±o atractivo y responsivo utilizando Tailwind CSS",
    "url": "https://datalaria.com/es/posts/founderz-programando-chatGPT/",
    "slug": "founderz-programando-chatGPT",
    "lang": "es",
    "categories": [
      "Herramientas",
      "Aprendizaje"
    ],
    "tags": [
      "ia",
      "chatgpt",
      "prompt engineering",
      "founderz",
      "desarrollo web",
      "no-code",
      "html",
      "matem√°ticas"
    ],
    "date": "2025-09-11",
    "timestamp": 0,
    "domain": "General",
    "image": "Founderz_IntroduccionIA.png"
  },
  {
    "objectID": "es_freepik",
    "title": "Freepik: La Historia del Gigante Malague√±o que Dom√≥ a la IA para Conquistar el Mundo Creativo",
    "description": "Un an√°lisis profundo de c√≥mo Freepik, la startup malague√±a, pas√≥ de ser un buscador de im√°genes a un l√≠der global, enfrent√°ndose a la disrupci√≥n de la IA con una estrategia audaz que est√° redefiniendo el futuro de la creatividad.",
    "content": "En 2022, la explosi√≥n de la inteligencia artificial generativa provoc√≥ un terremoto en la industria creativa. Modelos como Midjourney o DALL-E 2 plantearon una pregunta existencial que hizo temblar los cimientos de los gigantes del contenido de stock: \"¬øQu√© sentido tiene un banco de im√°genes cuando los usuarios pueden crear exactamente lo que necesitan desde cero?\" . Muchas empresas se paralizaron. Una, sin embargo, vio la amenaza venir y decidi√≥ apostar \"todo o nada\" por una reinvenci√≥n total. Esa empresa no naci√≥ en Silicon Valley, sino en M√°laga. Esta es la historia de Freepik , el viaje de una startup espa√±ola que, gracias a una cultura √∫nica y una agilidad estrat√©gica asombrosa, no solo ha sobrevivido a la disrupci√≥n de la IA, sino que se est√° posicionando para liderar la nueva era de la creatividad. El ADN: Mentalidad de Google, Coraz√≥n de \"Cantera\" El √©xito de Freepik no fue casual. Fundada en 2010, su cultura se forj√≥ a partir de una combinaci√≥n √∫nica de talentos: los hermanos Alejandro (el dise√±ador frustrado que tuvo la idea original) y Pablo Blanes, junto a Joaqu√≠n Cuenca, un emprendedor en serie que ya hab√≠a vendido su empresa Panoramio a Google. Cuenca import√≥ la disciplina y la metodolog√≠a de datos de Google, pero al operar desde M√°laga, lejos de los grandes fondos, no pod√≠an permitirse \"fichajes estrella\". En sus propias palabras, eran como el \"Athletic de Bilbao\" : su fortaleza resid√≠a en la \"cantera\", en desarrollar el talento local y en un enfoque obsesivo en el producto. Su modelo de negocio inicial fue una genialidad disruptiva. Empezaron como un simple buscador que indexaba recursos gratuitos. Su modelo freemium eliminaba la barrera del coste, atrayendo a millones de usuarios. ¬øSu primera fuente de ingresos? Una astuta alianza con su principal competidor, Shutterstock, a quien enviaban tr√°fico a cambio de una comisi√≥n. Al mismo tiempo, el requisito de atribuci√≥n para los recursos gratuitos convirti√≥ a su masiva base de usuarios en un ej√©rcito de marketing global no remunerado , generando millones de enlaces que catapultaron su dominio en Google. El Pivote Existencial: Sacrificar el Crecimiento para Ganar el Futuro Freepik ya era un gigante antes de la IA. Tras ser adquirida por el fondo EQT en 2020 por una cifra estimada de 250 millones de euros, duplic√≥ sus ingresos hasta los 61,5 millones en 2021 y super√≥ a Shutterstock en n√∫mero de suscriptores premium. Pero entonces lleg√≥ el \"terremoto\" de la IA generativa. La direcci√≥n entendi√≥ que una respuesta tibia no era suficiente; la amenaza requer√≠a una reinvenci√≥n total. Tomaron una decisi√≥n valiente: apostar \"todo o nada\" por la IA , reasignando masivamente sus recursos para construir un nuevo ecosistema de herramientas creativas. Esta decisi√≥n tuvo un coste deliberado. Tras crecer un 45% en 2022, el crecimiento de la compa√±√≠a se desaceler√≥ dr√°sticamente a solo un 11% en 2023 . Fue una ralentizaci√≥n consciente: sacrificaron los resultados a corto plazo para invertir en su supervivencia y liderazgo a largo plazo. | A√±o Fiscal | Ingresos (‚Ç¨M) | Crecimiento Anual (%) | Contexto Estrat√©gico Clave | | :--- | :--- | :--- | :--- | | 2019 | 31 | - | Crecimiento org√°nico pre-adquisici√≥n. | | 2021 | 61.5 | - | Crecimiento acelerado post-EQT; supera a Shutterstock. | | 2022 | 79 | 45% | Fuerte crecimiento; inicio de la disrupci√≥n de la IA. | | 2023 | 88 | 11% | A√±o del pivote a la IA : Crecimiento ralentizado para invertir en la AI Suite. | | 2024 (E) | > 100 | > 13.6% | Retorno al crecimiento impulsado por las nuevas herramientas de IA. | El resultado de esta inversi√≥n es la Freepik AI Suite , un ecosistema que incluye desde un generador de im√°genes y v√≠deo hasta herramientas innovadoras como Pikaso (que convierte bocetos en im√°genes en tiempo real) y la tecnolog√≠a de escalado de la reci√©n adquirida Magnific . El Arma Secreta: Un Ecosistema H√≠brido para el Mundo Real La estrategia de Freepik no es ganar a Midjourney en la creaci√≥n de arte conceptual. Su objetivo es ganar en el flujo de trabajo comercial . Mientras Midjourney sirve al \"Artista/Explorador\", Freepik se ha posicionado para dominar el segmento, mucho m√°s grande, del \"Creador/Comunicador\": el profesional de marketing, el educador o el peque√±o empresario que necesita crear contenido de aspecto profesional de forma r√°pida y, sobre todo, legalmente segura. Su ventaja competitiva reside en un modelo h√≠brido √∫nico: Flujo de Trabajo Integrado : Un usuario puede generar una imagen con IA, quitarle el fondo, a√±adirle un icono de su librer√≠a de stock y ponerle texto con su editor, todo dentro de la misma plataforma y suscripci√≥n. Esto es imposible en herramientas de IA puras. Biblioteca de Stock como Foso Estrat√©gico : Su gigantesca librer√≠a de m√°s de 200 millones de activos ya no es solo un producto, sino una ventaja defensiva. Ofrece una v√≠a r√°pida y segura para los usuarios (\"es m√°s f√°cil retocar una foto de stock que generar la perfecta desde cero\") y funciona como un potencial dataset \"limpio",
    "url": "https://datalaria.com/es/posts/freepik/",
    "slug": "freepik",
    "lang": "es",
    "categories": [
      "casos_exito",
      "IA"
    ],
    "tags": [
      "freepik",
      "ia",
      "ia generativa",
      "malaga",
      "startup",
      "modelo de negocio",
      "disrupcion"
    ],
    "date": "2025-10-11",
    "timestamp": 0,
    "domain": "General",
    "image": ""
  },
  {
    "objectID": "es_game_snake",
    "title": "De Cero a H√©roe: Crea un Snake Game Cyberpunk con Ranking en Tiempo Real usando Supabase y JS Vanilla",
    "description": "Aprende a crear un juego web estilo arcade con est√©tica ne√≥n utilizando solo HTML5 Canvas y JS Vanilla. Descubre c√≥mo integrar un ranking global en tiempo real usando la base de datos PostgreSQL de Supabase.",
    "content": "¬øRecuerdas el cl√°sico Snake de los viejos Nokia? Ahora imag√≠nalo con luces de ne√≥n, m√∫sica sintetizada, part√≠culas explosivas y, lo m√°s importante: conectado a la nube . El desarrollo de videojuegos web ha vivido un renacimiento gracias a la potencia de HTML5 Canvas y las herramientas Serverless . Antiguamente, si quer√≠as guardar las puntuaciones de tus jugadores para hacer un ranking global, necesitabas montar un servidor, configurar una API REST, gestionar bases de datos y pagar un hosting. Hoy, gracias al Backend-as-a-Service (BaaS), podemos hacerlo en minutos. En este tutorial, vamos a construir \"Neon Snake\" , un arcade web est√©tico y funcional que utiliza Supabase (una alternativa Open Source a Firebase) para persistir datos en PostgreSQL. üõ†Ô∏è El Stack Tecnol√≥gico Vamos a mantenerlo simple pero potente. Sin frameworks pesados, solo la pureza de la web: Frontend: HTML5 Canvas + JavaScript Vanilla (ES6+). Estilos: Tailwind CSS (v√≠a CDN) para la interfaz de usuario. Backend: Supabase (PostgreSQL + Realtime). Audio: Web Audio API para efectos de sonido generados por c√≥digo. Paso 1: Configurando el Backend con Supabase Antes de escribir una sola l√≠nea de JavaScript, necesitamos nuestro cerebro en la nube. Supabase nos ofrece una base de datos PostgreSQL real con una API generada autom√°ticamente. Crea una cuenta gratuita en supabase.com . Crea un nuevo proyecto llamado NeonSnake . Ve a la secci√≥n Table Editor y crea una nueva tabla llamada snake_scores . La Magia del SQL Para agilizar el proceso, Supabase te permite ejecutar comandos SQL directamente. Ve al SQL Editor y pega el siguiente c√≥digo. Esto crear√° la estructura de la tabla y configurar√° las pol√≠ticas de seguridad (RLS - Row Level Security) para que tu juego pueda leer y escribir datos p√∫blicamente. [code block] Nota de Seguridad: En un juego comercial, usar√≠amos autenticaci√≥n de usuarios completa. Para este arcade tipo demo, permitimos el acceso p√∫blico (an√≥nimo) para reducir la fricci√≥n y que cualquiera pueda jugar y registrar su r√©cord al instante. Finalmente, ve a Settings > API y copia tu Project URL y tu anon public key . Las necesitar√°s en breve. Paso 2: El Frontend Cyberpunk (HTML5 + Canvas) Nuestro juego vivir√° en un solo archivo index.html . Usaremos Tailwind CSS para los men√∫s flotantes (Game Over, Start Screen) y el elemento <canvas> para renderizar el juego a 60 FPS. La estructura b√°sica es la siguiente: [code block] L√≥gica del Juego: M√°s all√° de comer manzanas Para darle ese toque \"Pro\" y diferenciarlo de un tutorial b√°sico, implementamos varias mec√°nicas clave en el JavaScript: Loop de Animaci√≥n: Usando requestAnimationFrame para mantener la fluidez a 60 FPS. Sistema de Part√≠culas: Un array de objetos que se generan en las coordenadas de la \"comida\" al ser ingerida y se desvanecen gradualmente, simulando una explosi√≥n de datos. Firewalls Din√°micos: A partir del nivel 5, generamos obst√°culos est√°ticos que el jugador debe esquivar, aumentando la dificultad progresivamente. Controles H√≠bridos: PC: Escuchamos eventos keydown para las flechas direccionales. M√≥vil: Capturamos touchstart y touchend para calcular la direcci√≥n del gesto ( swipe ) y mover la serpiente. Paso 3: Conectando los puntos (Integraci√≥n con Supabase) Aqu√≠ es donde ocurre la magia del Backend-as-a-Service. Vamos a conectar nuestro Canvas con la base de datos PostgreSQL. Dentro de nuestra etiqueta <script type=\"module\"> : 1. Inicializaci√≥n del Cliente [code block] 2. El Flujo de \"Game Over\" Cuando el jugador pierde y pulsa \"UPLOAD DATA\", necesitamos asegurarnos de que el dato se guarda antes de mostrar la tabla de clasificaci√≥n. Usaremos async/await para controlar este flujo as√≠ncrono. [code block] 3. Obtener el Ranking (Select) Recuperamos el Top 20 ordenado por puntuaci√≥n descendente para mostrar qui√©n manda en el servidor. [code block] Bonus: Realtime ‚ö° ¬øQuieres que el ranking se actualice en la pantalla de todos los jugadores conectados si alguien rompe el r√©cord en ese preciso instante? Supabase hace esto trivial con sus suscripciones: [code block] El Resultado Final Al unir todo, obtienes una experiencia de usuario fluida y altamente competitiva: El usuario juega fren√©ticamente esquivando \"Firewalls\" de ne√≥n. Al chocar, el juego se detiene con un efecto visual de \"glitch\". Introduce su nombre (ej: \"Neo\"). Al pulsar enviar, en milisegundos , su nombre aparece en la tabla de clasificaci√≥n. Si su amigo est√° jugando en otro m√≥vil al mismo tiempo, ver√° el nombre de \"Neo\" aparecer en su pantalla sin recargar la p√°gina. Y una vez creado... s√≥lo nos queda disfrutar y jugar üêçüòä Datalaria Snake Game Conclusi√≥n Hemos pasado de un simple canvas est√°tico a una aplicaci√≥n Fullstack en tiempo real sin tocar un servidor backend tradicional (Node.js, PHP, Python) ni gestionar infraestructura. La combinaci√≥n de la creatividad del desarrollo de videojuegos con la potencia de herramientas BaaS como Supabase abre un mundo de posibilidades para desarrolladores frontend. ¬øEl siguiente paso?",
    "url": "https://datalaria.com/es/posts/game_snake/",
    "slug": "game_snake",
    "lang": "es",
    "categories": [
      "Desarrollo Web",
      "GameDev",
      "Tutoriales"
    ],
    "tags": [
      "JavaScript",
      "Supabase",
      "PostgreSQL",
      "Canvas",
      "HTML5",
      "TailwindCSS"
    ],
    "date": "2026-02-07",
    "timestamp": 0,
    "domain": "General",
    "image": "imagen_snake_game.png"
  },
  {
    "objectID": "es_graphext",
    "title": "Graphext: La Startup Espa√±ola que Invirti√≥ 7 A√±os en Crear el 'F√≥rmula 1' del An√°lisis de Datos",
    "description": "La historia de Graphext, la startup espa√±ola que desafi√≥ las normas invirtiendo 7M‚Ç¨ en I+D antes de escalar, creando una plataforma √∫nica de IA Explicable que redefine el an√°lisis de datos.",
    "content": "En el vertiginoso mundo de las startups, la m√°xima suele ser \"crece r√°pido o muere\". Lanzar un producto m√≠nimo viable, conseguir tracci√≥n y levantar rondas de financiaci√≥n millonarias parece el camino obligado. Pero, ¬øy si hubiera otra forma? ¬øUna estrategia paciente, casi artesanal, centrada en construir una tecnolog√≠a tan avanzada que cree un foso competitivo insalvable antes de pisar el acelerador comercial? Esa es la historia de Graphext , la startup fundada por dos ingenieros inform√°ticos espa√±oles que dedicaron siete a√±os y aproximadamente 7 millones de euros a perfeccionar su plataforma antes de buscar una financiaci√≥n significativa. Hoy, armados con una tecnolog√≠a de vanguardia y una visi√≥n clara centrada en la Inteligencia Artificial Explicable (XAI) , est√°n listos para redefinir c√≥mo las empresas interact√∫an con sus datos. El Origen: De Analizar Twitter a Entender el Mundo La chispa de Graphext no surgi√≥ en un plan de negocio, sino en la curiosidad. Los fundadores, Victoriano Izquierdo y Miguel Cant√≥n , ingenieros inform√°ticos con vocaci√≥n emprendedora desde ni√±os, empezaron con una herramienta llamada contexto.io , enfocada en analizar las conexiones en Twitter. Pronto se dieron cuenta de que el verdadero potencial resid√≠a en ir m√°s all√°, en crear contextos de informaci√≥n m√°s amplios, en visualizar las redes ocultas que conectan personas y organizaciones en cualquier conjunto de datos. As√≠ naci√≥ Graphext en 2015, fusionando 'graph' (grafo) y 'context'. Su misi√≥n: democratizar la ciencia de datos , cerrar la brecha entre los expertos en c√≥digo y los analistas de negocio que tienen las preguntas importantes pero no las herramientas para responderlas directamente. Quer√≠an superar las limitaciones de Excel (demasiado b√°sico) y los notebooks de programaci√≥n como Jupyter (demasiado complejos para no t√©cnicos), aspirando a crear algo nuevo: una herramienta \"tan interactiva como Figma, pero para la ciencia de datos\" . La Tecnolog√≠a: Un \"F√≥rmula 1\" en tu Navegador Lo que diferencia radicalmente a Graphext es su arquitectura. Tras a√±os de I+D intensivo, han construido lo que su CEO describe como un \"F√≥rmula 1\": una m√°quina de an√°lisis incre√≠blemente potente. Su secreto reside en exprimir tecnolog√≠as web de vanguardia como WebAssembly (Wasm) , WebGL y Apache Arrow . Gracias a Wasm, gran parte del procesamiento de datos (¬°hasta un 80-90%!) ocurre directamente en el navegador del usuario , no en un servidor remoto. El resultado es una fluidez asombrosa: explorar y filtrar millones de filas de datos se siente instant√°neo. Han desarrollado sus propias librer√≠as de compresi√≥n y un lenguaje \"low-code\" interno. Esta profunda inversi√≥n tecnol√≥gica crea, seg√∫n sus fundadores, un foso competitivo muy dif√≠cil de replicar. La Plataforma: Del Dato Crudo al Modelo Explicable Graphext no es solo una herramienta de visualizaci√≥n, es una plataforma integral \"no-code/low-code\" que cubre todo el ciclo de vida del an√°lisis: Conexi√≥n Universal : Importa desde un simple CSV o conecta directamente a data warehouses modernos (Snowflake, BigQuery, Databricks, Redshift). Exploraci√≥n Visual Interactiva (EDA) : El coraz√≥n de la herramienta. Filtra, agrupa, cruza variables y enriquece datos sobre la marcha. Modelado Avanzado sin C√≥digo : Aplica algoritmos de machine learning (clustering, NLP para an√°lisis de texto, an√°lisis de im√°genes) con clics, no con c√≥digo. Predicci√≥n con Explicabilidad (XAI) : Crea modelos predictivos (para predecir qu√© cliente abandonar√° o qu√© lead de ventas es m√°s prometedor) y, crucialmente, entiende por qu√© el modelo hace esa predicci√≥n. Esta apuesta por la IA Explicable es el eje de su estrategia futura. El Dilema del \"F√≥rmula 1\": Potencia vs. Accesibilidad A pesar de su enfoque \"no-code\", Graphext reconoce una tensi√≥n: su herramienta es tan potente que requiere un \"piloto\" h√°bil para sacarle todo el partido. No es para principiantes absolutos, sino para analistas de negocio, cient√≠ficos de datos y power users que buscan superpoderes. Esta dualidad se refleja en su modelo de negocio h√≠brido: Autoservicio (Free y Pro) : Para captar usuarios y permitir la difusi√≥n org√°nica (Product-Led Growth). Enterprise : Con precios personalizados y servicios de formaci√≥n e ingenier√≠a de datos, reconociendo que las grandes empresas necesitan acompa√±amiento. La empresa est√° evolucionando de vender tickets de 1.000‚Ç¨ a cerrar contratos de seis o siete cifras con corporaciones como McDonald's o Roche . Una Estrategia Financiera At√≠pica y Paciente Graphext desafi√≥ la norma del capital riesgo. Durante sus primeros 7 a√±os, se financi√≥ con una combinaci√≥n inteligente: Capital Semilla Modesto : Rondas peque√±as, incluyendo una liderada por K Fund . Subvenciones P√∫blicas Clave : Aproximadamente 2 millones de euros de fondos europeos (Horizonte 2020, EIC Fund), cruciales para financiar el I+D sin diluirse excesivamente. Esta estrategia les permiti√≥ construir su \"F√≥rmula 1\" tecnol√≥gico con paciencia. Una vez madurado el producto y reduc",
    "url": "https://datalaria.com/es/posts/graphext/",
    "slug": "graphext",
    "lang": "es",
    "categories": [
      "casos_exito",
      "IA"
    ],
    "tags": [
      "graphext",
      "ia",
      "ia explicable",
      "no-code",
      "startup",
      "big data",
      "visualizacion"
    ],
    "date": "2025-11-28",
    "timestamp": 0,
    "domain": "General",
    "image": "cover.png"
  },
  {
    "objectID": "es_ia-educacion-deep_research",
    "title": "Investigando con IA: C√≥mo Cre√© un Informe detallado sobre el impacto de la IA en la Educaci√≥n a nivel global en Minutos con Deep Research de Gemini",
    "description": "Un caso pr√°ctico de c√≥mo us√© la funcionalidad Deep Research de Gemini para generar un informe exhaustivo sobre el impacto de la IA en la educaci√≥n y un resumen revelador de la misma.",
    "content": "La investigaci√≥n profunda es la base del conocimiento, pero seamos sinceros: es un proceso arduo y que requiere mucha dedicaci√≥n y trabajo. Recopilar docenas de fuentes, sintetizar datos contradictorios y estructurar una narrativa coherente puede llevar semanas, si no meses. ¬øY si pudi√©ramos acelerar ese proceso de semanas a minutos? Esa es la promesa de la funcionalidad Deep Research de Gemini , una herramienta que he puesto a prueba para crear un informe exhaustivo sobre uno de los temas m√°s candentes de nuestro tiempo: el impacto de la Inteligencia Artificial en la Educaci√≥n . Este post no es solo sobre los fascinantes hallazgos de esa investigaci√≥n, sino tambi√©n sobre el propio proceso. Es la cr√≥nica de c√≥mo una simple pregunta a la IA puede generar un an√°lisis de calidad acad√©mica, y c√≥mo podemos transformar esa densa informaci√≥n en una historia accesible para todos. El Punto de Partida: Una Pregunta a Gemini Todo comenz√≥ con una conversaci√≥n entre amigos sobre como nos imagin√°bamos la educaci√≥n de esta generaci√≥n y las pr√≥ximas teniendo en cuenta el impacto de la Inteligencia Artificial que lo est√° cambiando todo. Para dotar de m√°s profundidad y detalle a nuestras reflexiones, lanzamos una simple petici√≥n a Gemini con su opci√≥n de Deep Research a ver que es lo que pasaba, le preguntamos simplemente \"Investiga sobre el impacto de la IA en la educaci√≥n a nivel global\". La herramienta se puso a trabajar, analizando y sintetizando informaci√≥n de multitud de fuentes para generar un informe completo y estructurado durante unos 5-10 minutos... y este fue el resultado de su trabajo: Investigaci√≥n sobre el impacto de la IA en la educaci√≥n de Deep Research Lo que sigue a continuaci√≥n son los hallazgos m√°s reveladores de ese an√°lisis, un conocimiento que ahora podemos explorar en una fracci√≥n del tiempo que habr√≠a llevado tradicionalmente manteniendo un alt√≠simo nivel de calidad en su contenido. El Panorama Global: Un Mercado Explosivo Chocando con la √âtica Para empezar, hablemos de dinero. El mercado de la IA en la educaci√≥n est√° en plena ebullici√≥n, con proyecciones que superan los 32 mil millones de d√≥lares para 2030 , impulsado por una demanda insaciable de aprendizaje personalizado. Sin embargo, esta expansi√≥n vertiginosa choca con la visi√≥n de organismos como la UNESCO , que abogan por un enfoque centrado en el ser humano y la √©tica. El problema es que la tecnolog√≠a se mueve mucho m√°s r√°pido que las pol√≠ticas y las regulaciones. Una encuesta de 2023 revel√≥ un dato alarmante: solo el 10% de las escuelas y universidades a nivel mundial tienen un marco oficial para el uso de la IA . Esto crea un \"abismo entre la pol√≠tica y la pr√°ctica\", donde los riesgos no se est√°n abordando en el su uso del d√≠a a d√≠a. Si quieres una visi√≥n general y r√°pida de este complejo panorama, he utilizado NotebookLM (recordando lo aprendido en Post de NotebookLM en Datalaria ) para generar un v√≠deo resumen con los puntos clave de este informe que puedes ver aqu√≠: V√≠deo Resumen del Informe sobre IA en Educaci√≥n . La Carrera Mundial por la IA Educativa: 4 Pa√≠ses, 4 Estrategias La forma en que cada pa√≠s adopta la IA en sus aulas es un reflejo de sus valores y ambiciones geopol√≠ticas. El aula se ha convertido en un nuevo escenario para la competencia global. China: Su estrategia es centralizada y obligatoria. A partir de 2025, la educaci√≥n en IA ser√° obligatoria para todos los estudiantes de primaria y secundaria . El curr√≠culo es sistem√°tico, diferenciado por edad y establece un m√≠nimo de ocho horas de instrucci√≥n en IA al a√±o. Los alumnos de primaria se centran en el aprendizaje experiencial con rob√≥tica, mientras que los de secundaria abordan proyectos avanzados y algoritmos. Singapur: Integra la IA como parte de su \"Plan Maestro de EdTech 2030\", con un enfoque sist√©mico y una fuerte inversi√≥n en I+D. La estrategia se centra en la plataforma nacional Student Learning Space (SLS) , que se est√° mejorando con herramientas de IA accesibles para todos los estudiantes. Ya se est√°n implementando programas piloto, como un Sistema de Aprendizaje Adaptativo para matem√°ticas que ofrece recomendaciones personalizadas en 33 escuelas desde 2023. Finlandia: Fiel a sus valores n√≥rdicos, basa su estrategia en la √©tica y la privacidad de datos . Su piedra angular es la formaci√≥n de docentes y la alfabetizaci√≥n en IA para todos los ciudadanos. Una de sus iniciativas m√°s reconocidas es el curso gratuito y mundialmente famoso \"Elements of AI\" de la Universidad de Helsinki. Adem√°s, su enfoque en la √©tica es tan profundo que llegan a exigir revisiones √©ticas para el uso de herramientas como ChatGPT en el aula. Estados Unidos: Con una estructura federal, carece de un mandato √∫nico. El enfoque se basa en incentivos y asociaciones p√∫blico-privadas, creando un mosaico de pol√≠ticas diversas. Esto se materializa en iniciativas como el \"Desaf√≠o Presidencial de IA\", que anima a los estudiantes a usar la IA para resolver problemas comunitarios. Esta descentralizaci√≥",
    "url": "https://datalaria.com/es/posts/ia-educacion-deep_research/",
    "slug": "ia-educacion-deep_research",
    "lang": "es",
    "categories": [
      "Herramientas",
      "IA"
    ],
    "tags": [
      "ia",
      "gemini",
      "deep research",
      "educaci√≥n",
      "investigaci√≥n",
      "edtech",
      "unesco",
      "deep reserach"
    ],
    "date": "2025-09-27",
    "timestamp": 0,
    "domain": "General",
    "image": "cover.png"
  },
  {
    "objectID": "es_ia_agents_part1",
    "title": "Proyecto Autopilot: Por qu√© me desped√≠ como Community Manager para construir un Ej√©rcito de agentes de IA",
    "description": "Expermentar con la IA y las nuevas tecnolog√≠as es mi pasi√≥n; la distribuci√≥n de estos contenidos no tanto. En esta serie, documento c√≥mo estoy construyendo un equipo automatizado de Agentes IA para mis posts y gestionar su divulgaci√≥n a trav√©s de mis redes sociales.",
    "content": "En el mundo de los blogs t√©cnicos y la ingenier√≠a, a menudo nos enfrentamos a la \"Paradoja del Constructor\". Podemos pasar 40 horas perfeccionando un tema concreto, definiendo una arquitectura o depurando peque√±os detalles t√©cnicos. Sin embargo, no se encuentran 15 minutos para promocionar el trabajo realizado de manera efectiva en redes sociales. He llegado a ese punto con Datalaria . El contenido est√° ah√≠, la arquitectura est√° optimizada, pero la distribuci√≥n sufre por culpa del cuello de botella principal... bueno, yo. Hoy tomo una decisi√≥n estrat√©gica. Me estoy \"despidiendo\" del rol de Community Manager, el cual realmente nunca llegu√© a ejercer. En mi lugar, no voy a contratar una agencia; voy a construir una y experimentar con los, tan de moda, \"agentes IA\". Bienvenido al Proyecto Autopilot : una serie de 5 partes donde construiremos, en vivo y en p√∫blico, un sistema aut√≥nomo de Agentes de IA que lee este blog, lo analiza al detalle y, de manera aut√≥noma, preparan el contenido para su promoci√≥n y lo distribuyen en las redes sociales mientras por mi parte estoy a otros menesteres. La Estrategia: Dogfooding Extremo El concepto es simple pero t√©cnicamente ambicioso. Vamos a ejecutar una estrategia de \"dogfooding\" (comer nuestra propia comida). En lugar de usar herramientas de terceros como Buffer o Hootsuite , construiremos un motor de distribuci√≥n personalizado utilizando las mismas tecnolog√≠as sobre las que escribimos: IA Generativa y Pipelines CI/CD . El \"Gran Objetivo\" es transformar el proceso de blogging. Actualmente, \"publicar\" significa hacer un push de un archivo Markdown a GitHub. En el futuro, ese git push desencadenar√° una reacci√≥n en cadena donde agentes inteligentes analizan, crean y distribuyen contenido. La Arquitectura: Conoce al Equipo Para resolver esto, un simple script de Python no es suficiente. Necesitamos capacidades de razonamiento. Necesitamos un sistema que entienda contexto, tono y audiencia. Estamos dise√±ando una arquitectura orientada a eventos alojada completamente dentro de GitHub Actions , utilizando Google Gemini como cerebro y CrewAI como orquestador. Aqu√≠ est√° el flujo conceptual del sistema que vamos a construir: graph TD %% Estilos classDef human fill:#ff9f43,stroke:#333,stroke-width:2px,color:white; classDef code fill:#5f27cd,stroke:#333,stroke-width:2px,color:white; classDef ai fill:#0abde3,stroke:#333,stroke-width:2px,color:white; classDef social fill:#ee5253,stroke:#333,stroke-width:2px,color:white; %% Nodos User(\"üë±‚Äç‚ôÇÔ∏è Yo / Autor\"):::human Git[\"üìÇ Repositorio GitHub <br/> Push nuevo archivo .md\"]:::code Action[\"‚öôÔ∏è GitHub Actions <br/> Runner CI/CD\"]:::code %% FIX: Cambiada direcci√≥n a LR para evitar solapamiento subgraph TeamAI [\"ü§ñ El Equipo (CrewAI + Gemini)<br/><br/>\"] direction TB Orchestrator{\"üß† Orquestador\"}:::ai Analyst[\"üïµÔ∏è Agente 1: El Analista <br/> (Extrae Metadata y Ganchos)\"]:::ai WriterX[\"üê¶ Agente 2: Redactor Twitter <br/> (Contenido Viral/Corto)\"]:::ai WriterLI[\"üíº Agente 3: Redactor LinkedIn <br/> (Tono Profesional)\"]:::ai end Review(\"üëÄ Revisi√≥n Humana <br/> Pull Request / Borrador\"):::human X[\"Twitter / X API\"]:::social LI[\"LinkedIn API\"]:::social %% Conexiones User -->|git push| Git Git -->|Trigger| Action Action -->|Inicia Proceso| Orchestrator Orchestrator -->|Texto Crudo| Analyst Analyst -->|JSON| Orchestrator Orchestrator -->|Contexto + Ganchos| WriterX Orchestrator -->|Contexto + Claves| WriterLI WriterX -->|Borrador| Review WriterLI -->|Borrador| Review Review -->|Aprobar| X Review -->|Aprobar| LI Decisiones de Arquitectura El Trigger (GitHub Actions): ¬øPor qu√© pagar por un servidor? El blog es est√°tico (Hugo), as√≠ que la automatizaci√≥n debe ser ef√≠mera. Solo se ejecuta cuando publico. El Cerebro (Gemini 3 Pro): Elegimos este modelo por su gran ventana de contexto. Necesita leer tutoriales t√©cnicos completos sin \"olvidar\" el principio. El Orquestador (CrewAI): Esto nos permite asignar personas o roles espec√≠ficos. No queremos una IA gen√©rica; queremos un \"Experto en Twitter C√≠nico\" y un \"Estratega Corporativo\" trabajando en paralelo. Prueba de Concepto: ¬øPuede la IA entender mi c√≥digo? Antes de escribir una sola l√≠nea del pipeline final, necesitaba validar la hip√≥tesis central: ¬øPuede Gemini entender realmente la estructura de mis posts en Hugo? Ejecut√© una prueba usando un prompt de sistema dise√±ado para actuar como un \"Editor T√©cnico Senior\". El objetivo no era escribir texto, sino extraer datos estructurados (JSON) de mis archivos Markdown crudos. El resultado fue prometedor: El modelo identific√≥ correctamente el Stack Tecnol√≥gico , gener√≥ un resumen, y lo m√°s importante, extrajo \"√Ångulos Provocativos\" para marketing. Este JSON estructurado es lo que alimentar√° a nuestros agentes redactores en la siguiente fase. La Hoja de Ruta (Roadmap) Esta serie es el n√∫cleo de la estrategia de contenido de Datalaria para los pr√≥ximos meses. Documentaremos el dolor, los bugs y las victorias en tiempo real. Post 1: La Estrategia (Est√°s aqu√≠). El Plan Maest",
    "url": "https://datalaria.com/es/posts/ia_agents_part1/",
    "slug": "ia_agents_part1",
    "lang": "es",
    "categories": [
      "Automatizaci√≥n",
      "Inteligencia Artificial",
      "DevOps"
    ],
    "tags": [
      "Agentes",
      "Gemini",
      "CrewAI",
      "GitHub Actions",
      "Python",
      "Dogfooding"
    ],
    "date": "2025-12-27",
    "timestamp": 0,
    "domain": "Projects",
    "image": "cover.png"
  },
  {
    "objectID": "es_ia_agents_part2",
    "title": "Autopilot - El Cerebro: Configurando Gemini y CrewAI para leer mi blog",
    "description": "Segundo cap√≠tulo de Proyecto Autopilot. Abrimos el IDE para conectar Python con Gemini Flash y crear nuestro primer Agente Analista capaz de entender c√≥digo Markdown.",
    "content": "En el Post 1: La Estrategia , promet√≠ que no usar√≠a herramientas de terceros para gestionar mis redes sociales. Promet√≠ construir un \"ej√©rcito de agentes\". Hoy, dejamos el PowerPoint y abrimos el IDE. Vamos a construir el Cerebro del sistema. El objetivo de hoy es t√©cnico y concreto: crear un script en Python que sea capaz de leer un archivo .md de mi repositorio local, \"leerlo\" como lo har√≠a un ingeniero senior, y devolverme un an√°lisis estructurado en JSON. El Stack Tecnol√≥gico: Velocidad sobre Potencia Bruta Para esta tarea, he tomado dos decisiones de arquitectura: El Orquestador: CrewAI. Necesito algo que maneje \"Agentes\" y \"Tareas\", no solo cadenas de texto. CrewAI me permite definir roles (qui√©n eres) y goals (qu√© quieres), lo cual es vital para los siguientes pasos. El Modelo: Google Gemini Flash. Al principio intent√© usar el modelo m√°s potente (Pro), pero me di cuenta de un error de principiante: para tareas de lectura masiva y extracci√≥n de datos, no necesitas al fil√≥sofo, necesitas al bibliotecario r√°pido. Flash es mucho m√°s r√°pido, barato (gratis en el tier actual) y tiene una ventana de contexto gigantesca. Paso 1: Higiene del Repositorio (Clean Monorepo) Antes de escribir c√≥digo, hay que organizar la casa. Mi blog est√° hecho en Hugo, y no quiero ensuciar la carpeta del sitio web con scripts de Python. He optado por una estructura de \"Monorepo Limpio\". He creado una carpeta autopilot en la ra√≠z del proyecto que act√∫a como un m√≥dulo independiente. [code block] Lecci√≥n aprendida: Configura tu .gitignore antes de hacer el primer commit. Si subes tu API Key a GitHub por error, los bots tardar√°n segundos en encontrarla o peor a√∫n, puede que te encuentres con costes indeseados de otros usuarios que hayan encontrado tu API Key. Paso 2: El C√≥digo (Manos a la obra) El coraz√≥n de este sistema no es main.py , sino c√≥mo definimos al agente. Usando la librer√≠a crewai y langchain_google_genai , defin√≠ a mi primer empleado digital: \"El Analista\" . El Problema de la \"Obsesi√≥n con OpenAI\" Aqu√≠ me top√© con el primer muro. CrewAI est√° dise√±ado por defecto para buscar una API Key de OpenAI (GPT-4). Aunque yo configur√© Gemini, el script fallaba con el error: ValueError: OPENAI_API_KEY is required La soluci√≥n fue forzar expl√≠citamente el LLM (Large Language Model) dentro de la definici√≥n del agente. As√≠ se ve el c√≥digo en src/agents.py : [code block] Paso 3: El Prompt de Ingenier√≠a (JSON Mode) Para que esto sea √∫til, el agente no puede simplemente \"charlar\" sobre el art√≠culo. Necesito datos que pueda procesar computacionalmente despu√©s. En src/tasks.py , defin√≠ la tarea con instrucciones estrictas de salida. No us√© el \"JSON Mode\" nativo de la API (que a veces es complejo de configurar), sino que confi√© en la capacidad de instrucci√≥n del modelo: \"OUTPUT FORMAT: Return ONLY a valid JSON object with keys: summary, target_audience, tech_stack, key_takeaways, marketing_hooks.\" El Resultado: ¬°Funciona! Ejecut√© el script contra uno de mis art√≠culos t√©cnicos m√°s densos (sobre procesos S&OP). Ten√≠a miedo de que el modelo alucinara o se perdiera en el texto. El resultado en la terminal fue este JSON limpio: [code block] Es impresionante. El modelo no solo resumi√≥ el texto, sino que entendi√≥ el contexto profundo : identific√≥ que el art√≠culo hablaba de \"Mermaid.js\" y \"BPMN\" y gener√≥ ganchos de marketing (\"marketing_hooks\") que realmente suenan a algo que yo escribir√≠a en Twitter. ¬øQu√© sigue? Ya tenemos el Cerebro ( autopilot ) capaz de leer y entender lo que escribo en content . Los datos est√°n estructurados y listos. Pero un JSON no consigue likes. En el pr√≥ximo post, vamos a construir a Los Creativos . Usaremos estos datos para alimentar a dos nuevos agentes con personalidades opuestas: un experto en viralidad para Twitter y un estratega corporativo para LinkedIn. Y veremos c√≥mo el Prompt Engineering puede cambiar dr√°sticamente el tono de una IA. üëâ C√≥digo Fuente: Puedes ver el c√≥digo de este m√≥dulo en la carpeta /autopilot del repositorio de Datalaria en GitHub .",
    "url": "https://datalaria.com/es/posts/ia_agents_part2/",
    "slug": "ia_agents_part2",
    "lang": "es",
    "categories": [
      "Ingenier√≠a de Software",
      "IA Generativa",
      "Python"
    ],
    "tags": [
      "CrewAI",
      "Gemini API",
      "Backend",
      "Clean Code",
      "Automatizaci√≥n"
    ],
    "date": "2025-12-31",
    "timestamp": 0,
    "domain": "General",
    "image": "cover.png"
  },
  {
    "objectID": "es_ia_agents_part3",
    "title": "Autopilot - Los Creativos: C√≥mo program√© a una IA para que fuera c√≠nica en Twitter y corporativa en LinkedIn",
    "description": "Tercer cap√≠tulo de Proyecto Autopilot. Transformamos datos fr√≠os en historias virales creando dos agentes con personalidades opuestas: un influencer c√≠nico y un l√≠der corporativo.",
    "content": "En el Post 2: El Cerebro , logramos algo t√©cnicamente importante: un script de Python capaz de leer mis art√≠culos t√©cnicos y extraer su esencia en un JSON estructurado. Pero tengo un problema. Si publico un JSON en Twitter, nadie lo va a leer. Los datos son fr√≠os. Las redes sociales son emocionales. Para que este sistema de \"Autopilot\" funcione, no necesito m√°s analistas; necesito creativos . Necesito redactores que entiendan la psicolog√≠a de cada plataforma. Hoy, vamos a darle alma a la m√°quina. Vamos a crear a \"Los Creativos\" . La Teor√≠a del Rol (El M√©todo Stanislavski para IA) Los Grandes Modelos de Lenguaje (LLMs) como Gemini son, en esencia, actores de m√©todo. Si les pides \"escribe un tweet\", te dar√°n un tweet gen√©rico, aburrido y lleno de hashtags como #Tecnolog√≠a #Innovaci√≥n. Pero si les das un rol , una historia de fondo ( backstory ) y una motivaci√≥n, su comportamiento cambia radicalmente. En ingenier√≠a de prompts, esto es la diferencia entre un chatbot y un agente. Para Datalaria, no quiero una voz gen√©rica. Quiero cubrir dos extremos del espectro: 1. El Caos (Twitter/X): Breve, directo, un poco c√≠nico y al√©rgico a lo corporativo. 2. El Orden (LinkedIn): Profesional, inspirador, enfocado en el valor de negocio. Dise√±ando las Personalidades (El C√≥digo) Usando CrewAI , definir estas personalidades es tan sencillo (y complejo) como escribir una biograf√≠a. Aqu√≠ est√° el c√≥digo real de src/agents.py que define a mis dos nuevos empleados digitales. 1. El Influencer Tech (Twitter) Le he pedido expl√≠citamente que odie la jerga corporativa y use min√∫sculas por est√©tica. [code block] 2. El L√≠der de Pensamiento (LinkedIn) Aqu√≠ buscamos el estilo \"Broetry\" (frases cortas con mucho espacio en blanco) que funciona en LinkedIn. [code block] Refactorizaci√≥n: El Desaf√≠o Multiling√ºe Datalaria es un blog global, as√≠ que me enfrent√© a un reto: ¬øNecesito crear 4 agentes distintos para escribir en Espa√±ol e Ingl√©s? La respuesta de ingenier√≠a es NO . Un agente es una entidad con una personalidad; el idioma es solo una herramienta. En lugar de duplicar agentes, dupliqu√© las Tareas ( Tasks ) . En src/tasks.py , ahora defino expl√≠citamente el idioma de salida: [code block] Esto hace que mi pipeline sea escalable. Si ma√±ana quiero publicar en franc√©s, solo a√±ado una tarea, no contrato a un nuevo agente. La Batalla de los Agentes: Resultados Reales Para probar esto, us√© mi art√≠culo sobre \"Procesos S&OP con IA y BPMN\" . Es un tema denso y aburrido si no se vende bien. Veamos qu√© hicieron los agentes con el mismo input. El Resultado en Twitter (El C√≠nico) Nota del autor: Este resultado me doli√≥ un poco, es m√°s directo que yo. Deja de ahogarte en muros de texto. Convertir manualmente narrativas industriales en diagramas es una tortura, es desperdiciar cerebro. Estamos usando genai para convertir docs de S&OP sucios en diagramas BPMN precisos en segundos. As√≠ es como dejas de ser un traductor humano. üßµ La mayor√≠a del \"an√°lisis de negocio\" es solo fricci√≥n costosa y tediosa. La ia identifica dependencias ocultas que los humanos pasan por alto. No es solo dibujar; es descubrir la l√≥gica enterrada bajo el relleno corporativo. Ingenier√≠aDeProcesos #Industry40 El Resultado en LinkedIn (El Corporativo) Deja de ahogarte en \"muros de texto\" y cementerios de PDFs. üß± Los procesos industriales son el coraz√≥n de tu empresa. Pero a menudo est√°n enterrados en densas narrativas de S&OP que nadie lee. Esto crea una brecha masiva entre lo que el negocio necesita y lo que ingenier√≠a construye. He pasado a√±os viendo a L√≠deres T√©cnicos y Gerentes de Operaciones luchar con esta \"capa de traducci√≥n\". ¬øLa buena noticia? La IA Generativa est√° cambiando el juego. Al usar IA como un Analista de Negocio virtual, puedes transformar narrativas complejas en diagramas BPMN precisos en segundos. No se trata solo de velocidad. Se trata de claridad. üëá ¬øC√≥mo est√° tu equipo cerrando la brecha entre negocio y tecnolog√≠a? Hablemos en los comentarios. Conclusi√≥n La diferencia es abismal. El mismo modelo (Gemini 3.0 Flash), leyendo el mismo art√≠culo, ha generado dos piezas de contenido completamente distintas, adaptadas al canal y al idioma. Ya tengo: 1. El Cerebro que entiende el c√≥digo. 2. Los Creativos que escriben el copy. 3. Los archivos generados en mi disco duro. Pero todav√≠a hay un \"humano\" en el bucle. Sigo teniendo que ejecutar python main.py manualmente y copiar-pegar estos textos en las redes sociales. En el pr√≥ximo post, entramos en territorio hostil. Vamos a intentar conectar a estos agentes con el mundo exterior. Pr√≥ximamente Post 4: La Pesadilla de las APIs. Intentar√© conectar mis agentes a Twitter y LinkedIn y (probablemente) casi perder√© la cabeza en el proceso. üëâ C√≥digo Fuente: El c√≥digo actualizado con los nuevos agentes y soporte multiling√ºe est√° disponible en la carpeta /autopilot del repo de GitHub .",
    "url": "https://datalaria.com/es/posts/ia_agents_part3/",
    "slug": "ia_agents_part3",
    "lang": "es",
    "categories": [
      "IA Generativa",
      "Prompt Engineering",
      "Python"
    ],
    "tags": [
      "CrewAI",
      "Personalidad IA",
      "Automatizaci√≥n",
      "Marketing",
      "Storytelling"
    ],
    "date": "2026-01-03",
    "timestamp": 0,
    "domain": "General",
    "image": "cover.png"
  },
  {
    "objectID": "es_ia_agents_part4",
    "title": "Autopilot - La Pesadilla de las APIs: C√≥mo venc√≠ a la burocracia de LinkedIn para automatizar mi empresa",
    "description": "Cuarto cap√≠tulo de Proyecto Autopilot. Lo que iba a ser un script de 10 minutos se convirti√≥ en una guerra de formularios. Narramos c√≥mo desbloqueamos el permiso 'w_organization_social' para publicar como Empresa.",
    "content": "Hasta ahora, todo era divertido. Ten√≠amos agentes de IA con personalidades c√≠nicas ( Post 3 ) y un cerebro capaz de analizar texto ( Post 2 ). Pero todo viv√≠a en la seguridad de mi terminal, en localhost . Para que Datalaria Autopilot fuera real, ten√≠a que salir al mundo. Aqu√≠ es donde el proyecto dej√≥ de ser un problema de ingenier√≠a y se convirti√≥ en una batalla contra la Burocracia de las APIs . El Objetivo: Publicar como Marca, no como Persona Mi requisito era claro: No quiero que el bot publique en mi perfil personal de LinkedIn. Quiero que publique en la P√°gina de Empresa de Datalaria , con el logo oficial y tono corporativo. T√©cnicamente, esto requiere un cambio en el endpoint de la API: * Perfil Personal: urn:li:person:12345 * P√°gina de Empresa: urn:li:organization:110125695 Parece un cambio de una l√≠nea de c√≥digo. Fueron un par de d√≠as de espera y gestiones. Batalla 1: Twitter (X) y el Muro Anti-Bots Primero, Twitter. Conseguir acceso a la API hoy en d√≠a requiere pasar un casting. Tuve que solicitar el Free Tier y escribir una \"carta de motivaci√≥n\" explicando que no soy un bot de spam, sino un t√©cnico aficionado de la IA. Tras superar el error 403 Forbidden (olvid√© activar los permisos de \"Read & Write\") y el error Duplicate Content (intent√© enviar el mismo \"Hello World\" dos veces), logr√© la conexi√≥n. Twitter/X estaba listo y a priori todo funcionaba de manera sencilla. Batalla 2: El Jefe (LinkedIn Company Pages) El mayor problema ocurri√≥ en LinkedIn. Dise√±√© mi script social_manager.py para usar un ID de empresa si exist√≠a en las variables de entorno: [code block] Al ejecutarlo, la consola me escupi√≥ un error rojo sangre: ‚ùå Error posting to LinkedIn: Status 403: ACCESS_DENIED El Permiso Fantasma: w_organization_social Descubr√≠ que el token est√°ndar de LinkedIn solo te da permiso w_member_social (publicar como persona). El permiso para empresas ( w_organization_social ) no exist√≠a en mi panel de desarrollador. Para desbloquearlo, tuve que completar una gimkana administrativa: Verificaci√≥n de P√°gina: Tuve que generar una URL especial en el Developer Portal y aprobarla con mi cuenta de administrador. Resultado: Company Verified . ‚úÖ A√∫n as√≠, no funcionaba: El permiso segu√≠a sin aparecer. La Solicitud Oculta: Tuve que solicitar acceso al producto \"Marketing Developer Platform\" . El Formulario: LinkedIn me hizo rellenar un cuestionario detallando que soy un \"Direct Customer\", que no soy una agencia de publicidad y que mi uso es estrictamente interno para automatizaci√≥n org√°nica. La Victoria Tras unas horas de espera, lleg√≥ el correo de aprobaci√≥n. Volv√≠ a generar el token y... ¬°ah√≠ estaba! Con el nuevo \"Super Token\" cargado en mi .env , ejecut√© el script una √∫ltima vez. [code block] Y la prueba definitiva en la red social: Conclusi√≥n y Siguientes Pasos He logrado lo que parec√≠a imposible: un script de Python que tiene autorizaci√≥n legal para actuar como mi empresa. Pero hay un problema final: Este token caduca en 60 d√≠as. Si no hago nada, en dos meses todo este sistema se romper√°. Adem√°s, sigo ejecutando el script manualmente desde mi ordenador. En el √∫ltimo post de esta serie, vamos a automatizarlo todo. Usaremos GitHub Actions para que el sistema se ejecute solo cada vez que subo un art√≠culo, y (si la API nos deja) implementaremos la renovaci√≥n autom√°tica de tokens. Pr√≥ximamente Post 5: Automatizaci√≥n Total (CI/CD). üëâ C√≥digo Fuente: El m√≥dulo social_manager.py final est√° disponible en el repo de GitHub .",
    "url": "https://datalaria.com/es/posts/ia_agents_part4/",
    "slug": "ia_agents_part4",
    "lang": "es",
    "categories": [
      "Backend",
      "Python",
      "APIs"
    ],
    "tags": [
      "LinkedIn API",
      "Twitter API",
      "OAuth",
      "Automation",
      "DevOps"
    ],
    "date": "2026-01-07",
    "timestamp": 0,
    "domain": "Projects",
    "image": "cover.png"
  },
  {
    "objectID": "es_ia_agents_part5",
    "title": "Autopilot - Final: De Localhost a la Nube con GitHub Actions y CI/CD",
    "description": "Cap√≠tulo final de Proyecto Autopilot. Ya no ejecuto scripts en mi ordenador. Ahora, un simple 'git push' despierta a mis agentes de IA, genera el contenido y lo publica en redes sociales tras mi aprobaci√≥n.",
    "content": "Hemos recorrido un largo camino. Empezamos dise√±ando un Cerebro capaz de leer ( Post 2 ), le dimos personalidad con Agentes Creativos ( Post 3 ) y luchamos contra la burocracia para conseguir unas Manos (APIs) que pudieran publicar legalmente ( Post 4 ). Pero nos quedaba un √∫ltimo gran paso para no ser un esclavo de mi terminal y es que ahora mismo, para publicar, ten√≠a que estar en mi ordenador, abrir la consola y ejecutar python main.py . Eso no es \"Piloto Autom√°tico\". Eso es \"Conducci√≥n Asistida\". Hoy, en el cap√≠tulo final, cortamos los cables. Nos vamos a la nube y automatizamos todo el proceso con mis agentes IA. La Arquitectura del Flujo (Pipeline) El objetivo es el GitOps : que mi √∫nica interacci√≥n con el sistema sea subir cambios a Git. Todo lo dem√°s debe ocurrir por magia (o mejor dicho, por GitHub Actions ). He dise√±ado un flujo de trabajo en dos fases: Fase de Detecci√≥n y Previsualizaci√≥n (Autom√°tica): GitHub detecta un nuevo archivo .md (o cambios en uno existente). Se activa el Orquestador . El sistema detecta el idioma del post (Espa√±ol/Ingl√©s) y calcula la URL correcta. La IA (o el sistema de plantillas) propone un tweet y un post de LinkedIn. El sistema me muestra una \"Vista Previa\" en los logs de ejecuci√≥n, pero no publica nada . Fase de Publicaci√≥n (Manual): El proceso se pausa autom√°ticamente gracias a los Environments de GitHub. Me llega una alerta para revisar el despliegue. Si le doy al bot√≥n verde ( Approve ), el sistema ejecuta la llamada real a las APIs. %%{init: {'theme': 'base', 'themeVariables': { 'primaryColor': '#f0f4f8', 'edgeLabelBackground':'#ffffff', 'tertiaryColor': '#e6e6e6'}}}%% graph TD %% Nodo Inicial START([GitHub detecta cambio en .md]) --> ORC %% --- FASE 1: AUTOM√ÅTICA --- subgraph Phase1 [\"üîπ Fase 1: Detecci√≥n y Previsualizaci√≥n (Autom√°tica)\"] direction TB ORC[Se activa el Orquestador] %% Tareas en paralelo ORC --> TASK1[Detectar Idioma y Calcular URL] ORC --> TASK2[IA propone Tweet y LinkedIn] %% Convergencia TASK1 --> LOGS TASK2 --> LOGS LOGS[Mostrar 'Vista Previa' en Logs de Ejecuci√≥n] LOGS --> NOPUB[üö´ NO SE PUBLICA NADA A√öN] end NOPUB --> PAUSE %% --- FASE 2: MANUAL --- subgraph Phase2 [\"üî∏ Fase 2: Publicaci√≥n (Manual)\"] direction TB PAUSE((‚è∏Ô∏è PAUSA AUTOM√ÅTICA<br/>GitHub Environments)) PAUSE --> ALERT[üîî Llega alerta para revisar despliegue] ALERT --> DECISION{¬øAprobar Despliegue?} %% Camino de Aprobaci√≥n DECISION -- \"Bot√≥n Verde (Approve) ‚úÖ\" --> EXEC[üöÄ Ejecutar llamada real a APIs] %% Camino de Rechazo (Impl√≠cito) DECISION -- \"Rechazar / Cancelar ‚ùå\" --> STOP([Fin del flujo sin publicar]) end %% Estilos para resaltar los pasos finales style EXEC fill:#d4edda,stroke:#28a745,stroke-width:2px,color:#155724 style STOP fill:#f8d7da,stroke:#dc3545,stroke-width:2px,color:#721c24 style PAUSE fill:#fff3cd,stroke:#ffc107,stroke-width:3px El Director de Orquesta ( orchestrator.py ) Necesitaba un script que uniera todas las piezas. Para ello, comenc√© desarrollando un orquestador en Python el cual act√∫a como puente entre el archivo Markdown y mis m√≥dulos de redes sociales. Este script es el encargado de la l√≥gica \"fina\" que a veces olvidamos: * ¬øEs un post en ingl√©s ( /en/ ) o en espa√±ol ( /es/ )? * ¬øTiene imagen destacada para generar la tarjeta de Twitter/X o LinkedIn? * ¬øQuiero que lo escriba la IA o quiero escribirlo yo? La Funcionalidad Estrella: \"Director's Cut\" A veces, la IA no acierta con el tono exacto, o simplemente quiero escribir yo mismo el copy para un anuncio especial. Para no perder la automatizaci√≥n pero mantener el control, implement√© una l√≥gica de \"Sobreescritura Manual\" usando el Frontmatter de Hugo. Si mi script detecta esto en la cabecera del art√≠culo: [code block] El sistema ignora la generaci√≥n autom√°tica y usa mis palabras exactas. Es el equilibrio perfecto: automatizaci√≥n por defecto, control manual cuando es necesario. Seguridad y CI/CD: Dormir Tranquilo El archivo .github/workflows/autopilot.yml es donde ocurre la magia. Aqu√≠ definimos los \"Secretos\" (mis claves de API de Twitter y LinkedIn) y las reglas del juego. Lo m√°s interesante es la protecci√≥n del entorno: [code block] Al definir el entorno como production , GitHub me obliga a revisar y aprobar el despliegue. Esto evita que un error en el c√≥digo o una \"alucinaci√≥n\" de la IA publique contenido no deseado. Adem√°s, hemos configurado el sistema para que Twitter/X genere las Cards con imagen autom√°ticamente y LinkedIn trate el contenido como un \"Art√≠culo\", asegurando que en ambas redes la imagen destacada del blog se vea grande y atractiva. El Resultado Final Ahora, mi proceso de publicaci√≥n es este: Escribo mi art√≠culo en Markdown tranquilamente. Hago git push . Me tomo un caf√©. ‚òï Entro a GitHub desde el m√≥vil, veo la \"Preview\" del tweet generado (en el idioma correcto). Sonr√≠o y pulso Approve . En segundos, el contenido aparece en Twitter y LinkedIn. Sin abrir la terminal. Sin tocar Python. Desde cualquier lugar. Publicaci√≥n en Twitter/X Publicaci√≥n en Linkedin Conclusi√≥n del Proyecto Autopilot",
    "url": "https://datalaria.com/es/posts/ia_agents_part5/",
    "slug": "ia_agents_part5",
    "lang": "es",
    "categories": [
      "DevOps",
      "GitHub Actions",
      "Python"
    ],
    "tags": [
      "CI/CD",
      "Automation",
      "Pipeline",
      "GitOps",
      "Workflow"
    ],
    "date": "2026-01-10",
    "timestamp": 0,
    "domain": "Projects",
    "image": "cover.png"
  },
  {
    "objectID": "es_ia_agents_part6",
    "title": "Autopilot - Extra: Programando una Newsletter Nativa en IA y de Mantenimiento Cero",
    "description": "Cap√≠tulo extra de Proyecto Autopilot. Implementamos un sistema de newsletter automatizado con Brevo que env√≠a emails personalizados a los suscriptores en su idioma preferido cada vez que publico un nuevo art√≠culo.",
    "content": "Pensaba que el Autopilot estaba completo. Twitter, LinkedIn, Dev.to... todo automatizado con un simple git push . Pero me faltaba algo importante: una conexi√≥n directa con los lectores . Las redes sociales son geniales para el alcance, pero los algoritmos deciden qui√©n ve mi contenido. Con una newsletter , yo tengo el control. El email llega directamente a la bandeja de entrada de quien realmente quiere leerme. As√≠ que me puse manos a la obra para a√±adir esta nueva pieza al puzzle. El Objetivo Quer√≠a un sistema que cumpliera estos requisitos: Formulario elegante integrado en el footer de todas las p√°ginas Biling√ºe : que funcione perfectamente en espa√±ol e ingl√©s Segmentaci√≥n por idioma : cada suscriptor elige su idioma preferido Automatizaci√≥n total : cuando publico un post, se env√≠a la newsletter autom√°ticamente Contenido personalizado : emails con el nombre del suscriptor y texto generado por IA Eligiendo la Plataforma: Brevo Despu√©s de evaluar opciones como Mailchimp, ConvertKit y Sendinblue (ahora Brevo), me decid√≠ por Brevo por varias razones: API gratuita para vol√∫menes peque√±os (300 emails/d√≠a) Personalizaci√≥n avanzada con variables en el contenido Listas y segmentaci√≥n para gestionar idiomas Integraci√≥n sencilla con Netlify Functions Cumplimiento GDPR (General Data Protection Regulation) Sede en Francia y cumplimiento GDPR nativo. Arquitectura de la Soluci√≥n El sistema tiene tres componentes principales: %%{init: {'theme': 'base', 'themeVariables': { 'primaryColor': '#f0f4f8', 'edgeLabelBackground':'#ffffff', 'tertiaryColor': '#e6e6e6'}}}%% flowchart LR subgraph Frontend[\"üåê Frontend (Hugo + Netlify)\"] FORM[Formulario en Footer] FUNC[Netlify Function] end subgraph Backend[\"ü§ñ Backend (Autopilot)\"] ORC[Orchestrator] BRAIN[Agente IA Newsletter] MGR[Newsletter Manager] end subgraph Brevo[\"üìß Brevo\"] LIST_ES[Lista ES #3] LIST_EN[Lista EN #4] CAMP[Campa√±as] end FORM -->|\"POST + idioma\"| FUNC FUNC -->|\"API Contacts\"| LIST_ES FUNC -->|\"API Contacts\"| LIST_EN ORC -->|\"Nuevo post\"| BRAIN BRAIN -->|\"Contenido IA\"| MGR MGR -->|\"API Campaigns\"| CAMP CAMP -->|\"Email ES\"| LIST_ES CAMP -->|\"Email EN\"| LIST_EN Parte 1: El Formulario de Suscripci√≥n Dise√±o del Footer El formulario vive en layouts/partials/extend_footer.html , un partial de Hugo que se inyecta autom√°ticamente en toda la web. Lo dise√±√© para que se adapte al idioma de la p√°gina: [code block] El selector de idioma es clave: por defecto toma el idioma de la p√°gina , pero el usuario puede cambiarlo si prefiere recibir los emails en otro idioma. Netlify Function: El Puente con Brevo El formulario env√≠a los datos a una Netlify Function que se encarga de comunicarse con la API de Brevo: [code block] Lo m√°s importante aqu√≠ es la segmentaci√≥n por idioma : - Los suscriptores que eligen ES van a la Lista #3 - Los que eligen EN van a la Lista #4 Parte 2: El Agente de Newsletter Para el contenido del email, cre√© un nuevo \"Agente\" en el sistema de IA del Autopilot. Este agente tiene una personalidad diferente a los de Twitter o LinkedIn: [code block] La diferencia clave con los agentes de redes sociales es el tono personal . Un email es una conversaci√≥n uno a uno, no un post para las masas. El agente no se limita a traducir. Si el post est√° en espa√±ol, el 'cerebro' cambia el contexto para generar un copy culturalmente relevante para la audiencia hispana, en lugar de una simple traducci√≥n literal Parte 3: El Newsletter Manager Esta es la pieza que orquesta todo el env√≠o. Se encarga de: Generar el HTML del email con dise√±o profesional Personalizar el contenido con el nombre del suscriptor Crear y enviar la campa√±a a la lista correcta [code block] El Template HTML El email tiene un dise√±o limpio con personalizaci√≥n din√°mica: [code block] La variable {{ contact.FIRSTNAME }} es magia de Brevo: autom√°ticamente reemplaza con el nombre de cada suscriptor. Parte 4: Integraci√≥n con el Orquestador El √∫ltimo paso fue conectar todo esto con el flujo existente del Autopilot: [code block] La l√≥gica es simple pero poderosa: - Si publico un post en /es/posts/ ‚Üí se env√≠a a la Lista ES - Si publico en /en/posts/ ‚Üí se env√≠a a la Lista EN No hay mezclas ni traducciones autom√°ticas. Cada audiencia recibe contenido en su idioma, sobre posts escritos para ellos. El Resultado Despu√©s de toda esta implementaci√≥n, mi flujo de trabajo qued√≥ as√≠: Escribo un art√≠culo en /content/es/posts/nuevo-articulo/ Hago git push GitHub Actions detecta el cambio El Autopilot: Genera tweets y posts de LinkedIn (como antes) NUEVO : Genera contenido de newsletter personalizado Crea una campa√±a en Brevo Env√≠a el email a todos los suscriptores de la Lista ES Email recibido por los suscriptores: El email incluye: - ‚úÖ Saludo personalizado con el nombre - ‚úÖ Contenido generado por IA con emoticonos sutiles - ‚úÖ Bot√≥n de llamada a la acci√≥n con gradiente - ‚úÖ Logo de Datalaria - ‚úÖ Enlace para darse de baja Configuraci√≥n en GitHub Actions Para que todo funcione en producci√≥n, a√±ad√≠ las siguientes variables al workflow: [cod",
    "url": "https://datalaria.com/es/posts/ia_agents_part6/",
    "slug": "ia_agents_part6",
    "lang": "es",
    "categories": [
      "DevOps",
      "Python",
      "Email Marketing"
    ],
    "tags": [
      "Newsletter",
      "Brevo",
      "API",
      "Automation",
      "Hugo",
      "Netlify Functions"
    ],
    "date": "2026-01-24",
    "timestamp": 0,
    "domain": "Projects",
    "image": "cover.png"
  },
  {
    "objectID": "es_ia_agents_part7",
    "title": "Autopilot - Ctrl: Auditor√≠a de Contenido IA con GitHub Copilot CLI",
    "description": "Construyo autopilot-ctrl, una CLI que usa GitHub Copilot CLI para auditar y mejorar autom√°ticamente el contenido generado por IA antes de publicarlo en redes sociales.",
    "content": "This is a submission for the GitHub Copilot CLI Challenge What I Built autopilot-ctrl es una herramienta de l√≠nea de comandos que audita contenido de redes sociales generado por IA antes de publicarlo. Pi√©nsalo como un \"quality gate\" para tu pipeline de contenido. El Problema Mi blog tiene un sistema de autopilot que genera autom√°ticamente posts para Twitter, LinkedIn y Newsletter cada vez que publico un art√≠culo. Funciona genial... la mayor√≠a del tiempo. Pero a veces la IA produce: üê¶ Tweets gen√©ricos sin gancho üíº Posts de LinkedIn sin estructura üìß Newsletters que revelan demasiado (o muy poco) Necesitaba una forma de evaluar la calidad ANTES de publicar y, si algo no pasaba el corte, mejorarlo autom√°ticamente. La Soluci√≥n autopilot-ctrl usa GitHub Copilot CLI para: Auditar contenido contra criterios espec√≠ficos por plataforma Asignar un score de calidad (0-10) Identificar problemas espec√≠ficos Generar versiones mejoradas del contenido que falla [code block] Demo Comandos disponibles: [code block] Screenshots del flujo: C√≥digo fuente: github.com/Dalaez/datalaria/autopilot/ctrl My Experience with GitHub Copilot CLI üöÄ C√≥mo Us√© Copilot CLI La magia de autopilot-ctrl est√° en c√≥mo integra Copilot CLI en modo no interactivo: [code block] Cada auditor√≠a env√≠a un prompt estructurado a Copilot CLI y parsea la respuesta natural para extraer: - Score num√©rico (ej: \"Rating: 7/10\") - Lista de issues (ej: \"No engagement\", \"Generic hook\") - Sugerencias de mejora üí° Lo Que Aprend√≠ El orden de las flags importa : -p DEBE ser el √∫ltimo argumento Prompts simples funcionan mejor : Los prompts largos y estructurados en modo no interactivo devuelven respuestas vac√≠as Copilot responde en lenguaje natural : Tuve que crear parsers flexibles para extraer datos de respuestas como \" Rating: 7/10 \" ‚ö° El Impacto en Mi Workflow Antes de autopilot-ctrl, revisaba manualmente cada post generado. Ahora: git push ‚Üí Autopilot genera contenido python -m ctrl audit generated_content.json ‚Üí Copilot eval√∫a Si algo falla ‚Üí python -m ctrl fix genera mejoras Contenido aprobado ‚Üí Se publica autom√°ticamente Tiempo ahorrado : ~15 minutos por publicaci√≥n. üõ†Ô∏è Stack T√©cnico Python + Click : Framework CLI Rich : UI de terminal con tablas y colores GitHub Copilot CLI : Motor de evaluaci√≥n IA YAML configs : Prompts personalizables por plataforma Conclusi√≥n autopilot-ctrl demuestra que GitHub Copilot CLI no es solo para generar c√≥digo. Es una herramienta poderosa para integrar IA en cualquier pipeline - en este caso, evaluaci√≥n de calidad de contenido. Si tienes un sistema que genera contenido autom√°ticamente, considera a√±adir un \"quality gate\" con Copilot CLI. Tu audiencia (y tus m√©tricas de engagement) te lo agradecer√°n. ¬øPreguntas? D√©jalas en los comentarios üëá Este post es parte de la serie Proyecto Autopilot , donde documento c√≥mo automatizo la creaci√≥n y publicaci√≥n de contenido usando IA.",
    "url": "https://datalaria.com/es/posts/ia_agents_part7/",
    "slug": "ia_agents_part7",
    "lang": "es",
    "categories": [
      "DevOps",
      "Python",
      "AI"
    ],
    "tags": [
      "devchallenge",
      "githubchallenge",
      "cli",
      "githubcopilot",
      "Copilot CLI",
      "Content Audit",
      "Automation"
    ],
    "date": "2026-02-01",
    "timestamp": 0,
    "domain": "Projects",
    "image": "cover.png"
  },
  {
    "objectID": "es_john-snow",
    "title": "El aut√©ntico John Snow, precursor de la Geolocalizaci√≥n",
    "description": "John Snow: el padre de la epidemiolog√≠a y pionero de la geolocalizaci√≥n",
    "content": "Para la mayor√≠a, el nombre John Snow (o Nieve) evoca al heredero del Trono de Hierro. Sin embargo, existi√≥ otro John Snow (1813‚Äì1858) al que el mundo deber√≠a conocer: el padre de la epidemiolog√≠a moderna. Gracias a su mapa del c√≥lera de 1854, tambi√©n es un referente en la visualizaci√≥n de datos y la geolocalizaci√≥n. John Snow fue un m√©dico ingl√©s qui√©n consigui√≥ demostrar de una manera innovadora que el c√≥lera era causado por el consumo de aguas contaminadas con materias fecales, rompiendo con las teor√≠as vigentes en dicha √©poca que aseguraban que el origen de la enfermedad radicada en la teor√≠a miasm√°tica , es decir, en el aire o las emanaciones f√©tidas de suelos o aguas impuras. Para ello, John Snow analiz√≥ detalladamente los casos del virulento brote de c√≥lera de Londres en 1854 detectando que aproximadamente 700 personas hab√≠an fallecido en el barrio del Soho durante la misma semana. Para facilitar la visualizaci√≥n del problema, el Doctor Snow se sirvi√≥ de un mapa de dicho barrio donde represent√≥ a modo de capas de informaci√≥n geogr√°fica los pozos de agua (identificados mediante un c√≠rculo y etiquetados ‚ÄúPump‚Äù), as√≠ como las muertes que se hab√≠an ido produciendo en cada uno de los edificios del √°rea afectada (representadas mediante l√≠neas negras apiladas en cada vivienda). A ra√≠z de dicha representaci√≥n de la informaci√≥n, identific√≥ el pozo situado en Broad Street, en pleno coraz√≥n de la epidemia, como el culpable de la misma dado que en su √°rea de servicio se registraron m√°s del 70% de las muertes. Sin embargo, y a pesar de la clarividencia de sus resultados, √©stos no fueron tenidos en cuenta de manera inmediata por las autoridades sino que tuvieron que ser acompa√±ados de aclaraciones a ciertas cuestiones confusas. Por ejemplo, el hecho de que apenas se hubieran producido muertes entre los trabajadores de un taller y de una destiler√≠a pr√≥xima a la zona se aclar√≥ en base a que dispon√≠an de pozos de agua privados. Adem√°s, tambi√©n se esclarecieron ciertos casos de muertes lejanas al √°rea de dicha bomba de agua y que se debieron a casos de personas que casualmente bebieron de esa fuente debido a visitas por la zona por motivos laborales o familiares. Finalmente, y gracias a que adem√°s se detect√≥ la causa de la epidemia (la contaminaci√≥n de las aguas de dicha fuente por la filtraci√≥n de los pa√±ales de un bebe que se hab√≠an tirado a un pozo negro d√≠as antes del inicio de la epidemia), John Snow consigui√≥ que las autoridades quitasen la manivela a la bomba de agua y con ello comenzaron a disminuir los casos de la enfermedad. Hoy en d√≠a en el actual barrio del Soho se conservan varias referencias al doctor John Snow, en la famosa Broad Street renombrada hoy como Broadwick Street, desde un pub con su nombre donde se reune la The John Snow Society, una escultura de la famosa bomba de agua, o incluso una peque√±a placa en el edificio donde estaba dicha bomba. A continuaci√≥n, y para m√°s informaci√≥n al respecto, adjunto el enlace a la charla TED \"The Ghost Map\" de Steven Johnson sobre el impacto que tuvo John Snow y su mapa del c√≥lera en la ciencia, las ciudades y las sociedades modernas. Fuentes de inter√©s TED Talks - Steven Johnson - The Ghost Map Wikipedia - John Snow Victorian Web - John Snow Master Telef√≥nica en Big Data & Business Analytics All3dform - John Snow y el mapa que salv√≥ Londres psanxiao - El mapa del c√≥lera de John Snow",
    "url": "https://datalaria.com/es/posts/john-snow/",
    "slug": "john-snow",
    "lang": "es",
    "categories": [
      "casos_exito"
    ],
    "tags": [
      "colera",
      "john snow",
      "mapa",
      "gis",
      "geolocalizaci√≥n",
      "Londres"
    ],
    "date": "2025-08-26",
    "timestamp": 0,
    "domain": "General",
    "image": "John_Snow.png"
  },
  {
    "objectID": "es_kilometro-0-por-que-nace-datalaria",
    "title": "Kil√≥metro 0: Por qu√© nace Datalaria",
    "description": "",
    "content": "¬°Bienvenido a Datalaria! Este es el primer post, el punto de partida de un proyecto que nace de una doble necesidad: la profesional y la personal. Profesionalmente, me muevo en un entorno industrial y tecnol√≥gico de alta exigencia. Veo cada d√≠a c√≥mo los datos y la inteligencia artificial est√°n dejando de ser conceptos abstractos para convertirse en herramientas cr√≠ticas. Este blog es mi laboratorio para experimentar con esas herramientas, para entenderlas en profundidad y, con suerte, para aplicar ese conocimiento en mi carrera. Personalmente, soy un curioso por naturaleza. Este es mi cuaderno de bit√°cora p√∫blico, mi compromiso para aprender de forma estructurada y compartir el viaje. Aqu√≠ documentar√© mis experimentos, mis errores y mis descubrimientos. Gracias por acompa√±arme en este kil√≥metro 0.",
    "url": "https://datalaria.com/es/posts/kilometro-0-por-que-nace-datalaria/",
    "slug": "kilometro-0-por-que-nace-datalaria",
    "lang": "es",
    "categories": [],
    "tags": [
      "Meta",
      "Introducci√≥n"
    ],
    "date": "2025-08-06",
    "timestamp": 1754487000,
    "domain": "General",
    "image": ""
  },
  {
    "objectID": "es_multiverse_computing",
    "title": "Multiverse Computing: La Startup Espa√±ola que Resuelve el Cuello de Botella de la IA con F√≠sica Cu√°ntica (Sin Ordenadores Cu√°nticos)",
    "description": "An√°lisis estrat√©gico de Multiverse Computing, la startup espa√±ola que ha levantado $326M aplicando matem√°ticas de f√≠sica cu√°ntica (Redes de Tensores) para comprimir modelos de IA en hardware cl√°sico, resolviendo el problema cr√≠tico del coste y la eficiencia de los LLMs.",
    "content": "En la carrera tecnol√≥gica actual, dos narrativas dominan los titulares: la revoluci√≥n de la Inteligencia Artificial Generativa y la promesa futura de la Computaci√≥n Cu√°ntica. Sin embargo, ambas enfrentan barreras significativas. Los modelos de IA (LLMs) son cada vez m√°s grandes, costosos y energ√©ticamente insostenibles. Por otro lado, los ordenadores cu√°nticos √∫tiles y tolerantes a fallos siguen siendo una promesa a a√±os de distancia. En la intersecci√≥n de estos dos desaf√≠os emerge Multiverse Computing , una startup fundada en San Sebasti√°n que ha encontrado una soluci√≥n ingeniosa y pragm√°tica. En lugar de esperar al hardware cu√°ntico del futuro, est√°n utilizando las herramientas matem√°ticas de la f√≠sica cu√°ntica hoy para resolver los problemas m√°s acuciantes de la IA en ordenadores cl√°sicos. Este enfoque, conocido como \"computaci√≥n inspirada en cu√°ntica\", les ha permitido cerrar recientemente una ronda de inversi√≥n Serie B de 215 millones de d√≥lares , alcanzando una valoraci√≥n de 500 millones y consolid√°ndose como una de las empresas de deep tech m√°s prometedoras de Europa. El Problema: La Crisis de Escalabilidad de la IA El despliegue de modelos como GPT-4 o Llama-3 est√° chocando con una pared de realidad: el coste de inferencia. Ejecutar estos modelos requiere una potencia de c√≥mputo y una energ√≠a inmensas, lo que limita su adopci√≥n y sostenibilidad. Las t√©cnicas tradicionales de compresi√≥n, como la \"cuantizaci√≥n\" (reducir la precisi√≥n de los n√∫meros) o el \"pruning\" (eliminar neuronas menos importantes), tienen l√≠mites antes de degradar significativamente la calidad del modelo. La industria necesita un cambio de paradigma para hacer que la IA sea econ√≥micamente viable a escala. La Soluci√≥n: Redes de Tensores, el \"Truco\" Matem√°tico Aqu√≠ es donde Multiverse Computing brilla. Su innovaci√≥n central no es el hardware, sino la aplicaci√≥n industrial de una estructura matem√°tica compleja llamada Redes de Tensores (Tensor Networks) . Originalmente desarrolladas en f√≠sica de la materia condensada para simular estados cu√°nticos complejos, las Redes de Tensores permiten descomponer problemas de dimensionalidad gigantesca en piezas manejables. Imagina un modelo de IA como una red inmensamente compleja de conexiones. Las Redes de Tensores permiten identificar y retener solo las correlaciones esenciales que contribuyen a la precisi√≥n, descartando el ruido y las conexiones redundantes. La ventaja clave: Esto se puede ejecutar en GPUs y CPUs actuales, emulando la eficiencia cu√°ntica sin necesitar un ordenador cu√°ntico. CompactifAI: Comprimiendo el Futuro de la IA El producto estrella que ha catapultado su valoraci√≥n es CompactifAI . A diferencia de otros m√©todos, CompactifAI reestructura fundamentalmente las capas de la red neuronal utilizando Redes de Tensores. Los resultados reportados son impresionantes: Tasa de Compresi√≥n: Reducci√≥n del tama√±o del modelo hasta en un 95% . Retenci√≥n de Precisi√≥n: Mantiene la exactitud del modelo original con una p√©rdida marginal (2-3%). Velocidad de Inferencia: Aceleraci√≥n entre 4x y 12x . Ahorro de Costes: Reducci√≥n de costes de inferencia entre un 50% y un 80% . M√°s all√° del ahorro en la nube, esta tecnolog√≠a es un habilitador cr√≠tico para la Edge AI . Permite ejecutar modelos potentes directamente en dispositivos con recursos limitados como smartphones, coches aut√≥nomos o drones, sin depender de la conexi√≥n a la nube, mejorando la latencia y la privacidad. Estrategia de Negocio: Pragmatismo Cu√°ntico Desde su fundaci√≥n en 2019, Multiverse adopt√≥ una filosof√≠a de \"Quantum Computing Basado en Valor\". Se alejaron de la especulaci√≥n y se centraron en generar ROI inmediato para sus clientes. Su estrategia se basa en varios pilares: Agnosticismo de Hardware: Su plataforma de software, Singularity , funciona tanto en los procesadores cu√°nticos actuales (de IBM, D-Wave, etc.) como en hardware cl√°sico. Esto mitiga el riesgo de apostar por una tecnolog√≠a de hardware inmadura. Foco en la Inferencia: Han pivotado estrat√©gicamente para abordar el mercado de inferencia de IA, proyectado en m√°s de $100 mil millones, donde el dolor del cliente (coste y eficiencia) es agudo y actual. Alianzas Estrat√©gicas Profundas: No solo firman acuerdos de marketing. Tienen colaboraciones t√©cnicas profundas que validan su tecnolog√≠a en entornos cr√≠ticos: Finanzas: Con el Banco de Canad√° , simulando la adopci√≥n de criptomonedas en redes econ√≥micas complejas intratables para la computaci√≥n cl√°sica. Energ√≠a: Con Iberdrola , optimizando la ubicaci√≥n de bater√≠as en la red el√©ctrica para integrar renovables, superando a los benchmarks cl√°sicos. Industria 4.0: Con Bosch , integrando algoritmos cu√°nticos en gemelos digitales para la detecci√≥n de defectos. Consultor√≠a: Con PwC Espa√±a , que integra CompactifAI en su oferta de servicios, actuando como cliente y canal de distribuci√≥n. Conclusi√≥n: Un Puente hacia la Era Cu√°ntica Multiverse Computing ha logrado lo que pocas startups de deep tech consiguen: navegar el \"Valle de",
    "url": "https://datalaria.com/es/posts/multiverse_computing/",
    "slug": "multiverse_computing",
    "lang": "es",
    "categories": [
      "Inteligencia Artificial",
      "Deep Tech",
      "Estrategia Empresarial",
      "Computaci√≥n Cu√°ntica"
    ],
    "tags": [
      "Multiverse Computing",
      "IA Generativa",
      "Compresi√≥n de Modelos",
      "Redes de Tensores",
      "Quantum-Inspired",
      "LLM",
      "Startup",
      "Inversi√≥n"
    ],
    "date": "2026-01-31",
    "timestamp": 0,
    "domain": "General",
    "image": "cover.png"
  },
  {
    "objectID": "es_nanobanana-ingenieria",
    "title": "De la Ingenier√≠a de Sistemas a la F√≥rmula 1: La IA (Nanobana) para ilustrar un Roadmap Complejo",
    "description": "Un caso pr√°ctico de c√≥mo utilic√© la herramienta de IA Nanobanana para transformar el Ciclo en V de la ingenier√≠a de sistemas en una historia visual comprensible usando una analog√≠a con la F√≥rmula 1.",
    "content": "En el mundo de la ingenier√≠a, uno de nuestros mayores desaf√≠os no es resolver el problema t√©cnico, sino comunicar la soluci√≥n . A menudo, nos encontramos frente a directivos, clientes u otros departamentos intentando explicar procesos complejos con diagramas √°ridos o documentos de cientos de p√°ginas y poco digeribles. El resultado suele ser miradas perdidas y una desconexi√≥n entre el equipo t√©cnico y los stakeholders que conduce a una gran confusi√≥n entre todas las partes. Recientemente, me enfrent√© a este mismo reto: necesitaba presentar un estado de situaci√≥n y progreso para la implantaci√≥n de un proceso complejo de ingenier√≠a cuyo roadmap se asimilaba en gran medida al tradicional Ciclo en V de la ingenier√≠a de sistemas . En lugar de recurrir al diagrama de siempre, decid√≠ experimentar y buscar una analog√≠a m√°s atractiva. ¬øY si pudiera usar la IA no solo para analizar datos, sino para contar una historia? Este post es la cr√≥nica y resultado ( Plantilla descargable ) de c√≥mo utilic√© la herramienta de IA de Google Nanobanana para transformar un roadmap de ingenier√≠a en una presentaci√≥n visual y atractiva, usando una analog√≠a que casi todo el mundo entiende y admira: el dise√±o de un monoplaza de F√≥rmula 1 . El Desaf√≠o: Hacer Entendible el Ciclo en V Para quien no lo conozca, el Ciclo en V es un modelo de desarrollo que representa la secuencia de las fases del ciclo de vida de un dise√±o o desarrollo de un producto. Su gran virtud es que conecta cada fase de dise√±o y desarrollo con su correspondiente fase de pruebas, asegurando que todo lo que se especifica, se verifica. Suena l√≥gico, pero su representaci√≥n gr√°fica aunque est√°ndar puede ser abstracta y poco inspiradora para aquellos que no est√°n familiarizados con ello. En mi caso, para el simil del proceso a exponer utilic√© una variante de diagrama en V b√°sico como el ilustrado a continuaci√≥n en lugar del diagrama en V tradicional en dise√±os Hardware o Software con un nivel m√°s de profundidad: El diagrama en V me facilitar√≠a que el roadmap de la iniciativa a presentar se entendiera mejor, pero para que el concepto calara y quedara una imagen del mismo en la retina de cada stakeholder, necesitaba una met√°fora potente. Y pocas cosas combinan dise√±o, pruebas y un objetivo claro de rendimiento como la creaci√≥n de un coche de F1. La Herramienta: Nanobanana al Rescate Para materializar esta idea, recurr√≠ a Nanobanana , una herramienta de IA especializada en la generaci√≥n de im√°genes y visuales con un estilo limpio y consistente. Su punto fuerte es la capacidad de creaci√≥n im√°genes fieles al contexto solicitado y su capacidad de mantener la coherencia en las diferentes interacciones realizadas, como en mi caso donde buscaba crear el flujo evolutivo de las im√°genes en el dise√±o siguiendo un hilo conductor a la met√°fora en vez de ir utilizando im√°genes sueltas inconexas. El Roadmap Visual: Dise√±ando un F1 Fase a Fase El plan era sencillo: para cada fase del Ciclo en V, generar√≠a una imagen que representara la etapa equivalente en el dise√±o de un monoplaza. A continuaci√≥n vemos el detalle de cada fase: Fase 1: Gesti√≥n de Requisitos En Ingenier√≠a : Es el punto de partida. Aqu√≠ se definen y documentan las necesidades del cliente y las restricciones del sistema. ¬øQu√© debe hacer el producto? La Analog√≠a F1 : Es la reuni√≥n inicial del equipo. Los ingenieros estudian el estricto reglamento de la FIA (las restricciones), analizan los datos de la temporada anterior y escuchan las peticiones del piloto (el cliente). El objetivo es claro: construir un coche ganador. La Petici√≥n a Nanobanana : \"Crea una imagen ilustrativa de un par de ingenieros de F1 en una sala de reuniones, analizando un reglamento t√©cnico y dibujando conceptos iniciales en una pizarra.\" Fase 2: Dise√±o Preliminar (Arquitectura del Sistema) En Ingenier√≠a : Se define la arquitectura de alto nivel. ¬øC√≥mo se estructurar√° el sistema? ¬øCu√°les son sus componentes principales y c√≥mo interact√∫an? La Analog√≠a F1 : Los ingenieros dise√±an el concepto general del coche: la distribuci√≥n de pesos, la filosof√≠a aerodin√°mica, la integraci√≥n del motor con el chasis. Es el esqueleto conceptual. La Petici√≥n a Nanobanana : \"Mant√©n una escena similar a la anterior pero cambiando la pizarra del fondo por una pantalla donde se est√° proyectando un modelo 3D conceptual de un coche de F1, mostrando sus sistemas principales (motor, chasis, aerodin√°mica) interconectados.\" Fase 3: Dise√±o Detallado En Ingenier√≠a : Se dise√±an en detalle los componentes individuales del sistema. La Analog√≠a F1 : El momento de la ultra-especializaci√≥n. El equipo de aerodin√°mica dise√±a cada mil√≠metro del aler√≥n delantero; el de suspensiones, cada pieza de titanio. Son los planos detallados de cada componente. La Petici√≥n a Nanobanana : \"Sobre este mismo escenario crea una imagen en la que un ingeniero est√° trabajando en un dise√±o CAD o plano detallado del veh√≠culo de F1 visualiz√°ndolo en la proyecci√≥n del fondo.\" Fase 4: Implementaci√≥n En Ingenier√≠a : La fase ",
    "url": "https://datalaria.com/es/posts/nanobanana-ingenieria/",
    "slug": "nanobanana-ingenieria",
    "lang": "es",
    "categories": [
      "Herramientas",
      "Ingenier√≠a"
    ],
    "tags": [
      "ia",
      "nanobanana",
      "ingenier√≠a de sistemas",
      "v-model",
      "visualizaci√≥n",
      "formula 1",
      "comunicaci√≥n",
      "ppt",
      "pptx",
      "comunicaci√≥n",
      "ingenier√≠a",
      "desarrollo",
      "roadmap"
    ],
    "date": "2025-09-20",
    "timestamp": 0,
    "domain": "Product",
    "image": "cover.png"
  },
  {
    "objectID": "es_netflix",
    "title": "Netflix: C√≥mo los Datos Forjan un Imperio en Plena Guerra del Streaming",
    "description": "Un an√°lisis de Netflix como empresa data-driven que utiliza Big Data e IA no solo para recomendar series, sino para tomar decisiones estrat√©gicas clave y dominar en las 'Guerras del Streaming'.",
    "content": "En un mundo donde cada mes parece nacer una nueva plataforma de streaming con multitud de contenidos, la pregunta ya no es c√≥mo Netflix consigui√≥ revolucionar la televisi√≥n, sino c√≥mo logra mantenerse en el trono. La respuesta, aunque compleja en su ejecuci√≥n, es simple en su concepto: una cultura obsesiva y radicalmente centrada en los datos. Lejos han quedado los d√≠as en que Netflix era el √∫nico disruptor. Hoy, con casi 300 millones de suscriptores (datos de mediados de 2024) y un presupuesto de contenido que supera los 17.000 millones de d√≥lares anuales , la compa√±√≠a no solo crea series y pel√≠culas; libra una batalla diaria por cada minuto de nuestra atenci√≥n. Y en esta guerra, su arsenal no son solo los guiones, sino los algoritmos. El ADN de Netflix: Una Obsesi√≥n por el Dato Desde el DVD Para entender el Netflix de hoy, hay que viajar a sus or√≠genes. Fundada en 1997 como un servicio de alquiler de DVDs por correo, la compa√±√≠a mostr√≥ su ADN data-driven muy pronto. En el a√±o 2000, mucho antes de que \"Big Data\" fuera un t√©rmino de moda, ya desarrollaba un sistema de recomendaci√≥n basado en las puntuaciones de sus usuarios. Esta obsesi√≥n culmin√≥ en 2006 con el legendario \"Netflix Prize\" : un mill√≥n de d√≥lares para el equipo que mejorara en un 10% la precisi√≥n de su algoritmo de recomendaci√≥n. Aunque la soluci√≥n ganadora nunca se implement√≥ por su complejidad, el mensaje fue claro: Netflix estaba dispuesto a invertir masivamente para entender a sus usuarios a un nivel sin precedentes. La Evoluci√≥n de la Inteligencia: M√°s All√° de \"House of Cards\" La producci√≥n de House of Cards en 2011 fue un hito, una apuesta multimillonaria basada en datos. Fue un √©xito rotundo, pero la sofisticaci√≥n actual de Netflix hace que aquello parezca un juego de ni√±os. Personalizaci√≥n Extrema: El Arte de la Car√°tula Perfecta Uno de los ejemplos m√°s fascinantes del uso de datos en Netflix es la personalizaci√≥n de las miniaturas (thumbnails) . La car√°tula que ves t√∫ para Stranger Things no es la misma que veo yo. Netflix realiza pruebas A/B con docenas de im√°genes y, bas√°ndose en tu historial, te muestra aquella con la que tienes m√°s probabilidades de interactuar. ¬øVes muchas comedias rom√°nticas? Te mostrar√° una imagen de la pareja protagonista. ¬øEres fan de la acci√≥n? Ver√°s una miniatura con una explosi√≥n o una escena de persecuci√≥n. Es una aplicaci√≥n de la psicolog√≠a y la estad√≠stica a escala masiva, dise√±ada para ganar tu clic en menos de dos segundos. La An√©cdota de Adam Sandler: Ignorando a los Cr√≠ticos A mediados de la d√©cada de 2010, las pel√≠culas de Adam Sandler recib√≠an cr√≠ticas nefastas. Sin embargo, los datos de Netflix contaban otra historia: sus comedias m√°s antiguas ten√≠an una audiencia de visionado masiva y recurrente en la plataforma a nivel global. Bas√°ndose exclusivamente en estos datos de comportamiento y no en la opini√≥n de los cr√≠ticos, Netflix firm√≥ con Sandler un contrato multimillonario por varias pel√≠culas exclusivas. El resultado fue un √©xito rotundo de audiencia, demostrando que los datos de los usuarios eran un indicador de √©xito m√°s fiable que las rese√±as de Hollywood. El Bot√≥n que Ahorra 195 A√±os al D√≠a: \"Saltar Intro\" El famoso bot√≥n \"Saltar Intro\" no fue una idea creativa, fue una conclusi√≥n extra√≠da de los datos. El equipo de Netflix analiz√≥ el comportamiento de los usuarios y detect√≥ que un gran porcentaje saltaba manualmente los primeros minutos de los episodios. Al confirmar este patr√≥n en miles de series, desarrollaron la funcionalidad. Seg√∫n un ex-directivo, este bot√≥n se pulsa 136 millones de veces al d√≠a, ahorrando a los usuarios un tiempo colectivo de 195 a√±os... cada d√≠a . El Cazador de Fen√≥menos Globales: El Caso \"El Juego del Calamar\" Este es quiz√°s el mejor ejemplo del poder de su inteligencia global. El Juego del Calamar no fue un proyecto de Hollywood. Fue una serie surcoreana que los algoritmos de Netflix detectaron como un √©xito regional con una tasa de finalizaci√≥n alt√≠sima. Al identificar este patr√≥n, la plataforma la impuls√≥ a nivel mundial, convirti√©ndola en un fen√≥meno cultural y su serie m√°s vista de la historia. Los Datos como Arma en las \"Guerras del Streaming\" En el saturado mercado actual, la batalla es feroz. Para entender el contexto, basta con ver la distribuci√≥n del mercado en Estados Unidos, uno de los campos de batalla m√°s representativos. Seg√∫n datos de Statista para el primer trimestre de 2025, la cuota de mercado de las plataformas de streaming en EE.UU. se reparte de la siguiente manera: Amazon Prime Video : 22% Netflix : 21% Max : 13% Disney+ : 12% Hulu : 11% Otros : 21% Con este panorama, el objetivo principal de Netflix es reducir la tasa de cancelaci√≥n ( churn rate ). Para ello, ha tomado decisiones de negocio valientes y pol√©micas, todas ellas fundamentadas en datos. Y es que aunque en la cuota de mercado parece que en el mercado estadounidense Amazon Prime Video le ha dado el sorpasso , Netflix sigue siendo el rey en cuanto a suscriptores e ingresos con un mu",
    "url": "https://datalaria.com/es/posts/netflix/",
    "slug": "netflix",
    "lang": "es",
    "categories": [
      "casos_exito"
    ],
    "tags": [
      "netflix",
      "streaming",
      "big data",
      "ia",
      "disney",
      "churn",
      "algoritmos"
    ],
    "date": "2025-09-15",
    "timestamp": 0,
    "domain": "General",
    "image": "cover_twitter.jpg"
  },
  {
    "objectID": "es_notebooklm-sql",
    "title": "Aprendiendo con IA - NotebookLM: De un libro a una plataforma de aprendizaje multimedia",
    "description": "Mi experiencia pr√°ctica generando una plataforma de aprendizaje y estudio multimedia a partir de un libro sobre SQL mediante la herrmienta de IA de Google NotebookLM",
    "content": "En la actualidad, en la era de la informaci√≥n y el conocimiento, cada vez que queremos aprender algo nuevo, nos enfrentamos a un desaf√≠o colosal con monta√±as de informaci√≥n en bruto. Libros, papers acad√©micos, cualquier tipo de documento, p√°ginas webs, videos, podcasts... Extraer el conocimiento que no es realmente √∫til de todas estas fuentes es un trabajo arduo que consume nuestro recurso m√°s valioso: el tiempo. Sin perder de vista el valor que ofrece la navegaci√≥n a la deriva entre estos mares de informaci√≥n en s√≠, formando y consolidando los cimientos de cada tem√°tica o conocimiento, siempre he buscado herramientas que optimicen este proceso y me permitan apuntar a los conceptos o directrices clave durante todo esta navegaci√≥n y proceso. En este art√≠culo, voy a hablar de la herramienta m√°s revolucionaria que he conocido hasta la fecha y que va a cambiar por completo mi flujo de trabajo NotebookLM de Google . Para ilustrar en qu√© consiste y entender su potenciar, me he basado en un ejemplo pr√°ctico a partir del interesant√≠smo libro compartido en freecodeamp por Daniel Garc√≠a Solla How to Design Structured Database Systems Using SQL sobre el dise√±o de bases dede datos SQL el cual, con la ayuda de NotebookLM, lo transform√© en un completo arsenal de aprendizaje multimedia. ¬øQu√© es NotebookLM y por qu√© es una revoluci√≥n silenciosa? Imagina un asistente de investigaci√≥n personal que ha le√≠do y entendido perfectamente solo las fuentes (documentos, videos, podcasts...) que t√∫ le proporcionas . Esa es la esencia de NotebookLM. A diferencia de otras IAs que bucean en todo internet (independientemente de la fiabilidad de las fuentes), NotebookLM se basa exclusivamente en tus fuentes . Esta simple premisa es su superpoder. Al estar \"anclado\" a tus documentos, elimina las alucinaciones y las respuestas gen√©ricas . Su conocimiento es tu conocimiento, lo que lo convierte en un experto a medida sobre cualquier tema que le entregues (PDFs, Google Docs, URLs, etc.). Mi Campo de Batalla: Un Libro T√©cnico sobre SQL Para ponerlo a prueba, eleg√≠ una fuente especializada y detallada: el libro completo How to Design Structured Database Systems Using SQL compartido por Daniel Garc√≠a Solla en freecodeamp . Mi objetivo no era solo \"resumirlo\", sino deconstruirlo y reconstruirlo en formatos que se adaptaran a diferentes formas de aprendizaje que me fueran facilitando el proceso para la asimilaci√≥n de los diferentes conceptos. El primer paso fue tan simple como entrar en la herramienta NotebookLM y crear un nuevo \"cuaderno\" enlazando a la URL con el contenido del libro. En segundos, NotebookLM no solo lo hab√≠a procesado, sino que ya me ofrec√≠a un resumen inicial y preguntas clave. Pero esto era solo el comienzo... De la Informaci√≥n al Conocimiento: Mis 4 Creaciones A partir de ah√≠ es donde la magia empieza a ocurrir. Usando una serie de prompts espec√≠ficos ya predefinidos sobre la propia herramienta, lanc√© sobre NotebookLM la generaci√≥n de diferentes recursos de aprendizaje de alto valor: 1. El Mapa Mental: La Estructura en un Vistazo üó∫Ô∏è Para entender la arquitectura del libro, necesitaba una vista de p√°jaro. Y, para ello NotebookLM me presenta un bot√≥n muy aparente llamado Mapa Mental . Presion√°ndolo, y como por arte de magia... Una estructura perfectamente organizada que me permiti√≥ navegar por los conceptos del libro, desde el modelo relacional hasta las consultas complejas, entendiendo c√≥mo cada pieza encajaba en el todo. Ideal para estructurar el estudio. 2. El Resumen para V√≠deo: El Guion Did√°ctico üé¨ Siguiente truco, ¬øser√≠a capaz de generar un recurso visual como un video explicativo de todo este contenido? La idea ser√≠a generar un video breve introductorio a los principales conceptos recogidos en el libro que me permitiera ver el bosque antes de adentrarme √°rbol a √°rbol. Y, con este fin, NotebookLM nos ofrece un bot√≥n con la funcionalidad de Resumen Video a partir de la cual se pone la gorra de editor de video y en unos minutos nos genera un impresionante video resumen con parte de los aspectos clave tratados en el libro y expuestos de manera clara y did√°ctica en un formato narrativo que mantiene la atenci√≥n del espectador. Resumen de video libro \"How to Design Structured Database Systems Using SQL\" : Resumen de video generado por NotebookLM 3. El Resumen de Audio: El Podcast para Aprender en Movimiento üéß Asombrado todav√≠a por estas capacidades de NotebookLM y queriendo poder compaginar los conceptos del libro mientras viajo o hago deporte, lanc√© la funcionalidad de Resumen de audio a partir de la cual, y como por arte de magia nuevamente, se genera un podcast de audio natural entre 2 personas f√°cil de seguir y donde lo m√°s sorprendente es como la IA de la herramienta es capaz de adaptar el tono, el formato o los contenidos de una manera muy natural y amena para enganchar al espectador. Resumen de audio libro \"How to Design Structured Database Systems Using SQL\" : Resumen de audio generado por NotebookLM 4. El Informe de Gu√≠a",
    "url": "https://datalaria.com/es/posts/notebooklm-sql/",
    "slug": "notebooklm-sql",
    "lang": "es",
    "categories": [
      "Herramientas"
    ],
    "tags": [
      "ia",
      "notebooklm",
      "google",
      "aprendizaje",
      "productividad",
      "sql",
      "mapa mental"
    ],
    "date": "2025-08-31",
    "timestamp": 0,
    "domain": "Product",
    "image": "notebooklm_hero.png"
  },
  {
    "objectID": "es_onboarding",
    "title": "De los '24 D√≠as' al √âxito Inmediato: Revolucionando el Onboarding con Inteligencia Artificial",
    "description": "El proceso de onboarding tradicional est√° roto, con un tiempo promedio de 24 d√≠as y un coste de 4.000‚Ç¨ por contrataci√≥n. Este post explora c√≥mo la IA Generativa y el Procesamiento Inteligente de Documentos (IDP) transforman la 'infoxicaci√≥n' burocr√°tica en una integraci√≥n fluida, ofreciendo un mentor virtual 24/7 que puede mejorar la retenci√≥n en un 82%.",
    "content": "¬øRecuerdas tu primer d√≠a en tu √∫ltimo trabajo? Probablemente, la emoci√≥n se mezcl√≥ r√°pidamente con una monta√±a de formularios, muchos documentos en diferentes rutas inaccesibles, manuales desactualizados o incompletos, herramientas de trabajo no disponibles... o incluso falta de equipo y sitio en la oficina? Si esto te suena familiar, no est√°s solo. No soy un experto en RR.HH., pero me ha tocado lidiar para evitar todos problemas con el onboarding de nuevos talentos, ante un proceso el cual actualmente es doloroso, caro y, francamente, ineficiente. El proceso de incorporaci√≥n tradicional est√° roto. Pero existe una nueva frontera donde la Inteligencia Artificial Generativa (GenAI) y el Procesamiento Inteligente de Documentos (IDP) no solo est√°n agilizando el papeleo, sino que est√°n redefiniendo la experiencia humana de unirse a una empresa. Bas√°ndonos en un an√°lisis reciente sobre la gesti√≥n documental e informativa en el onboarding, exploramos c√≥mo transformar la \"infoxicaci√≥n\" en integraci√≥n efectiva. La Cruda Realidad del Onboarding Tradicional: Cifras que Duelen El panorama actual sobre la incorporaci√≥n de nuevos talentos es preocupante en las grandes empresas tecnol√≥gicas. A pesar de la inversi√≥n masiva en reclutamiento, la fase cr√≠tica de \"aterrizaje\" sigue siendo un cuello de botella costoso, al margen de que adem√°s, apaga la chispa de la motivaci√≥n y limita el potencial de los nuevos trabajadores. Los datos son contundentes: El coste del tiempo: El tiempo promedio para completar un proceso de onboarding es de 24 d√≠as . Eso es casi un mes de productividad reducida. El coste financiero: El coste promedio por cada nueva contrataci√≥n ronda los 4.000‚Ç¨ . El enfoque equivocado: El 58% de las organizaciones admite que su onboarding se centra principalmente en el papeleo y los procesos, en lugar de en la cultura o el desarrollo. La desconexi√≥n: Quiz√°s el dato m√°s alarmante es que solo el 12% de los empleados cree que su empresa hace un buen trabajo en el onboarding. La consecuencia de esta mala gesti√≥n es clara: el 31% de los trabajadores ha abandonado un empleo en los primeros seis meses. Estamos perdiendo talento casi tan r√°pido como lo contratamos. El Problema: \"Beber de una Manguera de Incendios\" Hay dos puntos de dolor cr√≠ticos que saturan al nuevo empleado: La Carga Documental (Burocracia): Son procesos repetitivos, manuales y propensos a errores humanos que consumen el tiempo de los equipos de RR.HH. La Sobrecarga Informativa (\"Infoxicaci√≥n\"): El nuevo empleado recibe de golpe pol√≠ticas de empresa, normativas, gu√≠as de herramientas y cultura corporativa. Es imposible retener tanta informaci√≥n de golpe. El resultado es un empleado abrumado que pasa sus primeras semanas navegando por la burocracia en lugar de aportar valor. La Soluci√≥n: Un Mentor 24/7 Impulsado por IA La revoluci√≥n en el onboarding no se limita a digitalizar documentos existentes; se trata de redefinir fundamentalmente c√≥mo interact√∫an los nuevos empleados con la empresa desde el primer momento. La propuesta clave es implementar la IA como un \"copiloto\" o mentor virtual, una gu√≠a constante que acompa√±a al empleado desde el d√≠a cero, facilitando su integraci√≥n y acelerando su productividad. Esta transformaci√≥n se logra mediante la combinaci√≥n estrat√©gica de dos tecnolog√≠as poderosas: IDP (Procesamiento Inteligente de Documentos): Capaz de extraer, entender y procesar informaci√≥n de documentos estructurados y semiestructurados. GenAI (Inteligencia Artificial Generativa): Utiliza Modelos de Lenguaje Grande (LLMs) para comprender el lenguaje natural, el contexto y generar respuestas humanas. Esta dupla tecnol√≥gica aborda los dos principales desaf√≠os del onboarding tradicional: 1. Automatizaci√≥n de la Burocracia (Gesti√≥n Documental) El proceso tradicional de recopilaci√≥n y verificaci√≥n de documentaci√≥n, solicitudes de equipos, accesos y permisos suele ser un cuello de botella que involucra a m√∫ltiples actores (el propio empleado, su responsable, RR.HH., IT, etc.), consumiendo tiempo valioso y generando fricciones. La IA transforma este proceso manual y tedioso en un flujo de trabajo √°gil y automatizado: Extracci√≥n y Validaci√≥n Autom√°tica de Datos: Los documentos del empleado (DNI, pasaporte, t√≠tulos, etc.) se suben a una plataforma segura. El IDP extrae autom√°ticamente los datos relevantes, verifica su autenticidad y los compara con la informaci√≥n existente, se√±alando cualquier discrepancia para su revisi√≥n. Herramientas: Plataformas como DocuSign CLM , Abbyy FlexiCapture o Kofax utilizan IDP para agilizar la gesti√≥n documental. Generaci√≥n Autom√°tica de Formularios y Contratos: Bas√°ndose en la informaci√≥n extra√≠da y los datos del puesto, la IA puede rellenar autom√°ticamente formularios fiscales, contratos laborales, acuerdos de confidencialidad y otros documentos legales, listos para la firma digital. Orquestaci√≥n de Solicitudes (IT, Facilities, etc.): La IA puede iniciar autom√°ticamente flujos de trabajo para solicitar el equipo inform√°tico ",
    "url": "https://datalaria.com/es/posts/onboarding/",
    "slug": "onboarding",
    "lang": "es",
    "categories": [
      "Inteligencia Artificial",
      "Recursos Humanos",
      "Productividad"
    ],
    "tags": [
      "Onboarding",
      "GenAI",
      "IDP",
      "Experiencia del Empleado",
      "Retenci√≥n de Talento",
      "Gesti√≥n Documental"
    ],
    "date": "2026-01-17",
    "timestamp": 0,
    "domain": "People",
    "image": "cover.png"
  },
  {
    "objectID": "es_reuniones_IA",
    "title": "Reuniones Excelentes en la Era de la IA: El Dec√°logo Definitivo",
    "description": "Un dec√°logo de 10 pasos pr√°cticos para transformar tus reuniones, haci√©ndolas m√°s cortas, efectivas y √°giles, con un enfoque especial en c√≥mo la IA puede ser tu mejor aliada.",
    "content": "Si hay un elemento universal en el mundo profesional que genera tanto amor como odio, son las reuniones. Cuando son efectivas, pueden alinear equipos, desbloquear ideas y acelerar proyectos. Pero cuando no lo son, se convierten en agujeros negros de tiempo y energ√≠a que devoran nuestra productividad. En una era donde el trabajo remoto e h√≠brido es la norma y las herramientas de inteligencia artificial est√°n redefiniendo nuestras capacidades, dominar el arte de la reuni√≥n eficaz ya no es una habilidad deseable, es una necesidad estrat√©gica. ¬øLa buena noticia? No se trata de magia, sino de m√©todo. A continuaci√≥n, presento un dec√°logo de 10 pasos pr√°cticos para transformar nuestras reuniones, con un enfoque especial en c√≥mo la tecnolog√≠a, y en particular la IA, puede convertirse en nuestro mejor copiloto para lograrlo. El Dec√°logo para Reuniones Excelentes 1. Reuniones Solo para lo Esencial La primera regla de un buen sistema de reuniones es... tener menos reuniones. Antes de convocar, preg√∫ntate: ¬øEs realmente necesario? ¬øSe puede resolver con un correo, un documento compartido o un mensaje r√°pido? Reserva el tiempo s√≠ncrono para debates, toma de decisiones complejas o sesiones de brainstorming que realmente requieran interacci√≥n en tiempo real. 2. Convoca a las Personas Adecuadas (y Respeta su Agenda) Una reuni√≥n con demasiados asistentes es una receta para la ineficiencia. Invita √∫nicamente a quienes son esenciales para la toma de decisiones o cuya contribuci√≥n es indispensable. Antes de fijar una hora, utiliza las herramientas de calendario para comprobar la disponibilidad de los asistentes clave y evitar conflictos innecesarios. 3. Define Objetivos y una Agenda Clara (con 24h de Antelaci√≥n) Una reuni√≥n sin agenda es como un barco sin tim√≥n. La convocatoria debe incluir siempre el objetivo principal, la agenda detallada y la documentaci√≥n previa. Enviar esto al menos 24 horas antes permite que los asistentes lleguen preparados y la reuni√≥n sea mucho m√°s productiva. 4. Empieza y Termina a la Hora (y si Puedes, Acorta) La puntualidad es una muestra de respeto por el tiempo de todos. Comienza a la hora fijada, incluso si no han llegado todos, y termina a la hora prevista. Un truco profesional es programar reuniones de 25 o 50 minutos en lugar de 30 o 60, para dejar un peque√±o margen entre compromisos. 5. Sin Interrupciones: Foco Absoluto Para que una reuni√≥n sea eficaz, requiere la atenci√≥n plena de sus participantes. Establece como norma silenciar los tel√©fonos m√≥viles y cerrar las aplicaciones de mensajer√≠a (Slack, Teams, etc.). El coste de la multitarea y las distracciones es mucho mayor de lo que pensamos. 6. Aporta Ideas y Fomenta la Participaci√≥n Activa Una reuni√≥n no es una conferencia. El moderador debe asegurarse de que todas las voces sean escuchadas. Fomenta un ambiente de seguridad psicol√≥gica donde se puedan debatir ideas, no personas. La escucha activa es fundamental. 7. Resume Conclusiones y Acciones al Finalizar Los √∫ltimos 5 minutos de la reuni√≥n son los m√°s importantes. Ded√≠calos a resumir verbalmente las decisiones clave, las acciones concretas, los responsables y las fechas l√≠mite. Esto asegura que todos salgan de la sala con la misma informaci√≥n. 8. Evita las \"Zonas Muertas\" del Calendario El momento en que se programa una reuni√≥n afecta directamente a la energ√≠a de los asistentes. Evita, siempre que sea posible, las reuniones a √∫ltima hora de la tarde o a primera hora de la ma√±ana. Las franjas a media ma√±ana suelen ser las m√°s productivas. 9. Establece un \"D√≠a Sin Reuniones\" Una pr√°ctica cada vez m√°s extendida y tremendamente eficaz. Consens√∫a con tu equipo un d√≠a a la semana o al mes en el que no se programen reuniones internas. Esto libera un bloque de tiempo ininterrumpido para el \"trabajo profundo\" ( deep work ). 10. Usa la Tecnolog√≠a (y la IA) como tu Copiloto Definitivo La tecnolog√≠a ya no es solo para conectar a personas en remoto, sino para hacer que todo el ciclo de vida de una reuni√≥n sea m√°s inteligente y √°gil. Aqu√≠ te presento un arsenal de herramientas y casos pr√°cticos. Antes de la Reuni√≥n: Planificaci√≥n Inteligente El √©xito de una reuni√≥n empieza mucho antes de que se inicie. Agendado Autom√°tico : Olv√≠date del interminable cruce de correos para encontrar un hueco. Herramientas como Clockwise o Motion analizan los calendarios de todo el equipo, encuentran el mejor momento para todos respetando sus bloques de trabajo profundo, e incluso reorganizan reuniones de forma inteligente si surge un conflicto. Creaci√≥n de Agendas con IA : En lugar de empezar de cero, puedes pedirle a un asistente de IA que te ayude. Caso Pr√°ctico (Prompt para Gemini/ChatGPT) : \"Genera una agenda detallada para una reuni√≥n de 60 minutos. El objetivo es decidir la estrategia de marketing para el lanzamiento del producto 'X'. Los asistentes son el Director de Marketing, un especialista en redes sociales y un analista de datos. Incluye tiempos para cada punto y preguntas clave a discutir.\" Durante la Reu",
    "url": "https://datalaria.com/es/posts/reuniones_IA/",
    "slug": "reuniones_IA",
    "lang": "es",
    "categories": [
      "Productividad",
      "Herramientas"
    ],
    "tags": [
      "ia",
      "reuniones",
      "productividad",
      "trabajo remoto",
      "prompt engineering",
      "buenas practicas"
    ],
    "date": "2025-11-21",
    "timestamp": 0,
    "domain": "Product",
    "image": "cover.png"
  },
  {
    "objectID": "es_sop_ingenieria-higiene-datos",
    "title": "S&OP: Por qu√© tu Excel te miente (y c√≥mo interrogarlo con Python)",
    "description": "Dejemos de limpiar datos a mano. En este primer cap√≠tulo de la serie Ingenier√≠a de S&OP, automatizamos la higiene de datos usando Python, Supabase y Estad√≠stica para detectar la verdad oculta tras el ruido.",
    "content": "En las reuniones de S&OP (Sales & Operations Planning), a menudo se discute sobre opiniones en lugar de hechos. \"Creo que venderemos m√°s\" , \"El mes pasado fue raro\" . El problema ra√≠z no es la falta de visi√≥n comercial, es la falta de integridad en la se√±al . La mayor√≠a de las cadenas de suministro se gestionan sobre hojas de c√°lculo que aceptan cualquier cosa: fechas como texto, espacios en blanco, y errores de dedo que convierten un pedido de 100 unidades en 100.000. Cuando alimentas tu algoritmo de predicci√≥n con esa \"basura\", obtienes basura amplificada (El efecto Bullwhip financiero). Hoy iniciamos la serie Ingenier√≠a del S&OP . No vamos a hablar de teor√≠a; vamos a construir una arquitectura de datos que audite tu negocio autom√°ticamente. El Problema: Signal-to-Noise Ratio En telecomunicaciones (mi background original), el ruido es cualquier interferencia que corrompe la se√±al. En Supply Chain, el \"ruido\" son los datos sucios. Si no filtras el ruido antes de planificar la demanda, est√°s inmovilizando capital . Un outlier no detectado es dinero en llamas. Si tu algoritmo ve un pico falso de 100.000 unidades, ordenar√° materia prima que no necesitas, quemando caja y ocupando espacio en almac√©n. La higiene de datos no es 'limpieza', es protecci√≥n del margen operativo. La Evidencia Visual Antes de ver una sola l√≠nea de c√≥digo, mira la diferencia entre lo que tu ERP exporta (arriba) y la realidad estad√≠stica de tu demanda (abajo). Arriba: Datos crudos con errores humanos. Abajo: La se√±al limpia lista para algoritmos de IA. La Soluci√≥n: Arquitectura de \"V√°lvula de Calidad\" Para solucionar esto, aplicamos First Principles Thinking . No necesitamos \"tener m√°s cuidado\" con el Excel. Necesitamos un sistema que matem√°ticamente proh√≠ba la entrada de datos sucios a nuestra \"Verdad √önica\". Hemos dise√±ado un pipeline automatizado con el siguiente stack: Cerebro: Python (Pandas + Scipy) para la l√≥gica estad√≠stica. Almac√©n: Supabase (PostgreSQL) como la \"Verdad √önica\". Agente: Un script que se ejecuta autom√°ticamente ante nuevos archivos. El C√≥digo: Estad√≠stica > Intuici√≥n No usamos reglas fijas (\"si es mayor que 1000, borra\"). Usamos estad√≠stica. Implementamos el Z-Score , que mide cu√°ntas desviaciones est√°ndar se aleja un dato de la media. Si una venta tiene un Z-Score > 3 (est√° a m√°s de 3 sigmas de la normalidad), es matem√°ticamente improbable que sea comportamiento est√°ndar. El sistema no lo borra (podr√≠a ser una venta real), pero lo marca para auditor√≠a y lo excluye de la predicci√≥n autom√°tica. Nota : Usamos Z-Score asumiendo normalidad para simplificar este ejemplo. En escenarios de producci√≥n con demanda intermitente, utilizamos m√©todos como IQR (Interquartile Range) o MAD (Median Absolute Deviation) que son m√°s robustos ante distribuciones no gaussianas. Aqu√≠ est√° la l√≥gica central de nuestra clase SupplyChainSanitizer : [code block] Open Kitchen: Pru√©balo t√∫ mismo Como ingeniero, desconf√≠o de lo que no puedo ejecutar. Por eso, he aislado la l√≥gica de limpieza en un Notebook interactivo en Colab . No necesitas instalar Python ni configurar bases de datos. He preparado un entorno ef√≠mero donde puedes: Generar un dataset de ventas corrupto (simulado). Ejecutar el motor de limpieza SupplyChainSanitizer . Ver c√≥mo el algoritmo detecta y separa el ruido. Haz clic en el bot√≥n, dale a \"Play\" en las celdas y observa la ingenier√≠a de datos en acci√≥n. Arquitectura de Producci√≥n (Behind the Scenes) Para los perfiles t√©cnicos interesados en c√≥mo esto escala en una empresa real (Datalaria Core): Ingesta: Los CSVs se suben a un Bucket privado en Supabase Storage o en una Base de datos local. Trigger: Un worker de Python detecta el archivo. Proceso: Ejecuta la limpieza en memoria (Docker Container). Persistencia: Los datos limpios se inyectan en PostgreSQL usando Row Level Security (RLS) para asegurar que nadie pueda alterar el hist√≥rico financiero manualmente. Nota de Seguridad: En producci√≥n, nunca conectamos scripts con permisos de superusuario. Usamos Service Roles espec√≠ficos y pol√≠ticas RLS estrictas para asegurar la integridad de la cadena de suministro. Visualizaci√≥n del Flujo de Datos El siguiente diagrama muestra c√≥mo los datos \"sucios\" pasan por nuestra V√°lvula de Calidad antes de llegar a la Verdad √önica: flowchart LR subgraph ORIGEN[\"üìÇ Origen\"] A[\"CSV del ERP (Datos Sucios)\"] end subgraph PIPELINE[\"üß† V√°lvula de Calidad (Python)\"] B[\"structural_clean()<br/>Fechas ¬∑ Nulos ¬∑ Duplicados\"] C[\"detect_outliers()<br/>Z-Score œÉ > 3\"] D[\"get_audit_report()<br/>M√©tricas de Higiene\"] end subgraph DESTINO[\"üóÑÔ∏è Verdad √önica\"] E[(\"Supabase<br/>PostgreSQL\")] F[\"Datos Limpios<br/>(Se√±al Pura)\"] G[\"Outliers Marcados<br/>(Para Auditor√≠a)\"] end A --> B --> C --> D D --> E E --> F E --> G style A fill:#ff6b6b,stroke:#c0392b,color:#fff style F fill:#2ecc71,stroke:#27ae60,color:#fff style G fill:#f39c12,stroke:#d35400,color:#fff style E fill:#3498db,stroke:#2980b9,color:#fff Leyenda: - üî¥ Rojo: Datos crudos con ruido (el problema) - üü¢ Ver",
    "url": "https://datalaria.com/es/posts/sop_ingenieria-higiene-datos/",
    "slug": "sop_ingenieria-higiene-datos",
    "lang": "es",
    "categories": [
      "Ingenier√≠a de S&OP",
      "Data Engineering",
      "Python"
    ],
    "tags": [
      "Supply Chain",
      "S&OP",
      "Supabase",
      "Pandas",
      "Z-Score"
    ],
    "date": "2026-02-14",
    "timestamp": 0,
    "domain": "S&OP",
    "image": "cover.png"
  },
  {
    "objectID": "es_the-goal",
    "title": "La Meta no es (solo) sobre f√°bricas: Sincronizando tu empresa en la era de la IA",
    "description": "Un an√°lisis profundo de 'La Meta' de Eli Goldratt y c√≥mo la Teor√≠a de Restricciones (TOC) se aplica m√°s all√° de la producci√≥n, a la ingenier√≠a, la gesti√≥n de proyectos y la cadena de suministro, potenciada por la IA y la Industria 4.0.",
    "content": "En 1984, Eliyahu M. Goldratt public√≥ \"La Meta\" , una novela de gesti√≥n que, disfrazada de historia sobre un gerente de f√°brica en apuros, inici√≥ una revoluci√≥n silenciosa. Muchos todav√≠a asocian este libro exclusivamente con la optimizaci√≥n de l√≠neas de producci√≥n. Sin embargo, su filosof√≠a subyacente, la Teor√≠a de Restricciones (TOC) , es un marco de pensamiento sist√©mico incre√≠blemente potente que se extiende mucho m√°s all√° de la manufactura. Hoy, en una era definida por la alta tecnolog√≠a, la gesti√≥n de proyectos de software, las cadenas de suministro globales y la explosi√≥n de la IA, los principios de \"La Meta\" son, parad√≥jicamente, m√°s relevantes que nunca. Este post explora c√≥mo la TOC se aplica a toda la empresa tecnol√≥gica moderna ‚Äîdesde la ingenier√≠a y las compras hasta la gesti√≥n de programas‚Äî y c√≥mo sirve de br√∫jula estrat√©gica para dirigir las potentes, pero costosas, herramientas de la Industria 4.0. El Dilema: \"Mundo del Coste\" vs. \"Mundo del Throughput\" El primer y m√°s grande obst√°culo que \"La Meta\" derriba es la contabilidad de costes tradicional. Goldratt argumenta que esta m√©trica es enga√±osa, ya que incentiva \"eficiencias locales\" que, a menudo, perjudican al sistema global. En un \"Mundo del Coste\" , un gerente de compras es premiado por encontrar un proveedor un 5% m√°s barato, y un jefe de producci√≥n es bonificado por mantener todas sus m√°quinas funcionando al 100% de eficiencia para \"absorber gastos generales\". La TOC demuestra que esta l√≥gica es fatalmente err√≥nea en un sistema interdependiente. Optimizar un recurso que no es un cuello de botella no mejora el rendimiento del sistema; de hecho, lo empeora, generando un exceso de inventario (I) que consume efectivo y aumenta los gastos operativos (OE). Para escapar de esta trampa, Goldratt redefine la meta de cualquier empresa comercial (\"ganar dinero ahora y en el futuro\") con tres m√©tricas operativas simples: Throughput (T): La velocidad a la que el sistema genera dinero a trav√©s de las ventas (ventas menos costes totalmente variables, como materias primas). Inventario (I): Todo el dinero invertido en cosas que se pretenden vender. La TOC lo trata como un pasivo, no un activo. Gastos Operativos (OE): Todo el dinero que el sistema gasta para convertir el Inventario en Throughput (costes fijos, salarios, etc.). El objetivo real de la empresa, por tanto, es: Aumentar el Throughput (T) mientras se reducen simult√°neamente el Inventario (I) y los Gastos Operativos (OE). Bajo esta nueva √≥ptica (el \"Mundo del Throughput\" ), la decisi√≥n de ese gerente de compras cambia. Si ese proveedor menos costoso es menos fiable y provoca una parada en la restricci√≥n del sistema, el Throughput perdido ser√° inmensamente mayor que el ahorro en el coste unitario. La Contabilidad del Throughput (la aplicaci√≥n financiera de la TOC) nos da el lenguaje financiero para priorizar la fiabilidad y la velocidad por encima del precio de compra en los puntos cr√≠ticos de nuestra cadena. El N√∫cleo de la TOC: Los 5 Pasos de Enfoque (POOGI) La TOC no es solo una teor√≠a, es un Proceso de Mejora Continua (POOGI) . El m√©todo para ejecutarlo se basa en 5 Pasos de Enfoque : IDENTIFICAR la Restricci√≥n: ¬øQu√© recurso, pol√≠tica o proceso dicta el ritmo de todo el sistema? No siempre es una m√°quina; puede ser un ingeniero senior sobrecargado, la demanda del mercado o una pol√≠tica interna absurda (como prohibir horas extras en el cuello de botella). EXPLOTAR la Restricci√≥n: Obtener lo m√°ximo del recurso que nos limita sin gastar dinero. Asegurarse de que el cuello de botella nunca pare por razones tontas (esperar materiales, reuniones innecesarias, configuraciones). SUBORDINAR todo lo dem√°s: Este es el paso m√°s radical. Todo el sistema debe ir al ritmo de la restricci√≥n. Poner a los recursos no-restringidos a trabajar al 100% es un desperdicio, ya que solo genera inventario (WIP) que la restricci√≥n no puede procesar. ELEVAR la Restricci√≥n: Si despu√©s de explotar y subordinar a√∫n necesitamos m√°s capacidad, solo ahora invertimos capital (CAPEX) para mejorar ese recurso (comprar otra m√°quina, contratar a otro ingeniero senior). REPETIR (Evitar la Inercia): En cuanto rompemos la restricci√≥n, otra parte del sistema se convertir√° en el nuevo cuello de botella. El ciclo debe reiniciarse inmediatamente en el Paso 1. Este ciclo se implementa t√°cticamente a trav√©s de Tambor-Amortiguador-Cuerda (DBR) : Tambor (Drum): La restricci√≥n, que marca el ritmo (el \"tambor\") para todo el sistema. Amortiguador (Buffer): Un buffer de tiempo (no de inventario) colocado justo antes de la restricci√≥n para asegurar que nunca se quede sin trabajo. Cuerda (Rope): Una se√±al que \"ata\" la liberaci√≥n de nuevos materiales al inicio del proceso al ritmo del tambor, evitando as√≠ que el sistema se inunde de trabajo en curso (WIP). La TOC como GPS para Lean y Six Sigma Una confusi√≥n com√∫n es ver la TOC, Lean y Six Sigma como metodolog√≠as competidoras. En realidad, son sin√©rgicas. Lean se enfoca en eliminar el desperdicio (",
    "url": "https://datalaria.com/es/posts/the-goal/",
    "slug": "the-goal",
    "lang": "es",
    "categories": [
      "Estrategia",
      "Gesti√≥n de Proyectos",
      "IA"
    ],
    "tags": [
      "toc",
      "la-meta",
      "teoria-de-restricciones",
      "gestion-de-proyectos",
      "cadena-de-suministro",
      "ia",
      "industria-4.0",
      "lean",
      "six-sigma",
      "cadena-critica"
    ],
    "date": "2025-12-13",
    "timestamp": 0,
    "domain": "General",
    "image": "cover.png"
  },
  {
    "objectID": "es_the_thinking_game",
    "title": "The Thinking Game: De c√≥mo DeepMind convirti√≥ los videojuegos en la mayor herramienta cient√≠fica de la historia",
    "description": "Un an√°lisis profundo del documental 'The Thinking Game', que narra la odisea de Demis Hassabis y DeepMind. Desde los p√≠xeles de Atari hasta la resoluci√≥n del plegamiento de prote√≠nas con AlphaFold, exploramos c√≥mo la b√∫squeda de la AGI est√° reescribiendo las reglas de la ciencia moderna.",
    "content": "En la era del hype desenfrenado por la Inteligencia Artificial Generativa, es f√°cil perder de vista el objetivo final. Mientras Silicon Valley compite por ver qui√©n tiene el chatbot m√°s elocuente, un laboratorio en Londres ha estado persiguiendo una meta diferente, casi esot√©rica: resolver la inteligencia para luego usarla para resolver todo lo dem√°s. El nuevo documental \"The Thinking Game\" (seleccionado por el Festival de Tribeca ) no es solo una pel√≠cula sobre tecnolog√≠a; es la cr√≥nica de una obsesi√≥n intelectual. Dirigida por Greg Kohs, la cinta sigue a Demis Hassabis y al equipo de Google DeepMind a lo largo de una d√©cada, narrando la transici√≥n desde una startup con problemas de liquidez hasta convertirse en el arquitecto de uno de los mayores avances cient√≠ficos del siglo XXI: AlphaFold [1] . Este post analiza los pilares t√©cnicos y filos√≥ficos que el documental expone, desgranando c√≥mo los videojuegos sirvieron de campo de entrenamiento para la ciencia dura y por qu√© la b√∫squeda de la Inteligencia Artificial General (AGI) es el \"Juego del Pensamiento\" definitivo. 1. El Arquitecto: De Ni√±o Prodigio a Visionario de la AGI Para entender DeepMind, hay que entender a Demis Hassabis. El documental hace un trabajo excelente al conectar los puntos de su biograf√≠a, revelando que la arquitectura de sus IAs no es accidental, sino un reflejo de su propia mente multidisciplinaria [2] . Hassabis no es el t√≠pico CEO tecnol√≥gico. Fue un ni√±o prodigio del ajedrez (el segundo mejor del mundo en su categor√≠a a los 13 a√±os) y comenz√≥ a programar a los 8 a√±os. A los 17, ya trabajaba en Bullfrog Productions dise√±ando la IA del ic√≥nico videojuego Theme Park . All√≠, Hassabis experiment√≥ con una premisa fascinante: simular el comportamiento humano mediante agentes aut√≥nomos. Si pon√≠as una tienda de comida con mucha sal al lado de una monta√±a rusa, los visitantes virtuales vomitaban. Nadie program√≥ el v√≥mito expl√≠citamente; fue una consecuencia emergente [3] . Tras su paso por la industria del videojuego, Hassabis dio un giro hacia la neurociencia en Cambridge. Su tesis: el cerebro humano es la √∫nica \"prueba de existencia\" que tenemos de que la inteligencia general es posible. DeepMind naci√≥ de esa intersecci√≥n: inspiraci√≥n biol√≥gica para construir silicio inteligente [4] . 2. El Campo de Pruebas: De los P√≠xeles a la Intuici√≥n La estrategia de DeepMind siempre fue clara: utilizar los juegos no como un fin, sino como un sandbox (caja de arena) seguro para entrenar algoritmos de Aprendizaje por Refuerzo ( Reinforcement Learning ). El documental estructura este progreso en tres fases cr√≠ticas que definieron la √∫ltima d√©cada de la IA: Fase 1: La Era Atari y el Aprendizaje Profundo El primer gran hito fue ense√±ar a una IA a jugar a juegos de Atari simplemente \"mirando\" los p√≠xeles de la pantalla, sin conocer las reglas. El momento \"Eureka\" lleg√≥ con el juego Breakout . Tras horas de entrenamiento, el agente DQN no solo aprendi√≥ a jugar, sino que descubri√≥ una estrategia √≥ptima: cavar un t√∫nel lateral para enviar la bola detr√°s del muro de ladrillos. Los desarrolladores no programaron esa t√°ctica; la m√°quina la invent√≥ [5] . Fase 2: AlphaGo y el \"Momento Sputnik\" El enfrentamiento contra Lee Sedol en 2016 es el eje dram√°tico de la primera mitad del filme. Aqu√≠, el documental destaca el famoso Movimiento 37 . Con una probabilidad de 1 entre 10.000 de ser jugado por un humano, ese movimiento fue la prueba definitiva de que la IA hab√≠a trascendido el c√°lculo bruto para entrar en el terreno de la creatividad e intuici√≥n . M√°s fascinante a√∫n fue la evoluci√≥n hacia AlphaZero . A diferencia de su predecesor, que estudi√≥ partidas humanas, AlphaZero aprendi√≥ desde la tabula rasa (pizarra en blanco), jugando contra s√≠ mismo millones de veces. Se convirti√≥ en su propio maestro, eliminando siglos de sesgo humano y descubriendo estrategias que ning√∫n Gran Maestro hab√≠a concebido jam√°s [6] . Fase 3: La Niebla de Guerra con AlphaStar El salto a StarCraft II represent√≥ el desaf√≠o de la informaci√≥n imperfecta. A diferencia del Go, donde todo el tablero es visible, en StarCraft hay \"niebla de guerra\". AlphaStar tuvo que aprender a explorar, planificar a largo plazo y tomar decisiones en tiempo real, alcanzando un nivel de Gran Maestro y demostrando que la IA pod√≠a manejar la incertidumbre [7] . 3. El Pivote Cient√≠fico: El Telescopio de AlphaFold Aqu√≠ es donde el documental y la misi√≥n de DeepMind alcanzan su cl√≠max. Hassabis usa una analog√≠a brillante que ayuda a entender la magnitud del salto desde los juegos a la biolog√≠a: \"Construir sistemas como AlphaGo fue como aprender a pulir lentes de vidrio con una precisi√≥n perfecta. Fue un arte dif√≠cil y t√©cnico, pero el fin √∫ltimo no era tener lentes bonitas en una estanter√≠a. AlphaFold fue el momento en que finalmente tomaron esas lentes, construyeron un telescopio y lo apuntaron al cielo biol√≥gico, revelando galaxias de estructuras de prote√≠nas que antes eran invisibles para el ojo humano.\" El problema",
    "url": "https://datalaria.com/es/posts/the_thinking_game/",
    "slug": "the_thinking_game",
    "lang": "es",
    "categories": [
      "Inteligencia Artificial",
      "Deep Tech",
      "Ciencia",
      "Documentales"
    ],
    "tags": [
      "DeepMind",
      "Demis Hassabis",
      "AlphaFold",
      "AGI",
      "AlphaGo",
      "The Thinking Game",
      "Biolog√≠a Computacional"
    ],
    "date": "2026-01-01",
    "timestamp": 0,
    "domain": "General",
    "image": "the_thinking_game.jpg"
  },
  {
    "objectID": "es_Visualizaciones-basicas",
    "title": "Visualizaciones b√°sicas",
    "description": "Conoce cu√°les son las visualizaciones b√°sicas m√°s utilizadas para analizar tus datos",
    "content": "Continuando con la informaci√≥n expuesta en el post estad√≠stica descriptiva , a continuaci√≥n, vamos a conocer las maneras m√°s b√°sicas de visualizar los datos o la informaci√≥n a analizar. Concretamente: Gr√°ficos de tarta Gr√°ficos de l√≠nea Gr√°ficos de rect√°ngulo y proyecci√≥n solar Histogramas y diagramas de barras Gr√°ficos de densidad Gr√°ficos de caja (Boxplots) Gr√°ficos de violin Gr√°ficos de dispersi√≥n Gr√°ficos de Pareto Nota: Para todos los ejemplos (excepto para los gr√°ficos de l√≠nea por sus caracter√≠sticas) se va a utilizar como dataset el fichero train.csv un subconjunto de 891 individuos representativo de la poblaci√≥n del Titanic utilizado para el entrenamiento de modelos de Aprendizaje autom√°tico. Gr√°ficos de tarta Los gr√°ficos de tarta o circulares (en ingl√©s pie charts ) son utilizados para representar la proporci√≥n de los datos de una variable o caracter√≠stica seg√∫n su valor respecto al total de los mismos. Este tipo de gr√°ficos presentan multitud de variantes y son ampliamente utilizados debido a su sencillez de preparaci√≥n e interpretaci√≥n. Sin embargo, muchas veces se abusa de este tipo de gr√°ficos o se utilizan indebidamente (ver el siguiente art√≠culo sobre el mal uso de los gr√°ficos de tarta ). Ejemplo: Proporciones de pasajeros del Titanic por estatus socioecon√≥mico Gr√°ficos de L√≠nea Los gr√°ficos de l√≠nea se utilizan para mostrar los datos de una variable sobre una secuencia de datos continuados. Generalmente, se utilizan para representar tendencias o relaciones (en agrupaci√≥n con m√°s variables) entre los datos durante un intervalo de tiempo continuo. Nota: Con el fin de visualizar un caso de uso en un intervalo de tiempo continuo, para los siguientes ejemplos se ha utilizado el dataset de la biblioteca de datos p√∫blicos del Gobierno de Espa√±a sobre los datos de poblaci√≥n por comunidades aut√≥nomas en la serie hist√≥rica desde 1997 a 2017. Ejemplo: Poblaci√≥n espa√±ola desde 1997 Ejemplo: Poblaci√≥n espa√±ola por comunidades desde 1997 Gr√°ficos de rect√°ngulo y proyecci√≥n solar Los gr√°ficos de rect√°ngulos (en ingl√©s treemaps ) y los gr√°ficos de proyecci√≥n solar (en ingl√©s sunburst ) son visualizaciones para representar datos jer√°rquicos y resumir la informaci√≥n en √°reas proporcionales a los valores correspondientes. En el caso gr√°ficos de rect√°ngulos, la representaci√≥n consiste en agrupaciones de los datos mediante rect√°ngulos con un √°rea proporcional a su valor respecto al total. Ejemplo: Proporci√≥n de pasajeros seg√∫n su estatus socioecon√≥mico clasificados por g√©nero (gr√°fico rect√°ngulo) En el caso de los gr√°ficos de proyecci√≥n solar, la representaci√≥n consiste en agrupaciones de los datos mediante diferentes anillos seg√∫n el nivel representado, y √©stos a su vez, se dividen a en partes proporcionales al valor de los datos que representan. Ejemplo: Proporci√≥n de pasajeros seg√∫n su estatus socioecon√≥mico clasificados por g√©nero (gr√°fico proyecci√≥n solar) Sobre el ejemplo representado, se puede apreciar r√°pidamente informaci√≥n sobre los datos jerarquizados representados como por ejemplo que el n√∫mero de pasajeros hombres era de casi 2/3 de la tripulaci√≥n, as√≠ como que los pasajeros hombres y de estatus socioecon√≥mico bajo representaban algo m√°s de 1/3 de la tripulaci√≥n. Histogramas y Gr√°ficos de barras Recopilando la informaci√≥n ya recogida en la entrada Estad√≠stica descriptiva , se explican dos de las representaciones m√°s utilizadas en los an√°lisis de datos, cuyo principal objetivo es representar la distribuci√≥n de los datos respecto a una variable o caracter√≠stica del conjunto de datos. As√≠ pues, los histogramas permiten visualizar datos continuos y los diagramas de barras representan datos discretos. Ejemplo: Distribuci√≥n de pasajeros por precio del pasaje Ejemplo: Distribuci√≥n de pasajeros por estatus socioecon√≥mico Como ya se vio en la entrada sobre ‚ÄúEstad√≠stica Descriptiva‚Äù, representando sobre estos gr√°ficos medidas de estad√≠stica descriptiva (como la media o la mediana), podemos caracterizar la distribuci√≥n de la variable bajo an√°lisis y con ello extraer m√°s informaci√≥n al respecto. Gr√°ficos de densidad Tambi√©n conocidos como gr√°ficos de densidad de Kernel, sirven para visualizar la distribuci√≥n de los datos en un intervalo continuo. Este tipo de gr√°ficos es una variaci√≥n de los Histogramas donde se representa de manera suavizada la forma de la distribuci√≥n. Ejemplo: Distribuci√≥n de la edad de los pasajeros del Titanic Diagramas de caja Los diagramas de caja o de bigote (en ingl√©s ‚Äúboxplots‚Äù) as√≠ llamados por la forma de representaci√≥n, sirven para representar una variable o caracter√≠stica en funci√≥n de su rango y sus cuartiles. Nota: Los cuartiles son 3 valores que dividen un conjunto de datos ordenados en 4 partes porcentualmente iguales. La visualizaci√≥n del boxplot consiste en una caja que comprende los datos desde el primer cuartil (Q1 - 25% de los datos bajo √©ste) al tercer cuartil (Q3 - 75% de los datos bajo √©ste) de los valores de la variable representada (rango intercuartil ‚Äì IQR). Dent",
    "url": "https://datalaria.com/es/posts/Visualizaciones-basicas/",
    "slug": "Visualizaciones-basicas",
    "lang": "es",
    "categories": [
      "fundamentos"
    ],
    "tags": [
      "estad√≠stica",
      "visualizaciones",
      "distribucion",
      "gr√°ficos",
      "representaciones"
    ],
    "date": "2025-08-12",
    "timestamp": 0,
    "domain": "General",
    "image": "visualizaciones_basicas.png"
  }
]